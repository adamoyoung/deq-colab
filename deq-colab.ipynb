{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "c022d438-ec47-4219-a1a6-1a7c0d431933",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 731,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 732,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.lax as lax\n",
        "from jax import random\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 733,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 734,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 735,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", ls=False, name=\"unknown\"):\n",
        "    bsz, total_hsize, seq_len = x0.shape\n",
        "    g = lambda y: f(y) - y\n",
        "    dev = x0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    x_est = x0           # (bsz, 2d, L')\n",
        "    gx = g(x_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_xest, lowest_gx = 0, x_est, gx\n",
        "\n",
        "    while nstep < threshold:\n",
        "        x_est, gx, delta_x, delta_gx, ite = line_search_jax(update, x_est, gx, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gx)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gx + x_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    # lowest_xest, lowest_gx = lax.stop_gradient(x_est.copy()), lax.stop_gradient(gx.copy())\n",
        "                    lowest_xest, lowest_gx = lax.stop_gradient(x_est), lax.stop_gradient(gx)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_x)\n",
        "        u = (delta_x - matvec_jax(part_Us, part_VTs, delta_gx)) / jnp.einsum('bij, bij -> b', vT, delta_gx)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    return {\"result\": lowest_xest,\n",
        "            \"lowest\": lowest_dict[stop_mode],\n",
        "            \"nstep\": lowest_step_dict[stop_mode],\n",
        "            \"prot_break\": prot_break,\n",
        "            \"abs_trace\": trace_dict['abs'],\n",
        "            \"rel_trace\": trace_dict['rel'],\n",
        "            \"eps\": eps,\n",
        "            \"threshold\": threshold}\n",
        "\n",
        "\n",
        "def newton_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", name=\"unknown\"):\n",
        "\n",
        "    g = lambda y: f(y) - y\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    x = x0\n",
        "    gx = g(x)\n",
        "    gx_norm = jnp.linalg.norm(gx)\n",
        "    nstep = 0\n",
        "    # print(gx_norm)\n",
        "\n",
        "    while nstep < threshold:\n",
        "      # solve system\n",
        "      delta_x = jnp.linalg.solve(jac_g(x),-g(x))\n",
        "      x = x + delta_x\n",
        "      gx = g(x)\n",
        "      gx_norm = jnp.linalg.norm(gx)\n",
        "      nstep += 1\n",
        "      # print(gx_norm)\n",
        "\n",
        "    return x, gx, gx_norm"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 736,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3)  # can also be (5, 5), modify later\n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "''' \n",
        "    assert statement we'll need    \n",
        "    assert that the number of branches == len(input_channel_vector)\n",
        "    assert also that num_branches == len(kernel_size_vector)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "EzcfTWz6agE_",
        "outputId": "f9acf059-440f-4abb-cb55-b30c90fd88e6"
      },
      "execution_count": 737,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    assert statement we'll need    \\n    assert that the number of branches == len(input_channel_vector)\\n    assert also that num_branches == len(kernel_size_vector)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 737
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def _downsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3, 3), strides=(2,2), padding=1, use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            #down_block.append(module)\n",
        "        seq = nn.Sequential(down_block)\n",
        "        return seq\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        #self.downsample_fn = self._downsample()\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            \n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(conv_down)\n",
        "            #down_block.append(group_down)\n",
        "                 \n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(module)\n",
        "            \n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "        #self.layers = down_block\n",
        "        #print('seq', self.layers)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        '''\n",
        "        z = z_plus\n",
        "        for i, lyr in enumerate(self.layers[:-1]):\n",
        "            print(i, z.shape)\n",
        "            z = lyr(z)\n",
        "            z = nn.relu(z)\n",
        "            print(i, z.shape)\n",
        "            #z = nn.sigmoid(z)  # nn.silu(z)  # jnp.tanh(z)  # nn.sigmoid(z)\n",
        "        out = self.layers[-1](z)\n",
        "        '''\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 738,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 739,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class f_theta(nn.Module):\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    features: Tuple[int] = (16, 4)\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    \n",
        "    # branches: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "\n",
        "\n",
        "    #  TODO HERE\n",
        "    # UnfilteredStackTrace: jax.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (64, 32640, 1) and (64, 32, 32, 24)\n",
        "    def cringy_reshape(self, in_vec, shape_list):\n",
        "        start = 0\n",
        "        out_vec = []\n",
        "        in_vec = jnp.array(in_vec)\n",
        "        for size in shape_list:\n",
        "            my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "            end = start+my_elems\n",
        "            my_chunk = in_vec[:, start:end]\n",
        "            start += my_elems\n",
        "            my_chunk = jnp.reshape(my_chunk, size)\n",
        "            out_vec.append(my_chunk)\n",
        "\n",
        "        return out_vec\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # self.downsample = DownSample(channels=self.channels,\n",
        "        #                              num_groups=self.num_groups)\n",
        "        # self.upsample = UpSample(channels=self.channels,\n",
        "        #                          num_groups=self.num_groups)\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    # array.append(z_plus[i])\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                 #channel_dimension=channel_dimensions)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                # channel_dimension=channel_dimensions)\n",
        "                    # array.append(nn.Module(sampled))\n",
        "                    array.append(sampled)\n",
        "            # fuse_layers.append(nn.Module(array))\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.relu,\n",
        "                                          nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  #kernel_init=self.kernel_init, \n",
        "                                                  use_bias=False),\n",
        "                                          nn.GroupNorm(num_groups=self.num_groups//2)]))\n",
        "                                                       #group_size=self.channels[i])]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, x, injection, shape_list):\n",
        "        x = self.cringy_reshape(x, shape_list)\n",
        "        #print('preshape injection', injection.shape)\n",
        "        #injection = self.cringy_reshape(injection, shape_list)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        \n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](x[i], i, injection[i])) # z, branch, x\n",
        "\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "          intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "          for j in range(self.num_branches):\n",
        "            if i == j:\n",
        "              intermediate_i += branch_outputs[j]\n",
        "            else:\n",
        "              if self.fuse_branches[i][j] is not None:\n",
        "                  temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                  intermediate_i += temp\n",
        "              else:\n",
        "                  raise Exception(\"Should not happen.\")\n",
        "              #print('mimimi', self.fuse_branches[i][j])\n",
        "              #intermediate_i += self.fuse_branches[i][j](branch_outputs[j])\n",
        "          fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "\n",
        "        return fuse_outputs\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 740,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQModel(nn.Module):\n",
        "    solver_fn: Callable\n",
        "\n",
        "    num_groups: int = 8\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "    branches: List[int] = field(default_factory=lambda:[1, 1, 1])\n",
        "    training: bool = True\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "    features: Tuple[int] = (16, 4)\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.branches)\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3,3), strides=(1,1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3,3), strides=(1,1))\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.model = f_theta(num_branches=len(self.channels), channels=self.channels, num_groups=self.num_groups)\n",
        "                                         \n",
        "    def __call__(self, x):\n",
        "        #x = self.transform(x)\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        #x = self.relu(self.conv1(x))\n",
        "        #x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.bn2(self.conv2(x), use_running_average=True))\n",
        "        \n",
        "        x_list = [x]\n",
        "        for i in range(1, self.num_branches):\n",
        "            bs, H, W, _ = x_list[-1].shape\n",
        "            new_item = jnp.zeros((bs, H//2, W//2, self.channels[i]))\n",
        "            x_list.append(new_item)\n",
        "        z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "        shape_list = [el.shape for el in z_list]\n",
        "        \n",
        "        bsz = x.shape[0]\n",
        "        # func = lambda z: self.model(x=z, injection=x_list, shape_list=shape_list)\n",
        "        def make_vec(in_vec):\n",
        "            return jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in in_vec], axis=1)\n",
        "        def func(z):\n",
        "            out = self.model(x=z, injection=x_list, shape_list=shape_list)\n",
        "            return make_vec(out) \n",
        "        # z_vec = jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in z_list], axis=1)\n",
        "        z_vec = make_vec(z_list) \n",
        "        result = self.solver_fn(func, z_vec, threshold=3)\n",
        "        z_vec = result['result']\n",
        "        output = z_vec\n",
        "        if self.training:\n",
        "            output = func(z_vec)\n",
        "            #output = func(z_vec.requires_grad_())\n",
        "        # jac_loss = jac_loss_estimate(output, z1) # comes from the follow-up paper\n",
        "        \n",
        "        y_list = self.model.cringy_reshape(output, shape_list) # TO DO -- for now without dropout!\n",
        "        return y_list"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 741,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 742,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[4, 8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def _make_layer(self, inplanes, planes):\n",
        "          downsample = False\n",
        "          if inplanes != planes * self.expansion:\n",
        "              downsample = True\n",
        "              #downsample = nn.Sequential([nn.Conv(planes*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False),\n",
        "               #                          nn.BatchNorm()])\n",
        "          return CLSBlock(inplanes, planes, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        incre_modules = []\n",
        "        for i, channels  in enumerate(self.channels):\n",
        "            print(\"channels/head_channels[i]\",  channels, self.output_channels[i])\n",
        "            incre_mod = self._make_layer(self.channels[i], self.output_channels[i])\n",
        "            incre_modules.append(incre_mod)\n",
        "        self.incre_modules = incre_modules\n",
        "        downsamp_modules = []\n",
        "        for i in range(len(self.channels)-1):\n",
        "            in_channels = self.output_channels[i] * self.expansion\n",
        "            out_channels = self.output_channels[i+1] * self.expansion\n",
        "            downsamp_module = nn.Sequential([nn.Conv(out_channels, kernel_size=(3,3), strides=(2,2), use_bias=True),\n",
        "                                            #nn.BatchNorm(),\n",
        "                                            nn.relu])\n",
        "            downsamp_modules.append(downsamp_module)\n",
        "        self.downsamp_modules = downsamp_modules\n",
        "\n",
        "        self.final_layer = nn.Sequential([nn.Conv(self.final_chansize, kernel_size=(1,1)),\n",
        "                                         #nn.BatchNorm(),\n",
        "                                         nn.relu])\n",
        "        # y = F.avg_pool2d(y, kernel_size=y.size()[2:]).view(y.size(0), -1)\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y_list):\n",
        "        y = self.incre_modules[0](y_list[0])\n",
        "        for i in range(len(self.downsamp_modules)):\n",
        "            y = self.incre_modules[i+1](y_list[i+1]) + self.downsamp_modules[i](y)\n",
        "        y = self.final_layer(y)\n",
        "        y = nn.avg_pool(y, window_shape=y.shape[1:3])\n",
        "        y = jnp.reshape(y, (y.shape[0], -1))\n",
        "        y = self.classifier(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 743,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    # label = jax.nn.one_hot(label, num_classes=num_classes)\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.CIFAR10(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.CIFAR10(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:1000], train_ds.targets[:1000])\n",
        "    test_images, test_labels = transform(test_ds.data[:200], test_ds.targets[:200])\n",
        "    return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 744,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_fn(head, mdeq, weights, images):\n",
        "    '''\n",
        "    mdeq: lambda function from below (taking weights and images as arguments)\n",
        "    '''\n",
        "    y_batch = mdeq(weights['mdeq'], images)\n",
        "    #y_batch = jnp.reshape(y_batch, (y_batch.shape[0], -1))\n",
        "    logits = head.apply(weights['head'], y_batch)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "SJKf2DGbNArr"
      },
      "execution_count": 745,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    '''\n",
        "    extra thing: warm-up using gradient descent in pytorch code of official repo\n",
        "    --> check impact of that and maybe also cost etc (eg if only one layer etc)\n",
        "    '''\n",
        "\n",
        "    max_itr = 7 \n",
        "    print_interval = 5\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    train_size = train_images.shape[0]\n",
        "    batch_size = 64\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    solver_fn = broyden_jax\n",
        "\n",
        "    my_model = MDEQModel(solver_fn=solver_fn)\n",
        "    my_deq = lambda mdeq_weights, images: my_model.apply(mdeq_weights, images)\n",
        "\n",
        "    # def cross_entropy_loss(*, logits, labels):\n",
        "    def cross_entropy_loss(logits, labels):\n",
        "        ''' \n",
        "        should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "        if getting funny results maybe remove log of logits\n",
        "        '''\n",
        "        one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "        softmaxed_vals = jax.nn.log_softmax(logits)\n",
        "        acc = (jnp.argmax(softmaxed_vals, axis=-1) == jnp.argmax(one_hot_labels, axis=-1)).mean()\n",
        "        output = (-one_hot_labels * softmaxed_vals).mean()\n",
        "        #print(softmaxed_vals, labels, output)\n",
        "        #acc = None\n",
        "        return output, acc\n",
        "        #return -jnp.mean(jnp.sum(one_hot_labels * jnp.log(logits), axis=-1))\n",
        "\n",
        "    png = jax.random.PRNGKey(0)\n",
        "    png, _ = jax.random.split(png, 2)\n",
        "    dummy_input = jnp.ones((batch_size, 32, 32, 3))\n",
        "    #cls_dummy_input = jnp.ones((32256,))\n",
        "    cls_dummy_input = [jnp.ones((64, 32, 32, 24)),\n",
        "                       jnp.ones((64, 16, 16, 24)),\n",
        "                       jnp.ones((64, 8, 8, 24)),]\n",
        "    mdeq_weights = my_model.init(rngs=png, x=dummy_input)\n",
        "    png, _ = jax.random.split(png, 2)\n",
        "    #head = nn.Sequential([nn.Dense(1000), nn.relu, nn.Dense(10)])\n",
        "    #head = nn.Dense(10)\n",
        "    head = Classifier()\n",
        "    cls_weights = head.init(png, cls_dummy_input)\n",
        "    weights = {'mdeq': mdeq_weights, 'head': cls_weights}\n",
        "\n",
        "    #optimizer = optax.adamw(learning_rate=0.001, weight_decay=0.001)\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    # optimizer = flax.optim.Adam(learning_rate=1e-3).create(weights)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "    def loss(weights, x_batch, y_true):\n",
        "        logits = forward_fn(head, my_deq, weights, x_batch)\n",
        "        # y_batch = my_deq(weights, x_batch)\n",
        "        return loss_fn(logits, y_true)\n",
        "  \n",
        "    def step(weights, opt_state, x_batch, y_true):\n",
        "        (loss_vals, acc), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=train_size, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler():\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "\n",
        "        return shuffled_indices\n",
        "\n",
        "\n",
        "    # g = generator(batch_size=batch_size)\n",
        "    # print('g', g)\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    for itr in range(max_itr):\n",
        "        idxs = list_shuffler()\n",
        "        start, end = 0, 0\n",
        "           \n",
        "        # batch_idxs = next(g)\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab]\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "  \n",
        "            weights, opt_state, loss_vals, acc = step(weights=weights,\n",
        "                                                 opt_state=opt_state,\n",
        "                                                 x_batch=x_batch,\n",
        "                                                 y_true=y_true)\n",
        "            #batch_loss = (loss_vals*x_batch.shape[0])/train_images.shape[0]\n",
        "            #batch_acc = (acc*x_batch.shape[0])/train_images.shape[0]\n",
        "\n",
        "            #train_log.append((batch_loss, batch_acc))\n",
        "            print(f\"batch_loss :: {loss_vals} // batch_acc :: {acc}\")\n",
        "            # loss_vals, grads = jax.value_and_grad(loss, has_aux=False)(optimizer.target, x_batch, y_true)\n",
        "            # optimizer = optimizer.apply_gradient(grads)\n",
        "            \n",
        "            if itr % print_interval == 0:\n",
        "                print(\"\\tat step\", itr, \"have loss\", loss_vals)\n",
        "\n",
        "            if loss_vals < 1e-5:\n",
        "                break\n",
        "\n",
        "            print('finally', loss_vals)\n",
        "        "
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 746,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breakdown of code overall:\n",
        "\n",
        "\n",
        "*   MDEQ modul\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "7RvQXU3CROAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cb18d3e3-89ba-40c6-9e2d-32be55fd9c70"
      },
      "execution_count": 747,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.25425930544734004 // batch_acc :: 0.109375\n",
            "\tat step 0 have loss 0.25425930544734004\n",
            "finally 0.25425930544734004\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23409196436405183 // batch_acc :: 0.109375\n",
            "\tat step 0 have loss 0.23409196436405183\n",
            "finally 0.23409196436405183\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23403982371091844 // batch_acc :: 0.109375\n",
            "\tat step 0 have loss 0.23403982371091844\n",
            "finally 0.23403982371091844\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2312781134620309 // batch_acc :: 0.140625\n",
            "\tat step 0 have loss 0.2312781134620309\n",
            "finally 0.2312781134620309\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23266382608562708 // batch_acc :: 0.15625\n",
            "\tat step 0 have loss 0.23266382608562708\n",
            "finally 0.23266382608562708\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23554442133754494 // batch_acc :: 0.109375\n",
            "\tat step 0 have loss 0.23554442133754494\n",
            "finally 0.23554442133754494\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2297242734581232 // batch_acc :: 0.09375\n",
            "\tat step 0 have loss 0.2297242734581232\n",
            "finally 0.2297242734581232\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2301609143614769 // batch_acc :: 0.09375\n",
            "\tat step 0 have loss 0.2301609143614769\n",
            "finally 0.2301609143614769\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2331400524824858 // batch_acc :: 0.078125\n",
            "\tat step 0 have loss 0.2331400524824858\n",
            "finally 0.2331400524824858\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2326367884874344 // batch_acc :: 0.125\n",
            "\tat step 0 have loss 0.2326367884874344\n",
            "finally 0.2326367884874344\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23471920862793924 // batch_acc :: 0.109375\n",
            "\tat step 0 have loss 0.23471920862793924\n",
            "finally 0.23471920862793924\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2296913977712393 // batch_acc :: 0.140625\n",
            "\tat step 0 have loss 0.2296913977712393\n",
            "finally 0.2296913977712393\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.227117133513093 // batch_acc :: 0.125\n",
            "\tat step 0 have loss 0.227117133513093\n",
            "finally 0.227117133513093\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23127351813018324 // batch_acc :: 0.09375\n",
            "\tat step 0 have loss 0.23127351813018324\n",
            "finally 0.23127351813018324\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2306761022657156 // batch_acc :: 0.15625\n",
            "\tat step 0 have loss 0.2306761022657156\n",
            "finally 0.2306761022657156\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (40, 200)\n",
            "???????? (40, 32, 32, 24)\n",
            "???????? (40, 16, 16, 24)\n",
            "???????? (40, 8, 8, 24)\n",
            "logit (40, 10)\n",
            "batch_loss :: 0.23358120024204254 // batch_acc :: 0.125\n",
            "\tat step 0 have loss 0.23358120024204254\n",
            "finally 0.23358120024204254\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23268216121941807 // batch_acc :: 0.046875\n",
            "finally 0.23268216121941807\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23626564703881742 // batch_acc :: 0.109375\n",
            "finally 0.23626564703881742\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2334321701899171 // batch_acc :: 0.09375\n",
            "finally 0.2334321701899171\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23163302037864925 // batch_acc :: 0.078125\n",
            "finally 0.23163302037864925\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23367898967117073 // batch_acc :: 0.09375\n",
            "finally 0.23367898967117073\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2351640496402979 // batch_acc :: 0.09375\n",
            "finally 0.2351640496402979\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2310290724039078 // batch_acc :: 0.140625\n",
            "finally 0.2310290724039078\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23053216934204102 // batch_acc :: 0.09375\n",
            "finally 0.23053216934204102\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23184623382985592 // batch_acc :: 0.109375\n",
            "finally 0.23184623382985592\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23161450363695624 // batch_acc :: 0.0625\n",
            "finally 0.23161450363695624\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22912937439978123 // batch_acc :: 0.125\n",
            "finally 0.22912937439978123\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2294903784990311 // batch_acc :: 0.15625\n",
            "finally 0.2294903784990311\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22919018231332303 // batch_acc :: 0.125\n",
            "finally 0.22919018231332303\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22838916927576067 // batch_acc :: 0.09375\n",
            "finally 0.22838916927576067\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23022045865654947 // batch_acc :: 0.109375\n",
            "finally 0.23022045865654947\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (40, 200)\n",
            "???????? (40, 32, 32, 24)\n",
            "???????? (40, 16, 16, 24)\n",
            "???????? (40, 8, 8, 24)\n",
            "logit (40, 10)\n",
            "batch_loss :: 0.22947704553604126 // batch_acc :: 0.025\n",
            "finally 0.22947704553604126\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23255581967532635 // batch_acc :: 0.078125\n",
            "finally 0.23255581967532635\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.23141989558935167 // batch_acc :: 0.015625\n",
            "finally 0.23141989558935167\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.2275406323373318 // batch_acc :: 0.15625\n",
            "finally 0.2275406323373318\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22855846025049686 // batch_acc :: 0.171875\n",
            "finally 0.22855846025049686\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22798544205725194 // batch_acc :: 0.140625\n",
            "finally 0.22798544205725194\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n",
            "batch_loss :: 0.22819444686174395 // batch_acc :: 0.09375\n",
            "finally 0.22819444686174395\n",
            "channels/head_channels[i] 24 4\n",
            "channels/head_channels[i] 24 8\n",
            "channels/head_channels[i] 24 16\n",
            "iiiii (64, 200)\n",
            "???????? (64, 32, 32, 24)\n",
            "???????? (64, 16, 16, 24)\n",
            "???????? (64, 8, 8, 24)\n",
            "logit (64, 10)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-747-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-746-9e581fe7d645>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m                                                  \u001b[0mopt_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                                  \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                                                  y_true=y_true)\n\u001b[0m\u001b[1;32m    106\u001b[0m             \u001b[0;31m#batch_loss = (loss_vals*x_batch.shape[0])/train_images.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m#batch_acc = (acc*x_batch.shape[0])/train_images.shape[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-746-9e581fe7d645>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(weights, opt_state, x_batch, y_true)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/combine.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(updates, state, params)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36mupdate_fn\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update_moment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0mnu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_update_moment_per_elem_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m     \u001b[0mcount_inc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumerics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_int32_increment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mmu_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_bias_correction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount_inc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36m_update_moment_per_elem_norm\u001b[0;34m(updates, moments, decay, order)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   return jax.tree_multimap(\n\u001b[0;32m--> 100\u001b[0;31m       lambda g, t: (1 - decay) * orderth_norm(g) + decay * t, updates, moments)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mtree_multimap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/tree_util.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mtree_multimap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(g, t)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   return jax.tree_multimap(\n\u001b[0;32m--> 100\u001b[0;31m       lambda g, t: (1 - decay) * orderth_norm(g) + decay * t, updates, moments)\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/optax/_src/transform.py\u001b[0m in \u001b[0;36morderth_norm\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     89\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0morderth_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misrealobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       \u001b[0mhalf_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morder\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5250\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5251\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5252\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5253\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeferring_binary_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/numpy/ufuncs.py\u001b[0m in \u001b[0;36mpower\u001b[0;34m(x1, x2)\u001b[0m\n\u001b[1;32m    324\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mlax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteger_pow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_power\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/lax/lax.py\u001b[0m in \u001b[0;36minteger_pow\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minteger_pow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m   \u001b[0;34mr\"\"\"Elementwise power: :math:`x^y`, where :math:`y` is a fixed integer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0minteger_pow_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mArray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    284\u001b[0m     assert (not config.jax_enable_checks or\n\u001b[1;32m    285\u001b[0m             all(isinstance(arg, Tracer) or valid_jaxtype(arg) for arg in args)), args\n\u001b[0;32m--> 286\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_top_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mbind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbind_with_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_raise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiple_results\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfull_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36mprocess_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_primitive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mprocess_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprimitive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/dispatch.py\u001b[0m in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m     91\u001b[0m   \u001b[0;34m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   compiled_fun = xla_primitive_callable(prim, *unsafe_map(arg_spec, args),\n\u001b[0;32m---> 93\u001b[0;31m                                         **params)\n\u001b[0m\u001b[1;32m     94\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trace_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_clear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_clear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/core.py\u001b[0m in \u001b[0;36m__hash__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;31m# the unique character code via hash(self.dtype.char)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     return hash((self.shape, self.dtype, self.weak_type,\n\u001b[0;32m-> 1242\u001b[0;31m                  tuple(self.named_shape.items())))\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mat_least_vspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fHT71ffxr6en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}