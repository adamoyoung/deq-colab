{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.lax as lax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", ls=False, name=\"unknown\"):\n",
        "    bsz, total_hsize, seq_len = x0.shape\n",
        "    g = lambda y: f(y) - y\n",
        "    dev = x0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    x_est = x0           # (bsz, 2d, L')\n",
        "    gx = g(x_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_xest, lowest_gx = 0, x_est, gx\n",
        "\n",
        "    while nstep < threshold:\n",
        "        x_est, gx, delta_x, delta_gx, ite = line_search_jax(update, x_est, gx, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gx)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gx + x_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_xest, lowest_gx = lax.stop_gradient(x_est.copy()), lax.stop_gradient(gx.copy())\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_x)\n",
        "        u = (delta_x - matvec_jax(part_Us, part_VTs, delta_gx)) / jnp.einsum('bij, bij -> b', vT, delta_gx)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    return {\"result\": lowest_xest,\n",
        "            \"lowest\": lowest_dict[stop_mode],\n",
        "            \"nstep\": lowest_step_dict[stop_mode],\n",
        "            \"prot_break\": prot_break,\n",
        "            \"abs_trace\": trace_dict['abs'],\n",
        "            \"rel_trace\": trace_dict['rel'],\n",
        "            \"eps\": eps,\n",
        "            \"threshold\": threshold}\n",
        "\n",
        "\n",
        "def newton_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", name=\"unknown\"):\n",
        "\n",
        "    g = lambda y: f(y) - y\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    x = x0\n",
        "    gx = g(x)\n",
        "    gx_norm = jnp.linalg.norm(gx)\n",
        "    nstep = 0\n",
        "    print(gx_norm)\n",
        "\n",
        "    while nstep < threshold:\n",
        "      # solve system\n",
        "      delta_x = jnp.linalg.solve(jac_g(x),-g(x))\n",
        "      x = x + delta_x\n",
        "      gx = g(x)\n",
        "      gx_norm = jnp.linalg.norm(gx)\n",
        "      nstep += 1\n",
        "      print(gx_norm)\n",
        "\n",
        "    return x, gx, gx_norm"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    input: jnp.DeviceArray\n",
        "    input_dim: int = 8\n",
        "    hidden_dim: int = 2*input_dim\n",
        "    kernel_size: Tuple[int] = (3, 3)  # can also be (5, 5), modify later\n",
        "    num_groups: int = 2\n",
        "    curr_branch: int = 0\n",
        "    \n",
        "    def setup(self, i, num_channels):  \n",
        "        self.input_dim = num_channels\n",
        "        self.hidden_dim =  2*input_dim\n",
        "        self.curr_branch = i\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size, strides=1)\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups, group_size=self.hidden_dim)\n",
        "        self.relu = nn.relu()\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size, strides=1)\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups, group_size=self.input_dim)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups, group_size=self.input_dim)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        \n",
        "        h2 = self.conv2(z)\n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        h2 = self.group2(h2)\n",
        "        h2 += x\n",
        "        \n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "''' \n",
        "    assert statement we'll need    \n",
        "    assert that the number of branches == len(input_channel_vector)\n",
        "    assert also that num_branches == len(kernel_size_vector)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EzcfTWz6agE_",
        "outputId": "820a1cce-7c56-4fdb-c3a8-0cc8ebc657c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    assert statement we'll need    \\n    assert that the number of branches == len(input_channel_vector)\\n    assert also that num_branches == len(kernel_size_vector)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    def setup(self, branches, channel_dimensions, num_groups):\n",
        "        self.in_chan, self.out_chan = channel_dimensions\n",
        "        self.num_groups = num_groups\n",
        "\n",
        "    def _downsample(self, branches):\n",
        "        from_res, to_res = branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "\n",
        "        for n in range(len(num_samples)):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=3, strides=2, padding=1,\n",
        "                               bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups,\n",
        "                                      group_size=inter_chan)\n",
        "            relu_down = nn.relu()\n",
        "            module_list = [conv_down, group_down]\n",
        "            if n < num_samples - 1:\n",
        "                module = nn.Sequential([conv_down,\n",
        "                                        group_down,\n",
        "                                        relu_down])\n",
        "            else:\n",
        "                module = nn.Sequential([conv_down,\n",
        "                                        group_down])\n",
        "            down_block.append(module)\n",
        "        return nn.Sequential(down_block)\n",
        "\n",
        "    def __call__(self, branches, z_plus):\n",
        "        downsample = self._downsample(branches)\n",
        "        return downsample(z_plus)\n"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    def setup(self, channel_dimensions, num_groups):\n",
        "        self.in_chan, self.out_chan = channel_dimensions\n",
        "        self.num_groups = num_groups\n",
        "\n",
        "    def _upsample(self, branches):\n",
        "        from_res, to_res = branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "        return [nn.Conv(features=self.out_chan, kernel_size=1, bias=False),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups, group_size=inter_chan),\n",
        "                        nn.Upsample(scale_factor=2**num_samples)]\n",
        "            \n",
        "\n",
        "    def __call__(self, branches, z_plus):\n",
        "        upsample = self._upsample(branches)\n",
        "        return upsample(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class f_theta(nn.Module):\n",
        "    features: Tuple[int] = (16, 4)\n",
        "    num_groups: int = 8\n",
        "    channels: List[int] = [24, 24, 24]\n",
        "    branches: List[int] = [1,1,1]\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        self.num_branches = len(self.branches)\n",
        "        self.res_block = MDEQBlock()\n",
        "        self.downsample = DownSample(channel_dimensions=self.channels,\n",
        "                                     num_groups=self.num_groups)\n",
        "        self.upsample = UpSample(channel_dimensions=self.channels,\n",
        "                                 num_groups=self.num_groups)\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in self.num_branches:\n",
        "          branches.append(MDEQBlock(i, self.channels[i]))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "\n",
        "        out = 1\n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    array.append(z_plus[i])\n",
        "                else:\n",
        "                    if i < j:\n",
        "                        sampled = self.downsample(z_plus=z_plus, branches=(i, j),\n",
        "                                                 channel_dimension=channel_dimensions)\n",
        "                    elif i > j:\n",
        "                        sampled = self.upsample(z_plus=z_plus, branches=(i, j),\n",
        "                                                 channel_dimension=channel_dimensions)\n",
        "                    array.append(nn.Module(sampled))\n",
        "            # fuse_layers.append(nn.Module(array))\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.relu(),\n",
        "                                          nn.Conv(features=self.channels[i], kernel_size=1, bias=False),\n",
        "                                          nn.GroupNorm(num_groups=self.num_groups//2,\n",
        "                                                       group_size=self.channels[i])]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, x, injection):\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](x[i], i, injection[i])) # z, branch, x\n",
        "\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "          intermediate_i = 0\n",
        "          for j in range(self.num_branches):\n",
        "            if i == j:\n",
        "              intermediate_i += branch_outputs[i]\n",
        "            else:\n",
        "              intermediate_i += self.fuse[i][j](branch_outputs[j])\n",
        "          fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "\n",
        "        return fuse_outputs\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQModel(nn.Module):\n",
        "    features: Tuple[int] = (16, 4)\n",
        "    num_branches: int = 3\n",
        "    num_groups: int = 8\n",
        "    channels: List[int] = [24, 24, 24]\n",
        "    branches: List[int] = [1,1,1]\n",
        "    training: bool = True\n",
        "    solver_fn: Callable = broyden_jax\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.branches)\n",
        "        self.transform = [nn.Sequential(OrderedDict([(nn.Conv(features=self.channels[i], kernel_size=3, stride=1)),\n",
        "                                       (nn.BatchNorm()),\n",
        "                                       (nn.relu()),\n",
        "                                       (nn.Conv(features=self.channels[i], kernel_size=3, stride=1)),\n",
        "                                       (nn.BatchNorm()),\n",
        "                                       (nn.relu())])) for i in range(self.num_branches)]\n",
        "        self.model = f_theta()\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        x = self.transform(x)\n",
        "        x_list = [x]\n",
        "        for i in range(self.num_branches):\n",
        "            bs, _, H, W = x_list[-1].shape\n",
        "            x_list.append(np.zeros(bs, self.channels[i], H//2, W//2))\n",
        "        z_list = [np.zeros(elem) for elem in x_list]\n",
        "\n",
        "        bsz = x.shape[0]\n",
        "        func = lambda z: self.model(z_list, x_list)\n",
        "        z_vec = jnp.cat([elem.reshape(bsz, -1, 1) for elem in z_list], dim=1)\n",
        "        result = self.solver(func, z_vec, threshold=0.001)\n",
        "        z_vec = result['result']\n",
        "        output = z_vec\n",
        "        if self.training:\n",
        "            output = func(z_vec.requires_grad_())\n",
        "        # jac_loss = jac_loss_estimate(output, z1) # comes from the follow-up paper\n",
        "        jac_loss = None\n",
        "        \n",
        "        y_list = output # TO DO -- for now without dropout!\n",
        "        return y_list, jac_loss"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    '''\n",
        "    extra thing: warm-up using gradient descent in pytorch code of official repo\n",
        "    --> check impact of that and maybe also cost etc (eg if only one layer etc)\n",
        "    '''\n",
        "\n",
        "    max_itr = 1000\n",
        "\n",
        "    def cross_entropy_loss(*, logits, labels):\n",
        "        one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "        return -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "\n",
        "    optimizer = optax.adamw(learning_rate=schedule, weight_decay=wd)\n",
        "    opt_state = optimizer.init(params)\n",
        "\n",
        "    def step(params, opt_state, x_batch, itr, r):\n",
        "        l_and_ls, grad = value_and_grad(loss, has_aux=True)(params, x_batch, itr, r)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, params)\n",
        "        params = optax.apply_updates(params, updates)\n",
        "        return params, opt_state, l_and_ls\n",
        "\n",
        "\n",
        "    data = loaded_data = stuff = None\n",
        "    solver_fn = broyden_jax\n",
        "    my_deq = MDEQModel(solver_fn=solver_fn)\n",
        "\n",
        "    loss = cross_entropy_loss\n",
        "\n",
        "    for itr in range(max_itr):\n",
        "        x_batch = next(g)\n",
        "        params, opt_state, loss_values = step(params, opt_state, x_batch, itr, r)\n",
        "        l, l_all = loss_values\n",
        "        l_all, r = l_all[:-1], l_all[-1]\n",
        "        \n",
        "        if itr % print_interval == 0:\n",
        "            print(f\"step {itr}, loss: {l}\", [l.item() for l in l_all])\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FQh6WBQmokr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breakdown of code overall:\n",
        "\n",
        "\n",
        "*   MDEQ modul\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "7RvQXU3CROAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}