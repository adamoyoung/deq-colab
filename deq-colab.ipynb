{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "69ab810d-5347-40f4-95c4-d31c989b77e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, z0, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False):\n",
        "    bsz, total_hsize = z0.shape\n",
        "    orig_shape = (bsz,total_hsize)\n",
        "    seq_len = 1\n",
        "    new_shape = (bsz,total_hsize,seq_len)\n",
        "    z0 = z0.reshape(*new_shape)\n",
        "    def g(_z):\n",
        "        # here it is safe to use x out of scope\n",
        "        return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(*new_shape)\n",
        "    dev = z0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    z_est = z0           # (bsz, 2d, L')\n",
        "    gz = g(z_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_zest, lowest_gz = 0, z_est, gz\n",
        "\n",
        "    while nstep < threshold:\n",
        "        z_est, gz, delta_z, delta_gz, ite = line_search_jax(update, z_est, gz, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gz)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gz + z_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_zest, lowest_gz = jnp.copy(z_est), jnp.copy(gz)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_z)\n",
        "        u = (delta_z - matvec_jax(part_Us, part_VTs, delta_gz)) / jnp.einsum('bij, bij -> b', vT, delta_gz)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    lowest_zest = lowest_zest.reshape(*orig_shape)\n",
        "    # print(\"broyden\",jnp.linalg.norm(z_est),jnp.linalg.norm(gz))\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_zest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_zest\n",
        "\n",
        "\n",
        "def newton_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    # note: f might ignore x0 (i.e. with backward pass)\n",
        "    orig_shape = z0.shape\n",
        "    def g(_z):\n",
        "      # this reshaping is to enable solving with Jacobian\n",
        "      return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(-1)\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    z = z0.reshape(-1)\n",
        "    gz = g(z)\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    nstep = 0\n",
        "\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      # solve system\n",
        "      jgz = jac_g(z)\n",
        "      # print(\"gz\",gz.shape,jnp.linalg.norm(gz))\n",
        "      # print(\"jgz\",jgz.shape,jnp.linalg.norm(jgz))\n",
        "      delta_z = jnp.linalg.solve(jgz,-gz)\n",
        "      # print(\"delta_z\",delta_z.shape,jnp.linalg.norm(delta_z))\n",
        "      z = z + delta_z\n",
        "      # need to compute gx here to decide whether to stop\n",
        "      gz = g(z)\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      nstep += 1\n",
        "\n",
        "    z = z.reshape(*orig_shape).astype(jnp.float32)\n",
        "\n",
        "    # assert False\n",
        "\n",
        "    return z\n",
        "\n",
        "def direct_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    nstep = 0\n",
        "    z_old = z0\n",
        "    z_new = f(z0,x0)\n",
        "    gz = z_new-z_old\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    min_gz_norm, min_z = gz_norm, z_new\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      z_old = z_new\n",
        "      z_new = f(z_old,x0)\n",
        "      gz = z_new-z_old\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      if gz_norm < min_gz_norm:\n",
        "        min_gz_norm, min_z = gz_norm, z_new\n",
        "      nstep += 1\n",
        "    # print(\"min_gz_norm\",min_gz_norm,\"nstep\",nstep)\n",
        "    return min_z\n"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3) \n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        # inject original input if resolution=0 \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        # residual\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EzcfTWz6agE_"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        \n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        conv_down = nn.Conv(features=self.in_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        relu_down = nn.relu\n",
        "\n",
        "        for n in range(num_samples-1):\n",
        "            down_block += [conv_down, group_down, relu_down]\n",
        "        conv_down = nn.Conv(features=self.out_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        down_block += [conv_down, group_down]\n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            img0, img1, img2, img3 = img.shape\n",
        "            new_shape = (img0, resize_rate*img1, resize_rate*img2, img3)\n",
        "            return jax.image.resize(img, new_shape, 'bilinear')\n",
        "        return upsampling_method\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_fn(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        raise ValueError\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps image to initial latent representation\n",
        "    AKA the grey part in the diagram\n",
        "    \"\"\"\n",
        "\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    training: bool = True\n",
        "\n",
        "    def setup(self):\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A tool for using the \n",
        "    \"\"\"\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=2\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[4, 8])\n",
        "    expansion: int = 2\n",
        "    final_chansize: int = 100\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def make_cls_block(self, in_chan, out_chan):\n",
        "          downsample = False\n",
        "          if in_chan != out_chan * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(in_chan, out_chan, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        combine_modules = []\n",
        "        for i  in range(len(self.channels)):\n",
        "            output_mod = self.make_cls_block(self.channels[i], self.output_channels[i])\n",
        "            combine_modules.append(output_mod)\n",
        "        self.combine_modules = combine_modules\n",
        "\n",
        "        self.final_layer_conv = nn.Conv(self.final_chansize, kernel_size=(1,1))\n",
        "        self.final_layer_bn = nn.BatchNorm()\n",
        "\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y):\n",
        "        y_final = self.combine_modules[0](y[0])\n",
        "        for i in range(len(self.channels)-1):\n",
        "            y_final = self.combine_modules[i+1](y[i+1]) \n",
        "        y_final = self.final_layer_bn(self.final_layer_conv(y_final), use_running_average=True)\n",
        "        y_final = nn.relu(y_final)\n",
        "        y_final = nn.avg_pool(y_final, window_shape=y_final.shape[1:3])\n",
        "        y_final = jnp.reshape(y_final, (y_final.shape[0], -1))\n",
        "        y_final = self.classifier(y_final)\n",
        "        return y_final"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 z: jnp.ndarray,\n",
        "                 x: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, z, x, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray,\n",
        "                      x: jnp.ndarray):\n",
        "    z = rootfind(solver_fn, f_fn, threshold, eps, weights, z, x)\n",
        "    # print(\"fwd residual\",jnp.linalg.norm(f_fn(weights,z,x)-z)/jnp.linalg.norm(z))\n",
        "    return z, (weights, z, x)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    weights, z, x = res\n",
        "    (_, vjp_fun) = jax.vjp(f_fn, weights, z, x)\n",
        "    def z_fn(z,x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "        return Jz_T + grad\n",
        "    z0 = jnp.zeros_like(grad)\n",
        "    x0 = None # dummy, z_fn does not use x\n",
        "    g = solver_fn(z_fn, z0, x0, threshold, eps)\n",
        "    #print(\"bwd residual\",jnp.linalg.norm(z_fn(g,x0)-g)/jnp.linalg.norm(g))\n",
        "    return (None, g, None)\n",
        "\n",
        "rootfind.defvjp(_rootfind_fwd, _rootfind_bwd)"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQFF(nn.Module):\n",
        "    \"\"\"\n",
        "    The f_{\\theta}(z,x) function that is repeatedly applied\n",
        "    AKA the yellow block in the diagram\n",
        "    \"\"\"\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "   \n",
        "    def setup(self):\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    array.append(sampled)\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  use_bias=False),\n",
        "                                           nn.relu,\n",
        "                                           nn.GroupNorm(num_groups=self.num_groups)]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, z, x, shape_tuple):\n",
        "        \n",
        "        batch_size = z.shape[0]\n",
        "        z_list = reshape_fn(z,shape_tuple)\n",
        "        x_list = reshape_fn(x,shape_tuple)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](z_list[i], i, x_list[i])) # z, branch, x\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "          # stick z back into into one vector\n",
        "        fuse_outputs = jnp.concatenate([fo.reshape(batch_size,-1) for fo in fuse_outputs],axis=1)\n",
        "        assert fuse_outputs.shape[1] == z.shape[1]\n",
        "        return fuse_outputs\n"
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdeq_inputs(x,num_branches):\n",
        "\n",
        "  batch_size = x.shape[0]\n",
        "  x_list = [x]\n",
        "  for i in range(1, num_branches):\n",
        "      bs, H, W, y = x_list[-1].shape\n",
        "      new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "      x_list.append(new_item)\n",
        "  z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "  shape_list = [el.shape for el in z_list]\n",
        "  # make them (batched) vectors\n",
        "  x_vec = jnp.concatenate([x.reshape(batch_size,-1) for x in x_list],axis=1)\n",
        "  z_vec = jnp.concatenate([z.reshape(batch_size,-1) for z in z_list],axis=1)\n",
        "  # i'm not sure if tuple is actually important but I like it for non-mutability\n",
        "  shape_tuple = tuple(shape_list)\n",
        "  return x_vec, z_vec, shape_tuple\n",
        "\n",
        "\n",
        "def mdeq_fn(x,encoder,decoder,deqff,all_weights,solver_fn=None,mode='broyden'):\n",
        "    \n",
        "    encoder_weights = all_weights[\"encoder\"]\n",
        "    decoder_weights = all_weights[\"decoder\"]\n",
        "    deqff_weights = all_weights[\"mdeqff\"]\n",
        "    batch_size = x.shape[0]\n",
        "    # transform the input image\n",
        "    x = encoder.apply(encoder_weights,x)\n",
        "    # construct inputs (lots of padding and concatenation)\n",
        "    x, z, shape_tuple = create_mdeq_inputs(x,deqff.num_branches)\n",
        "    \n",
        "    if mode == \"implicit\":\n",
        "        # solve a fixed point equation for fwd/bwd passes\n",
        "        threshold = 7\n",
        "        eps = 1e-3\n",
        "        # the root function can only take 3 ndarrays as input\n",
        "        def deqff_root(_weights,_z,_x):\n",
        "          # note: it's safe to pass the shape_tuple here (no tracers)\n",
        "          return deqff.apply(_weights,_z,_x,shape_tuple)\n",
        "        # apply rootfinder with custom vjp\n",
        "        z = rootfind(solver_fn,deqff_root,threshold,eps,deqff_weights,z,x)\n",
        "  \n",
        "    elif mode == \"non_implicit\":\n",
        "        # use direct solver, backpropagate through the steps\n",
        "        threshold = 5\n",
        "        eps=1e-2\n",
        "        evals = 0\n",
        "        z_old = z\n",
        "        z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "        #residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "        residual = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(z_new-z_old, axis=1) / (jnp.linalg.norm(z_old, axis=1)+1e-9),axis=0))\n",
        "\n",
        "        while evals < threshold and residual > eps:\n",
        "            z_old = z_new\n",
        "            z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "            #residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "            residual = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(z_new-z_old, axis=1) / (jnp.linalg.norm(z_old, axis=1)+1e-9),axis=0))\n",
        "\n",
        "            evals += 1\n",
        "        z = z_new\n",
        "      \n",
        "    elif mode == \"warmup\":\n",
        "        # fixed number of function applications\n",
        "        threshold = 1 #5\n",
        "        for i in range(threshold):\n",
        "            z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "\n",
        "    z_list = reshape_fn(z,shape_tuple)\n",
        "    log_probs = decoder.apply(decoder_weights,z_list)\n",
        "\n",
        "    # this evaluation is just for the purposes of calculating residual\n",
        "    # we don't want any gradient stuff\n",
        "    f_z = jax.lax.stop_gradient(deqff.apply(deqff_weights,z,x,shape_tuple))\n",
        "    residuals = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(f_z-z, axis=1) / (jnp.linalg.norm(z, axis=1)+1e-9),axis=0))\n",
        "    \n",
        "    return log_probs, residuals"
      ],
      "metadata": {
        "id": "5hI7s8DE1kqX"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(images,labels,encoder,decoder,deqff,all_weights,solver,mode):\n",
        "    loss, acc = 0,0\n",
        "    batch_size = 128\n",
        "    start, end = 0, 0\n",
        "    loss_vals, acc_vals, res_vals = [], [], []\n",
        "\n",
        "    while end < images.shape[0]:\n",
        "        end = min(start+batch_size, images.shape[0])\n",
        "        x_batch = images[start:end]\n",
        "\n",
        "        y_true = labels[start:end]\n",
        "        start = end\n",
        "\n",
        "        log_probs, residual = jax.lax.stop_gradient(mdeq_fn(x_batch,encoder,decoder,deqff,all_weights,solver_fn=solver,mode=mode))\n",
        "        loss, acc = cross_entropy_loss(log_probs, y_true)\n",
        "        loss_vals.append(loss * x_batch.shape[0])\n",
        "        acc_vals.append(acc * x_batch.shape[0])\n",
        "        res_vals.append(residual * x_batch.shape[0])\n",
        "    return sum(jnp.array(loss_vals)) / images.shape[0], sum(jnp.array(acc_vals)) / images.shape[0], sum(jnp.array(res_vals)) / images.shape[0]"
      ],
      "metadata": {
        "id": "1kUlVk4z0LOV"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:2500], train_ds.targets[:2500])\n",
        "    test_images, test_labels = transform(test_ds.data[:250], test_ds.targets[:250])\n",
        "\n",
        "    print(f\"NUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"NUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(mode=\"implicit\",solver=\"broyden\"):\n",
        "\n",
        "    assert mode in [\"implicit\", \"non_implicit\", \"warmup\"], \"INCORRECT MODE\"\n",
        "    assert solver in [\"direct\",\"broyden\"]\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    image_size = train_images.shape[1]\n",
        "    batch_size = 128\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    if solver == \"broyden\":\n",
        "        solver_fn = broyden_jax\n",
        "    elif solver == \"direct\":\n",
        "        solver_fn = direct_jax\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    num_groups = 2\n",
        "    channels = [6, 6] # [24,24]\n",
        "    num_branches = 2\n",
        "\n",
        "    # instantiation\n",
        "    encoder = Encoder(channels=channels)\n",
        "    decoder = Classifier(channels=channels,) # not sure about what to pass\n",
        "    mdeqff = MDEQFF(num_branches=num_branches, channels=channels, num_groups=num_groups)\n",
        "\n",
        "    # weight initialization\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng, _ = jax.random.split(prng, 2)\n",
        "    x_dummy = jnp.ones((batch_size, image_size, image_size, 1))\n",
        "    x_dummy_2, encoder_weights = encoder.init_with_output(prng,x_dummy)\n",
        "    x_dummy_3, z_dummy, shape_tuple = create_mdeq_inputs(x_dummy_2,num_branches)\n",
        "    z_dummy_2, mdeqff_weights = mdeqff.init_with_output(prng,z_dummy,x_dummy_3,shape_tuple)\n",
        "    z_dummy_3 = reshape_fn(z_dummy_2,shape_tuple)\n",
        "    o_dummy, classifier_weights = decoder.init_with_output(prng,z_dummy_3)\n",
        "\n",
        "    # collect weights\n",
        "    weights = {'encoder': encoder_weights, 'mdeqff': mdeqff_weights ,'decoder': classifier_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true, mode):\n",
        "        logits, residual = mdeq_fn(x_batch,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "        loss, acc = loss_fn(logits, y_true)\n",
        "        return loss, (acc, residual)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true, mode):\n",
        "        (loss_vals, (acc, residual)), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true, mode)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc, residual\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler(seed):\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "        return shuffled_indices\n",
        "    \n",
        "    max_epoch = 10\n",
        "    warmup_epochs = 0\n",
        "    if mode == \"warmup\":\n",
        "      warmup_epochs = 3\n",
        "      max_epoch += warmup_epochs\n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    if mode == \"warmup\":\n",
        "        print(f\"TRAINING MODE:::{mode} (then implicit)\")\n",
        "    else:\n",
        "        print(f\"TRAINING MODE:::{mode}\")\n",
        "    train_loss_vals, val_loss_vals = [], []\n",
        "    train_acc_vals, val_acc_vals = [], []\n",
        "    train_res_vals, val_res_vals = [], []\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        if mode == \"warmup\":\n",
        "          if epoch >= warmup_epochs:\n",
        "            mode = \"implicit\"\n",
        "            print(\"-------------DONE WARM UP---------------\")\n",
        "          elif epoch == 0:\n",
        "            print(\"----------STARTING WARM UP--------------\")\n",
        "            \n",
        "        idxs = list_shuffler(epoch)\n",
        "        start, end = 0, 0\n",
        "\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        res_vals = []\n",
        "\n",
        "        counter = 0\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab,...]\n",
        "            #x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "            weights, opt_state, batch_loss, batch_acc, batch_res = step(weights=weights,\n",
        "                                                                        opt_state=opt_state,\n",
        "                                                                        x_batch=x_batch,\n",
        "                                                                        y_true=y_true,\n",
        "                                                                        mode=mode)\n",
        "            \n",
        "            counter += 1\n",
        "            loss_vals.append(batch_loss * x_batch.shape[0])\n",
        "            acc_vals.append(batch_acc * x_batch.shape[0])\n",
        "            res_vals.append(batch_res * x_batch.shape[0])\n",
        "\n",
        "\n",
        "            print(f\"> epoch {epoch+1}, batch {counter} :: batch_loss = {batch_loss}, batch_acc = {batch_acc}, batch_res = {batch_res} \")\n",
        "\n",
        "        epoch_loss = sum(jnp.array(loss_vals)) / len(idxs)\n",
        "        epoch_acc = sum(jnp.array(acc_vals)) / len(idxs)\n",
        "        epoch_res = sum(jnp.array(res_vals)) / len(idxs)\n",
        "        \n",
        "        train_loss_vals.append(epoch_loss)\n",
        "        train_acc_vals.append(epoch_acc)\n",
        "        train_res_vals.append(epoch_res)\n",
        "\n",
        "        val_loss, val_acc, val_res = predict(test_images,test_labels,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "\n",
        "        val_loss_vals.append(val_loss)\n",
        "        val_acc_vals.append(val_acc)\n",
        "        val_res_vals.append(val_res)\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(f\"\\tTRAIN epoch = {epoch+1} / loss = {epoch_loss} / acc = {epoch_acc} / res = {epoch_res}\")\n",
        "            print(f\"\\tVAL epoch = {epoch+1} / loss = {val_loss} / acc = {val_acc} / res = {val_res}\")\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss)\n",
        "    results = {'train_loss_vals': train_loss_vals,\n",
        "               'train_acc_vals': train_acc_vals,\n",
        "               'train_res_vals': train_res_vals,\n",
        "               'val_loss_vals': val_loss_vals,\n",
        "               'val_acc_vals': val_acc_vals,\n",
        "               'val_res_vals': val_res_vals}\n",
        "    return results"
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------------------------------\n",
        "# Method #1: MDEQ trained with implicit differentiation using Broyden solver"
      ],
      "metadata": {
        "id": "YsYagqwzL_SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "broyden_results = train(mode=\"implicit\", solver='broyden')"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf710f8a-cf52-4ffa-bdd0-27e989583e06"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM TRAINING IMAGES:::2500\n",
            "NUM TEST IMAGES:::250\n",
            "TRAINING MODE:::implicit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> epoch 1, batch 1 :: batch_loss = 2.331523895263672, batch_acc = 0.1015625, batch_res = 0.40893012285232544 \n",
            "> epoch 1, batch 2 :: batch_loss = 2.3362154960632324, batch_acc = 0.109375, batch_res = 0.4048537611961365 \n",
            "> epoch 1, batch 3 :: batch_loss = 2.3335423469543457, batch_acc = 0.109375, batch_res = 0.4121779203414917 \n",
            "> epoch 1, batch 4 :: batch_loss = 2.336082935333252, batch_acc = 0.09375, batch_res = 0.4122615456581116 \n",
            "> epoch 1, batch 5 :: batch_loss = 2.318230152130127, batch_acc = 0.0625, batch_res = 0.4109991192817688 \n",
            "> epoch 1, batch 6 :: batch_loss = 2.310617208480835, batch_acc = 0.046875, batch_res = 0.40419870615005493 \n",
            "> epoch 1, batch 7 :: batch_loss = 2.3210527896881104, batch_acc = 0.1015625, batch_res = 0.3983643054962158 \n",
            "> epoch 1, batch 8 :: batch_loss = 2.3156306743621826, batch_acc = 0.09375, batch_res = 0.4134972095489502 \n",
            "> epoch 1, batch 9 :: batch_loss = 2.3296897411346436, batch_acc = 0.0859375, batch_res = 0.40927815437316895 \n",
            "> epoch 1, batch 10 :: batch_loss = 2.3104562759399414, batch_acc = 0.109375, batch_res = 0.407916396856308 \n",
            "> epoch 1, batch 11 :: batch_loss = 2.293388843536377, batch_acc = 0.09375, batch_res = 0.4022984504699707 \n",
            "> epoch 1, batch 12 :: batch_loss = 2.321337938308716, batch_acc = 0.078125, batch_res = 0.40767550468444824 \n",
            "> epoch 1, batch 13 :: batch_loss = 2.3013405799865723, batch_acc = 0.15625, batch_res = 0.41805845499038696 \n",
            "> epoch 1, batch 14 :: batch_loss = 2.3074240684509277, batch_acc = 0.125, batch_res = 0.40427398681640625 \n",
            "> epoch 1, batch 15 :: batch_loss = 2.291537046432495, batch_acc = 0.1015625, batch_res = 0.4020276665687561 \n",
            "> epoch 1, batch 16 :: batch_loss = 2.302757978439331, batch_acc = 0.0859375, batch_res = 0.40536147356033325 \n",
            "> epoch 1, batch 17 :: batch_loss = 2.2983322143554688, batch_acc = 0.1015625, batch_res = 0.41299277544021606 \n",
            "> epoch 1, batch 18 :: batch_loss = 2.3222126960754395, batch_acc = 0.078125, batch_res = 0.4083573818206787 \n",
            "> epoch 1, batch 19 :: batch_loss = 2.315372943878174, batch_acc = 0.0625, batch_res = 0.4066130816936493 \n",
            "> epoch 1, batch 20 :: batch_loss = 2.2828047275543213, batch_acc = 0.1617647111415863, batch_res = 0.3965241014957428 \n",
            "\tTRAIN epoch = 1 / loss = 2.314725637435913 / acc = 0.09640000015497208 / res = 0.40759241580963135\n",
            "\tVAL epoch = 1 / loss = 2.285099506378174 / acc = 0.13600000739097595 / res = 0.4156922399997711\n",
            "> epoch 2, batch 1 :: batch_loss = 2.293611526489258, batch_acc = 0.1328125, batch_res = 0.41017448902130127 \n",
            "> epoch 2, batch 2 :: batch_loss = 2.313464641571045, batch_acc = 0.109375, batch_res = 0.4154183864593506 \n",
            "> epoch 2, batch 3 :: batch_loss = 2.29610013961792, batch_acc = 0.1171875, batch_res = 0.4145811200141907 \n",
            "> epoch 2, batch 4 :: batch_loss = 2.317748546600342, batch_acc = 0.078125, batch_res = 0.40228354930877686 \n",
            "> epoch 2, batch 5 :: batch_loss = 2.283944606781006, batch_acc = 0.1171875, batch_res = 0.40115082263946533 \n",
            "> epoch 2, batch 6 :: batch_loss = 2.295957565307617, batch_acc = 0.109375, batch_res = 0.4186086654663086 \n",
            "> epoch 2, batch 7 :: batch_loss = 2.2827811241149902, batch_acc = 0.125, batch_res = 0.40763992071151733 \n",
            "> epoch 2, batch 8 :: batch_loss = 2.294522285461426, batch_acc = 0.1640625, batch_res = 0.40549758076667786 \n",
            "> epoch 2, batch 9 :: batch_loss = 2.299649238586426, batch_acc = 0.0859375, batch_res = 0.4108012914657593 \n",
            "> epoch 2, batch 10 :: batch_loss = 2.2902355194091797, batch_acc = 0.09375, batch_res = 0.4037777781486511 \n",
            "> epoch 2, batch 11 :: batch_loss = 2.2950081825256348, batch_acc = 0.1171875, batch_res = 0.4039219915866852 \n",
            "> epoch 2, batch 12 :: batch_loss = 2.296497106552124, batch_acc = 0.125, batch_res = 0.4023701548576355 \n",
            "> epoch 2, batch 13 :: batch_loss = 2.2911062240600586, batch_acc = 0.140625, batch_res = 0.41521042585372925 \n",
            "> epoch 2, batch 14 :: batch_loss = 2.311640739440918, batch_acc = 0.078125, batch_res = 0.4081251323223114 \n",
            "> epoch 2, batch 15 :: batch_loss = 2.288606643676758, batch_acc = 0.1015625, batch_res = 0.40514054894447327 \n",
            "> epoch 2, batch 16 :: batch_loss = 2.3028950691223145, batch_acc = 0.1171875, batch_res = 0.4042719304561615 \n",
            "> epoch 2, batch 17 :: batch_loss = 2.3038413524627686, batch_acc = 0.1171875, batch_res = 0.4069708287715912 \n",
            "> epoch 2, batch 18 :: batch_loss = 2.2988030910491943, batch_acc = 0.15625, batch_res = 0.4070226550102234 \n",
            "> epoch 2, batch 19 :: batch_loss = 2.302154779434204, batch_acc = 0.140625, batch_res = 0.4052497446537018 \n",
            "> epoch 2, batch 20 :: batch_loss = 2.2898924350738525, batch_acc = 0.1764705926179886, batch_res = 0.40013638138771057 \n",
            "\tTRAIN epoch = 2 / loss = 2.2976036071777344 / acc = 0.11879999935626984 / res = 0.40759238600730896\n",
            "\tVAL epoch = 2 / loss = 2.2841901779174805 / acc = 0.23199999332427979 / res = 0.4156922399997711\n",
            "> epoch 3, batch 1 :: batch_loss = 2.2879066467285156, batch_acc = 0.1640625, batch_res = 0.4070890545845032 \n",
            "> epoch 3, batch 2 :: batch_loss = 2.2856154441833496, batch_acc = 0.1953125, batch_res = 0.4070841073989868 \n",
            "> epoch 3, batch 3 :: batch_loss = 2.3024535179138184, batch_acc = 0.171875, batch_res = 0.40825462341308594 \n",
            "> epoch 3, batch 4 :: batch_loss = 2.291566848754883, batch_acc = 0.2109375, batch_res = 0.4072493314743042 \n",
            "> epoch 3, batch 5 :: batch_loss = 2.288372039794922, batch_acc = 0.1875, batch_res = 0.41011250019073486 \n",
            "> epoch 3, batch 6 :: batch_loss = 2.283719062805176, batch_acc = 0.2109375, batch_res = 0.40833359956741333 \n",
            "> epoch 3, batch 7 :: batch_loss = 2.284064769744873, batch_acc = 0.203125, batch_res = 0.4152839183807373 \n",
            "> epoch 3, batch 8 :: batch_loss = 2.2994070053100586, batch_acc = 0.1640625, batch_res = 0.41101670265197754 \n",
            "> epoch 3, batch 9 :: batch_loss = 2.2916579246520996, batch_acc = 0.15625, batch_res = 0.40454381704330444 \n",
            "> epoch 3, batch 10 :: batch_loss = 2.2866835594177246, batch_acc = 0.1640625, batch_res = 0.4115222692489624 \n",
            "> epoch 3, batch 11 :: batch_loss = 2.2937746047973633, batch_acc = 0.1171875, batch_res = 0.39781564474105835 \n",
            "> epoch 3, batch 12 :: batch_loss = 2.3015847206115723, batch_acc = 0.0625, batch_res = 0.40645456314086914 \n",
            "> epoch 3, batch 13 :: batch_loss = 2.2916746139526367, batch_acc = 0.1015625, batch_res = 0.4057712256908417 \n",
            "> epoch 3, batch 14 :: batch_loss = 2.2830259799957275, batch_acc = 0.15625, batch_res = 0.4101431965827942 \n",
            "> epoch 3, batch 15 :: batch_loss = 2.2930872440338135, batch_acc = 0.109375, batch_res = 0.40656447410583496 \n",
            "> epoch 3, batch 16 :: batch_loss = 2.29935359954834, batch_acc = 0.1171875, batch_res = 0.3990061581134796 \n",
            "> epoch 3, batch 17 :: batch_loss = 2.282133102416992, batch_acc = 0.21875, batch_res = 0.41426658630371094 \n",
            "> epoch 3, batch 18 :: batch_loss = 2.2865381240844727, batch_acc = 0.15625, batch_res = 0.3975740075111389 \n",
            "> epoch 3, batch 19 :: batch_loss = 2.2845516204833984, batch_acc = 0.140625, batch_res = 0.41336750984191895 \n",
            "> epoch 3, batch 20 :: batch_loss = 2.2864415645599365, batch_acc = 0.1617647111415863, batch_res = 0.4128679931163788 \n",
            "\tTRAIN epoch = 3 / loss = 2.2902705669403076 / acc = 0.15839999914169312 / res = 0.40759241580963135\n",
            "\tVAL epoch = 3 / loss = 2.2810752391815186 / acc = 0.164000004529953 / res = 0.4156922399997711\n",
            "> epoch 4, batch 1 :: batch_loss = 2.2766780853271484, batch_acc = 0.2109375, batch_res = 0.4069104790687561 \n",
            "> epoch 4, batch 2 :: batch_loss = 2.292850971221924, batch_acc = 0.1171875, batch_res = 0.41148051619529724 \n",
            "> epoch 4, batch 3 :: batch_loss = 2.296544075012207, batch_acc = 0.15625, batch_res = 0.40162393450737 \n",
            "> epoch 4, batch 4 :: batch_loss = 2.2867815494537354, batch_acc = 0.1953125, batch_res = 0.40849220752716064 \n",
            "> epoch 4, batch 5 :: batch_loss = 2.291720390319824, batch_acc = 0.1484375, batch_res = 0.4027068018913269 \n",
            "> epoch 4, batch 6 :: batch_loss = 2.2775962352752686, batch_acc = 0.234375, batch_res = 0.40759146213531494 \n",
            "> epoch 4, batch 7 :: batch_loss = 2.279737949371338, batch_acc = 0.1875, batch_res = 0.4041091799736023 \n",
            "> epoch 4, batch 8 :: batch_loss = 2.2843637466430664, batch_acc = 0.2265625, batch_res = 0.40768423676490784 \n",
            "> epoch 4, batch 9 :: batch_loss = 2.292757987976074, batch_acc = 0.1796875, batch_res = 0.40810081362724304 \n",
            "> epoch 4, batch 10 :: batch_loss = 2.2758243083953857, batch_acc = 0.2421875, batch_res = 0.4064791202545166 \n",
            "> epoch 4, batch 11 :: batch_loss = 2.280412197113037, batch_acc = 0.2265625, batch_res = 0.4043521285057068 \n",
            "> epoch 4, batch 12 :: batch_loss = 2.274265766143799, batch_acc = 0.234375, batch_res = 0.4091337323188782 \n",
            "> epoch 4, batch 13 :: batch_loss = 2.2851340770721436, batch_acc = 0.171875, batch_res = 0.4114585518836975 \n",
            "> epoch 4, batch 14 :: batch_loss = 2.2771639823913574, batch_acc = 0.171875, batch_res = 0.4136350452899933 \n",
            "> epoch 4, batch 15 :: batch_loss = 2.2811930179595947, batch_acc = 0.1171875, batch_res = 0.4141034483909607 \n",
            "> epoch 4, batch 16 :: batch_loss = 2.271481513977051, batch_acc = 0.140625, batch_res = 0.4066978096961975 \n",
            "> epoch 4, batch 17 :: batch_loss = 2.290696382522583, batch_acc = 0.125, batch_res = 0.4026704430580139 \n",
            "> epoch 4, batch 18 :: batch_loss = 2.297534942626953, batch_acc = 0.0390625, batch_res = 0.40752649307250977 \n",
            "> epoch 4, batch 19 :: batch_loss = 2.2857699394226074, batch_acc = 0.1171875, batch_res = 0.40621939301490784 \n",
            "> epoch 4, batch 20 :: batch_loss = 2.268578290939331, batch_acc = 0.20588235557079315, batch_res = 0.41376692056655884 \n",
            "\tTRAIN epoch = 4 / loss = 2.2837088108062744 / acc = 0.17159999907016754 / res = 0.40759244561195374\n",
            "\tVAL epoch = 4 / loss = 2.2659857273101807 / acc = 0.21199999749660492 / res = 0.4156922399997711\n",
            "> epoch 5, batch 1 :: batch_loss = 2.286695957183838, batch_acc = 0.109375, batch_res = 0.41248375177383423 \n",
            "> epoch 5, batch 2 :: batch_loss = 2.2772679328918457, batch_acc = 0.1484375, batch_res = 0.40284550189971924 \n",
            "> epoch 5, batch 3 :: batch_loss = 2.279144763946533, batch_acc = 0.09375, batch_res = 0.3990287780761719 \n",
            "> epoch 5, batch 4 :: batch_loss = 2.2677323818206787, batch_acc = 0.1953125, batch_res = 0.4081822633743286 \n",
            "> epoch 5, batch 5 :: batch_loss = 2.2739248275756836, batch_acc = 0.2578125, batch_res = 0.41128695011138916 \n",
            "> epoch 5, batch 6 :: batch_loss = 2.2773420810699463, batch_acc = 0.1796875, batch_res = 0.41011449694633484 \n",
            "> epoch 5, batch 7 :: batch_loss = 2.28682279586792, batch_acc = 0.1640625, batch_res = 0.4069986641407013 \n",
            "> epoch 5, batch 8 :: batch_loss = 2.2773852348327637, batch_acc = 0.1875, batch_res = 0.4082585871219635 \n",
            "> epoch 5, batch 9 :: batch_loss = 2.291111946105957, batch_acc = 0.171875, batch_res = 0.4069215655326843 \n",
            "> epoch 5, batch 10 :: batch_loss = 2.2753124237060547, batch_acc = 0.1796875, batch_res = 0.4105505347251892 \n",
            "> epoch 5, batch 11 :: batch_loss = 2.265063762664795, batch_acc = 0.2265625, batch_res = 0.40876805782318115 \n",
            "> epoch 5, batch 12 :: batch_loss = 2.257218837738037, batch_acc = 0.21875, batch_res = 0.40492236614227295 \n",
            "> epoch 5, batch 13 :: batch_loss = 2.2800421714782715, batch_acc = 0.1640625, batch_res = 0.41545015573501587 \n",
            "> epoch 5, batch 14 :: batch_loss = 2.2789974212646484, batch_acc = 0.1484375, batch_res = 0.4012776017189026 \n",
            "> epoch 5, batch 15 :: batch_loss = 2.260634183883667, batch_acc = 0.21875, batch_res = 0.4034258723258972 \n",
            "> epoch 5, batch 16 :: batch_loss = 2.2546520233154297, batch_acc = 0.2890625, batch_res = 0.4071970582008362 \n",
            "> epoch 5, batch 17 :: batch_loss = 2.269265651702881, batch_acc = 0.203125, batch_res = 0.4146891236305237 \n",
            "> epoch 5, batch 18 :: batch_loss = 2.2696492671966553, batch_acc = 0.1640625, batch_res = 0.4050014615058899 \n",
            "> epoch 5, batch 19 :: batch_loss = 2.261199474334717, batch_acc = 0.1796875, batch_res = 0.40730953216552734 \n",
            "> epoch 5, batch 20 :: batch_loss = 2.264105796813965, batch_acc = 0.10294117778539658, batch_res = 0.40673360228538513 \n",
            "\tTRAIN epoch = 5 / loss = 2.2728841304779053 / acc = 0.18199999630451202 / res = 0.40759244561195374\n",
            "\tVAL epoch = 5 / loss = 2.248532295227051 / acc = 0.1679999977350235 / res = 0.4156922399997711\n",
            "> epoch 6, batch 1 :: batch_loss = 2.2522244453430176, batch_acc = 0.15625, batch_res = 0.4066685438156128 \n",
            "> epoch 6, batch 2 :: batch_loss = 2.2637572288513184, batch_acc = 0.1328125, batch_res = 0.4110179841518402 \n",
            "> epoch 6, batch 3 :: batch_loss = 2.278533935546875, batch_acc = 0.0859375, batch_res = 0.40378686785697937 \n",
            "> epoch 6, batch 4 :: batch_loss = 2.244060516357422, batch_acc = 0.15625, batch_res = 0.41001594066619873 \n",
            "> epoch 6, batch 5 :: batch_loss = 2.242518424987793, batch_acc = 0.1796875, batch_res = 0.41190671920776367 \n",
            "> epoch 6, batch 6 :: batch_loss = 2.274628162384033, batch_acc = 0.1015625, batch_res = 0.4056569039821625 \n",
            "> epoch 6, batch 7 :: batch_loss = 2.256348133087158, batch_acc = 0.15625, batch_res = 0.40350836515426636 \n",
            "> epoch 6, batch 8 :: batch_loss = 2.2487640380859375, batch_acc = 0.1484375, batch_res = 0.4100440442562103 \n",
            "> epoch 6, batch 9 :: batch_loss = 2.2743077278137207, batch_acc = 0.1640625, batch_res = 0.4034567177295685 \n",
            "> epoch 6, batch 10 :: batch_loss = 2.2365646362304688, batch_acc = 0.2109375, batch_res = 0.4029291868209839 \n",
            "> epoch 6, batch 11 :: batch_loss = 2.2476658821105957, batch_acc = 0.1875, batch_res = 0.4032590389251709 \n",
            "> epoch 6, batch 12 :: batch_loss = 2.2495031356811523, batch_acc = 0.203125, batch_res = 0.40423262119293213 \n",
            "> epoch 6, batch 13 :: batch_loss = 2.2707431316375732, batch_acc = 0.109375, batch_res = 0.40517598390579224 \n",
            "> epoch 6, batch 14 :: batch_loss = 2.238715410232544, batch_acc = 0.203125, batch_res = 0.4134843051433563 \n",
            "> epoch 6, batch 15 :: batch_loss = 2.2374305725097656, batch_acc = 0.171875, batch_res = 0.41171425580978394 \n",
            "> epoch 6, batch 16 :: batch_loss = 2.246929407119751, batch_acc = 0.1953125, batch_res = 0.4100418984889984 \n",
            "> epoch 6, batch 17 :: batch_loss = 2.2422361373901367, batch_acc = 0.15625, batch_res = 0.4125044643878937 \n",
            "> epoch 6, batch 18 :: batch_loss = 2.2302184104919434, batch_acc = 0.125, batch_res = 0.40557458996772766 \n",
            "> epoch 6, batch 19 :: batch_loss = 2.2349674701690674, batch_acc = 0.1484375, batch_res = 0.40703827142715454 \n",
            "> epoch 6, batch 20 :: batch_loss = 2.235058546066284, batch_acc = 0.20588235557079315, batch_res = 0.4118075370788574 \n",
            "\tTRAIN epoch = 6 / loss = 2.2506234645843506 / acc = 0.15880000591278076 / res = 0.40759241580963135\n",
            "\tVAL epoch = 6 / loss = 2.223123073577881 / acc = 0.2160000056028366 / res = 0.4156922399997711\n",
            "> epoch 7, batch 1 :: batch_loss = 2.236292600631714, batch_acc = 0.1640625, batch_res = 0.407766729593277 \n",
            "> epoch 7, batch 2 :: batch_loss = 2.222184419631958, batch_acc = 0.1953125, batch_res = 0.40555307269096375 \n",
            "> epoch 7, batch 3 :: batch_loss = 2.2205657958984375, batch_acc = 0.1875, batch_res = 0.4129555821418762 \n",
            "> epoch 7, batch 4 :: batch_loss = 2.2406163215637207, batch_acc = 0.109375, batch_res = 0.4007813632488251 \n",
            "> epoch 7, batch 5 :: batch_loss = 2.2187814712524414, batch_acc = 0.203125, batch_res = 0.4139729142189026 \n",
            "> epoch 7, batch 6 :: batch_loss = 2.2493021488189697, batch_acc = 0.1484375, batch_res = 0.41468629240989685 \n",
            "> epoch 7, batch 7 :: batch_loss = 2.2095565795898438, batch_acc = 0.25, batch_res = 0.40451210737228394 \n",
            "> epoch 7, batch 8 :: batch_loss = 2.2243714332580566, batch_acc = 0.1953125, batch_res = 0.4105771780014038 \n",
            "> epoch 7, batch 9 :: batch_loss = 2.2177109718322754, batch_acc = 0.2578125, batch_res = 0.40936505794525146 \n",
            "> epoch 7, batch 10 :: batch_loss = 2.209689140319824, batch_acc = 0.25, batch_res = 0.4029913544654846 \n",
            "> epoch 7, batch 11 :: batch_loss = 2.209007501602173, batch_acc = 0.2734375, batch_res = 0.40282678604125977 \n",
            "> epoch 7, batch 12 :: batch_loss = 2.2255070209503174, batch_acc = 0.1640625, batch_res = 0.40907126665115356 \n",
            "> epoch 7, batch 13 :: batch_loss = 2.191422939300537, batch_acc = 0.2890625, batch_res = 0.40226471424102783 \n",
            "> epoch 7, batch 14 :: batch_loss = 2.191596031188965, batch_acc = 0.2578125, batch_res = 0.41125163435935974 \n",
            "> epoch 7, batch 15 :: batch_loss = 2.2029051780700684, batch_acc = 0.234375, batch_res = 0.4046894311904907 \n",
            "> epoch 7, batch 16 :: batch_loss = 2.175577402114868, batch_acc = 0.25, batch_res = 0.4079165756702423 \n",
            "> epoch 7, batch 17 :: batch_loss = 2.2004103660583496, batch_acc = 0.1953125, batch_res = 0.4110126495361328 \n",
            "> epoch 7, batch 18 :: batch_loss = 2.182734966278076, batch_acc = 0.2265625, batch_res = 0.40316203236579895 \n",
            "> epoch 7, batch 19 :: batch_loss = 2.198904037475586, batch_acc = 0.1640625, batch_res = 0.41150519251823425 \n",
            "> epoch 7, batch 20 :: batch_loss = 2.097722291946411, batch_acc = 0.25, batch_res = 0.40268731117248535 \n",
            "\tTRAIN epoch = 7 / loss = 2.2088475227355957 / acc = 0.21240000426769257 / res = 0.40759244561195374\n",
            "\tVAL epoch = 7 / loss = 2.1568009853363037 / acc = 0.19200000166893005 / res = 0.4156922399997711\n",
            "> epoch 8, batch 1 :: batch_loss = 2.176072120666504, batch_acc = 0.1796875, batch_res = 0.40077275037765503 \n",
            "> epoch 8, batch 2 :: batch_loss = 2.174802780151367, batch_acc = 0.15625, batch_res = 0.4098174273967743 \n",
            "> epoch 8, batch 3 :: batch_loss = 2.18900465965271, batch_acc = 0.1953125, batch_res = 0.40798449516296387 \n",
            "> epoch 8, batch 4 :: batch_loss = 2.1986236572265625, batch_acc = 0.1484375, batch_res = 0.4171977639198303 \n",
            "> epoch 8, batch 5 :: batch_loss = 2.1795144081115723, batch_acc = 0.1796875, batch_res = 0.4178342819213867 \n",
            "> epoch 8, batch 6 :: batch_loss = 2.1868577003479004, batch_acc = 0.234375, batch_res = 0.4075812101364136 \n",
            "> epoch 8, batch 7 :: batch_loss = 2.146427631378174, batch_acc = 0.2578125, batch_res = 0.40086010098457336 \n",
            "> epoch 8, batch 8 :: batch_loss = 2.1458420753479004, batch_acc = 0.21875, batch_res = 0.4168342053890228 \n",
            "> epoch 8, batch 9 :: batch_loss = 2.1609413623809814, batch_acc = 0.171875, batch_res = 0.40382149815559387 \n",
            "> epoch 8, batch 10 :: batch_loss = 2.1278576850891113, batch_acc = 0.2734375, batch_res = 0.4070608615875244 \n",
            "> epoch 8, batch 11 :: batch_loss = 2.1547794342041016, batch_acc = 0.234375, batch_res = 0.4009935259819031 \n",
            "> epoch 8, batch 12 :: batch_loss = 2.1202948093414307, batch_acc = 0.2578125, batch_res = 0.40721648931503296 \n",
            "> epoch 8, batch 13 :: batch_loss = 2.0949182510375977, batch_acc = 0.234375, batch_res = 0.40520161390304565 \n",
            "> epoch 8, batch 14 :: batch_loss = 2.1081442832946777, batch_acc = 0.203125, batch_res = 0.40714403986930847 \n",
            "> epoch 8, batch 15 :: batch_loss = 2.095794916152954, batch_acc = 0.234375, batch_res = 0.4091310203075409 \n",
            "> epoch 8, batch 16 :: batch_loss = 2.1150872707366943, batch_acc = 0.171875, batch_res = 0.40672385692596436 \n",
            "> epoch 8, batch 17 :: batch_loss = 2.1485557556152344, batch_acc = 0.140625, batch_res = 0.40365833044052124 \n",
            "> epoch 8, batch 18 :: batch_loss = 2.039656162261963, batch_acc = 0.2109375, batch_res = 0.4011746644973755 \n",
            "> epoch 8, batch 19 :: batch_loss = 2.0733821392059326, batch_acc = 0.2265625, batch_res = 0.40953049063682556 \n",
            "> epoch 8, batch 20 :: batch_loss = 2.1419198513031006, batch_acc = 0.25, batch_res = 0.4145898222923279 \n",
            "\tTRAIN epoch = 8 / loss = 2.1388518810272217 / acc = 0.20800000429153442 / res = 0.40759244561195374\n",
            "\tVAL epoch = 8 / loss = 2.0835297107696533 / acc = 0.24400000274181366 / res = 0.4156922399997711\n",
            "> epoch 9, batch 1 :: batch_loss = 2.1167469024658203, batch_acc = 0.2109375, batch_res = 0.40761512517929077 \n",
            "> epoch 9, batch 2 :: batch_loss = 2.0563337802886963, batch_acc = 0.2734375, batch_res = 0.4101923108100891 \n",
            "> epoch 9, batch 3 :: batch_loss = 2.087341785430908, batch_acc = 0.28125, batch_res = 0.40579256415367126 \n",
            "> epoch 9, batch 4 :: batch_loss = 2.1435694694519043, batch_acc = 0.15625, batch_res = 0.41213083267211914 \n",
            "> epoch 9, batch 5 :: batch_loss = 2.062056064605713, batch_acc = 0.2890625, batch_res = 0.41028639674186707 \n",
            "> epoch 9, batch 6 :: batch_loss = 2.060544967651367, batch_acc = 0.3046875, batch_res = 0.4080960154533386 \n",
            "> epoch 9, batch 7 :: batch_loss = 2.0564770698547363, batch_acc = 0.234375, batch_res = 0.4093899130821228 \n",
            "> epoch 9, batch 8 :: batch_loss = 2.068399429321289, batch_acc = 0.2890625, batch_res = 0.40318697690963745 \n",
            "> epoch 9, batch 9 :: batch_loss = 2.068203926086426, batch_acc = 0.203125, batch_res = 0.4043499827384949 \n",
            "> epoch 9, batch 10 :: batch_loss = 2.1413838863372803, batch_acc = 0.21875, batch_res = 0.4066404104232788 \n",
            "> epoch 9, batch 11 :: batch_loss = 2.01487398147583, batch_acc = 0.2265625, batch_res = 0.40973806381225586 \n",
            "> epoch 9, batch 12 :: batch_loss = 1.982730507850647, batch_acc = 0.2265625, batch_res = 0.40683677792549133 \n",
            "> epoch 9, batch 13 :: batch_loss = 1.997844934463501, batch_acc = 0.2421875, batch_res = 0.41568851470947266 \n",
            "> epoch 9, batch 14 :: batch_loss = 1.9948313236236572, batch_acc = 0.25, batch_res = 0.40392598509788513 \n",
            "> epoch 9, batch 15 :: batch_loss = 2.124770164489746, batch_acc = 0.1640625, batch_res = 0.40486714243888855 \n",
            "> epoch 9, batch 16 :: batch_loss = 2.0315375328063965, batch_acc = 0.234375, batch_res = 0.40867018699645996 \n",
            "> epoch 9, batch 17 :: batch_loss = 2.0369696617126465, batch_acc = 0.2265625, batch_res = 0.3980705738067627 \n",
            "> epoch 9, batch 18 :: batch_loss = 1.9989709854125977, batch_acc = 0.2890625, batch_res = 0.4081282615661621 \n",
            "> epoch 9, batch 19 :: batch_loss = 1.9313470125198364, batch_acc = 0.3125, batch_res = 0.4061565399169922 \n",
            "> epoch 9, batch 20 :: batch_loss = 1.9298847913742065, batch_acc = 0.3235294222831726, batch_res = 0.416050523519516 \n",
            "\tTRAIN epoch = 9 / loss = 2.048009157180786 / acc = 0.2460000067949295 / res = 0.40759244561195374\n",
            "\tVAL epoch = 9 / loss = 1.9711865186691284 / acc = 0.25999999046325684 / res = 0.4156922399997711\n",
            "> epoch 10, batch 1 :: batch_loss = 2.0444936752319336, batch_acc = 0.28125, batch_res = 0.39860999584198 \n",
            "> epoch 10, batch 2 :: batch_loss = 1.9788905382156372, batch_acc = 0.25, batch_res = 0.4110041856765747 \n",
            "> epoch 10, batch 3 :: batch_loss = 2.0153613090515137, batch_acc = 0.2109375, batch_res = 0.4064135253429413 \n",
            "> epoch 10, batch 4 :: batch_loss = 1.9645953178405762, batch_acc = 0.28125, batch_res = 0.4030555188655853 \n",
            "> epoch 10, batch 5 :: batch_loss = 1.91237473487854, batch_acc = 0.2578125, batch_res = 0.41508176922798157 \n",
            "> epoch 10, batch 6 :: batch_loss = 1.9941740036010742, batch_acc = 0.3203125, batch_res = 0.4063819944858551 \n",
            "> epoch 10, batch 7 :: batch_loss = 1.944568395614624, batch_acc = 0.2890625, batch_res = 0.4064164161682129 \n",
            "> epoch 10, batch 8 :: batch_loss = 1.8991230726242065, batch_acc = 0.3125, batch_res = 0.4047253727912903 \n",
            "> epoch 10, batch 9 :: batch_loss = 1.977327823638916, batch_acc = 0.2421875, batch_res = 0.40428823232650757 \n",
            "> epoch 10, batch 10 :: batch_loss = 1.9735534191131592, batch_acc = 0.2578125, batch_res = 0.40804266929626465 \n",
            "> epoch 10, batch 11 :: batch_loss = 1.971167802810669, batch_acc = 0.2734375, batch_res = 0.4031572937965393 \n",
            "> epoch 10, batch 12 :: batch_loss = 2.028468132019043, batch_acc = 0.234375, batch_res = 0.4084969758987427 \n",
            "> epoch 10, batch 13 :: batch_loss = 1.8978946208953857, batch_acc = 0.3125, batch_res = 0.4099849760532379 \n",
            "> epoch 10, batch 14 :: batch_loss = 1.8761982917785645, batch_acc = 0.34375, batch_res = 0.40931788086891174 \n",
            "> epoch 10, batch 15 :: batch_loss = 1.9771641492843628, batch_acc = 0.2890625, batch_res = 0.40766799449920654 \n",
            "> epoch 10, batch 16 :: batch_loss = 1.830672264099121, batch_acc = 0.3359375, batch_res = 0.4075250029563904 \n",
            "> epoch 10, batch 17 :: batch_loss = 1.9186428785324097, batch_acc = 0.328125, batch_res = 0.4132114052772522 \n",
            "> epoch 10, batch 18 :: batch_loss = 1.8712856769561768, batch_acc = 0.3203125, batch_res = 0.41115349531173706 \n",
            "> epoch 10, batch 19 :: batch_loss = 1.907759189605713, batch_acc = 0.2578125, batch_res = 0.4108070731163025 \n",
            "> epoch 10, batch 20 :: batch_loss = 1.8977023363113403, batch_acc = 0.30882352590560913, batch_res = 0.40554842352867126 \n",
            "\tTRAIN epoch = 10 / loss = 1.9451837539672852 / acc = 0.2847999930381775 / res = 0.40759244561195374\n",
            "\tVAL epoch = 10 / loss = 1.886752724647522 / acc = 0.31200000643730164 / res = 0.4156922399997711\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below, we plot the accuracy and loss curves for the training and validation sets for every epoch during training of the implicit model. "
      ],
      "metadata": {
        "id": "9-yIlKDSMlsa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing package\n",
        "import matplotlib.pyplot as plt\n",
        "  \n",
        "# create data\n",
        "train_loss_broyden = broyden_results['train_loss_vals']\n",
        "val_loss_broyden = broyden_results['val_loss_vals']\n",
        "epochs = np.arange(len(val_loss_broyden))\n",
        "  \n",
        "# plot lines\n",
        "plt.plot(epochs, train_loss_broyden, label = \"train_loss_broyden\")\n",
        "plt.plot(epochs, val_loss_broyden, label = \"val_loss_broyden\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.xlabel(\"loss\")\n",
        "plt.title(\"Loss curves of implicit MDEQ using Broyden solver\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MYYcdktwNC6j",
        "outputId": "abfe0fbf-dbf4-4979-d191-0bd58e39baee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create data\n",
        "train_acc_broyden = broyden_results['train_acc_vals']\n",
        "val_acc_broyden = broyden_results['val_acc_vals']\n",
        "  \n",
        "# plot lines\n",
        "plt.plot(epochs, train_acc_broyden, label = \"train_acc_broyden\")\n",
        "plt.plot(epochs, val_acc_broyden, label = \"val_acc_broyden\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.xlabel(\"accuracy\")\n",
        "plt.title(\"Accuracy curves of implicit MDEQ using Broyden solver\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vQdRBtTtatAy",
        "outputId": "c8f751c1-9a71-4a9c-ce16-b0b0f31aa184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEWCAYAAABollyxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd1gVx9rAfwOiiIIFO2Lvioq99xiNsUeNNcaoMTFfetFriumm3FzjvabYYk2MvcfYNXYBsfcWQEUEQVBA4Mz3xyzmiJSDngbM73l42LM75d3Z3XfffWfmHSGlRKPRaDS5FxdHC6DRaDQa26IVvUaj0eRytKLXaDSaXI5W9BqNRpPL0Ypeo9Focjla0Ws0Gk0uRyt6jVURQvQVQoQIIeKEEP7pHI8TQlSxQb0dhBChZr9PCCE6WJDPJvLkNIQQfwghnnO0HNZGCDFXCPGZo+VIRQixQwgx2t71Or2iNxrmlhCigKNl0VjEt8ArUsrCUsrDaQ8a+y/aWggpZV0p5Q4L0t2XxxKlIISQQogbQoh8ZvvcjH3SbN8OIUSCECJWCHFbCBEohJhgfh8LISYLIZKMl03qX7TZcSGEeEcIcU4IES+E+FsI8YUQIn82myNLpJTdpZTzrF2u8QI2mZ1fmBDiY2vXo8kcp1b0QohKQFtAAr3sXHe+rFM5D04kb0XghKOFsDG3gO5mv7sb+9LyipTSEygLvAU8C2wQQgizNL8bL5vUv6Jmx6YBY4ERgKdRTxdgsfVOxS5cTT0/oA3wghCiT3oJneg+zjFY0mZOrehRN/h+YC7wwGelEMJXCLFCCBEhhIgUQvzP7NgYIcQpw5o6KYRoZOyXQohqZunuW3Cpn/5CiPeEENeBX4QQxYQQ64w6bhnb5c3yFxdC/CKEuGocX2XsPy6E6GmWzk0IcTM9V4ZxvLcQItiw/C4IIboZ+y8LIbqYpZsshFhobFcyzucFIcTfwDbj8/uVNGUfEUL0M7ZrCSE2CyGihBBnhBADzdI9ZbRVrGF1vZ2BrC5CiPeFEFcMK3a+EKKIEKKAECIOcAWOCCEuZJD//jUw2v8HQ+44IcQeIUQZIcRUoz1Pm7eZ0R4TDTlvGW3vnkE999tOCOEqhPiX0baxhnXtay6PEGIsMBR415BlbXrlGixA3ZupjADmZ5RYSnnH+LroBbQEemRSdqr81YGXgaFSyn1SymQp5QmgP9BDCNE+q/M2fpvfM+5CiIXG8xIthDgkhChtHLvvUhBCjBRC7BZCfGu08yUhRHezMisLIXYZbblFCDE9tY6skFJeAvYCdczKk0KI8UKIc8A5Y98YIcR5415dI4QoZ+yfLoT4d5pzXiOEeMPY9hdCBBmy/Q64p0n7tPGsRQsh9goh6qdpu7eFEEeFEDFCiN8zub+qCSF2GuluGnWlHmtltG2M8b9VOvkLGDLUM9tXUqgvt1IWyvqeEOIocEdkpeyllE77B5xH3eyNgSSgtLHfFTgC/AcoZFzMNsaxAUAY0BQQQDWgonFMAtXMyp8LfGZsdwCSga+AAkBBwBv1YHmgLKqlwCqz/OuB34FigBvQ3tj/LspSS03XGziWwTk2A2KAJ1AvXh+glnHsMtDFLO1kYKGxXck4n/lGGxREKZw9ZunrANHG+RQCQoDngXyAP3ATqGOkvQa0NbaLAY0ykHeUcV2qAIWBFcACs+MPtHE6+e8fN9r/pnF93YFtwCXjPFyBz4DtZnkvA8cBX6A4sCfN9QtNk7aLsf0OcAyoadwTDQDvDOT5LIt7UgL1gHCgqNFW4cY+aZZuBzA6nfy7gK/SXs900o0DrmRwbCfweQbHMrtnXgTWou5nV6PdvdLKC4xEPW9jjHQvAVcBYRzfh3LR5UdZ6LczOY+016U66vnslKZNNxvXtCDQybgvGqHu3f8Cu8yel6uAi/G7BHAXKG3IcwV4A/U8PmOcR+o94g/cAJob5/Wc0V4FzNruIFDOkOUUMC6D8/oNmIR6Zs31T3HU191w1HM22PjtnU47zzG/jsB4YGM2ZA1GPQsFs9KlTmvRCyHaoNwAS6SUgcAFYIhxuBnqYrwjlbWUIKXcbRwbDXwtpTwkFeellFcsrNYEfCSlTJRSxkspI6WUy6WUd6WUscDnQHtDvrKoT+lxUspbUsokKeVOo5yFwFNCCC/j93CUFZgeLwBzpJSbpZQmKWWYlPK0hfICTDbaIB5YCTQUQlQ0jg0FVkgpE4GngctSyl+ksg4PA8tRL0ZQD0QdIYSXcT5BGdQ3FPhOSnlRShkHTASezdKiyJiVUspAKWWCIX+ClHK+lDIF9RJN+xX0PylliJQyCnU9BltQx2jgfSnlGeOeOCKljHxEeQESUApzkPG3xthnCVdRyiCVgYbFlvq33dhfAvXyTY9rQMnsi00SynipJqVMMdr9dgZpr0gpZxrXYR7K/VRaCFEBZUR9KKW8Zzx3a7Kot5xxbreBs8ABYHeaNF9KKaOM+3go6pkIMu7diUBLIUQlKeVBlGHU2cj3LLBDShkOtEAp+KnG87gMOGRWx1jgZynlAeP85wGJRr5Upkkprxr311qgYQbnlITST+XS6J8ewDkp5QLjOfsNOA30TKeMXw35Uxli7MuOrCFGm2WK0yp61Btsk5TypvH7V/5x3/iibsTkdPL5ol4Kj0KEoXAAEEJ4CCF+FspNcRtljRUVQrga9URJKR/yzUopr6Kszf5CiKKoF8KiDOp8HHlBWemp9caivjJSb57BZvVWBJqbKxXUA1XGON4feAq4YnyStsygvnIoqymVKyjLpfQjyh9uth2fzu/CadKHmG1fMeTJisdt4/SYj/ryyNRtkw4+QJTZ7yVSyqJmfx2N/TdRyjU9yhrHs8sC4E9gsVDuxq+FEG4ZpL2euiGlvGtsFka1d5TZPnjwmqTHVePcvFBfQfGol4c55mU8cI8ZBkUkqu0w8g4ztofxjxFVDgiThslrYH6vVgTeSvMM+PLgPXTdbPsuD99/qbyL+jo8KNQIr1HpyW4mgw8Psx3wEEI0F6o/siHK2LFU1qza/T5OqeiFEAWBgUB7IcR1oXzmbwANhBANUCdYIQMrMgSomkHRd1GframUSXM8bSjPt1Cf+82Nm7RdqohGPcUNRZ4eqTfjAGCflDIsg3SZyXsnC3nTk/k3YLChqN1RN1NqPTvTKJXCUsqXAIwvoN5AKWAVsCQDma6ibsJUKqBcXuHpJ7c6vmnqvmpBnsza2JzshHL9C8PK5WHrNF2E6hdobOTNim2ArxCiWTpltEC5ANIjw3vGsHI/llLWAVqhvvJGkD2uoe578zp8M0qcFillDMpoS2vhmrf9A/eYEKIQ6ksk9RlaCPQ2dEFt1P2aKpuPEA90dlcw2w5BuUrMnwEPw+rOFlLK61LKMVLKciiX2A9C9T2lfT5SZXjo+Te+lpagDLLBwDrDWLNUVovvV6dU9EAfIAXlY25o/NVGPSAjUH60a8AUIUQhoTqZWht5ZwFvCyEaC0U1M1dGMDBEqM65bhhumEzwRFkf0UKI4sBHqQeklNeAP1AXuJhQHa7tzPKuQvkYXyNzi2828LwQorNQHZ0+QohaZvI+a5TdBOVzzIoNqBvtE1Q/gcnYvw6oIYQYbpTnJoRoKoSoLYTIL4QYKoQoIqVMQvlcTRmU/xvwhlAdcoWBL4x60vu6sgXjhRDljesxCeXeyYpZwKdCiOrGPVFfCOGdTrpwVN9DlhhWY0+gVxoL8iGML8P2wGrUvbvBgvLPAj8Bi4QQLYx7ti7K3bYX2JJB1gzvGSFERyGEn/FFehvlfsjoOmck1xUgAJhs3DctSd8tkS7GPfMsmY/M+g31TDQUajjqF8ABKeVlQ4ZQlEtmAbDczHWxD2V0vGqcfz+UmzeVmcA4w4IWhu7oIYTwtFR+s/MYIP4ZmHELpXRNqGtbQwgxRAiRTwgxCKXH1mVQ1K8o999Q/nHbWFVWwDk7Y4GNwL/T2T8Q9WmVD/WWXIX6pLuJ8leZd2SdAeJQnXf+xv4mqBssFnWT/EYGnXnGvnIoyykO5Vt80big+eQ/HS/zUAriFsofbp5/FsrCKpzF+fYFjhpynQeeNPZXQfkz41AumWk83BmbL53yZhvHmqbZX9MoJ8Jot22ol2h+o81voRTAIYzOpXTKdgE+RFkcESjrqpjZ8ex2xn5mdmw0yt+a+rsakGz2+zLKX3sS1ck8D/BI7/rxYGesK/A+qqM31ji/8unIUx2lKKMx63TPSP40+6vxcGdsglFfLHAY9WJyN0szGaVs49L8lTJr6/eMeyLRqHspUCST9s3snhmMei7uoO7ZafxzL+/gwc7Y3Zlct6oooysW2ArMAGZnIE8HlAJMPbdIQ65qmbUp6hm+gHJzrUu9XmbHhxn5OqbZ38Ro61iUEfA7D95j3YzrH40yFpcCnmnvGbPrk1En89coKz3OkHOs2bE2QCCqLyEQs2eJdDrpjesbBeRPs99iWbP6S+1F19gAIcSHQA0p5bAsE2uyRAhxGfWQZGTN5mqEmmjUF2gnpYzOKr29EGpo4Wkp5UdZJrZene1QRkZFqZVYljir6ybHY7gWXkBZOxrNY2Mo0hk8OPLC7hguv6qGq7EbavjwqqzyWbF+N5RLdJZW8pahFb0NEEKMQbk2/pBS7nK0PJrcg5Tyf1LKjQ4Wowz/uDSnAS/JdMJd2AIhRG2UK6MsMNUedeYGtOtGo9FocjnaotdoNJpcjtMFECpRooSsVKmSo8XQaDSaHEVgYOBNKWW6M6adTtFXqlSJgIAAR4uh0Wg0OQohRIahXrTrRqPRaHI5WtFrNBpNLkcreo1Go8nlOJ2PPj2SkpIIDQ0lIcHSSLCa3IK7uzvly5fHzS2jIIsajSYrcoSiDw0NxdPTk0qVKvFgYDpNbkZKSWRkJKGhoVSuXNnR4mg0OZYc4bpJSEjA29tbK/k8hhACb29v/SWn0TwmOULRA1rJ51H0dddoHp8co+g1Go0mV3N8ORxbZpOitaLXaDQaR3PrMqx5DQ7NAlO21oKxCK3oLSQ6Opoffvgh2/meeuopoqOdJnQ4I0eOZNky21gN5hQunNFSmxqN5gFSkmHFWBAC+s0AF+urZa3oLSQjRZ+cnPkKehs2bKBo0YyWlXVOsjonjUZjRXZ/ByEHoMd3ULRC1ukfgRwxvNKcj9ee4OTV21Yts045Lz7qWTfTNBMmTODChQs0bNgQNzc33N3dKVasGKdPn+bs2bP06dOHkJAQEhISeO211xg7dizwT+yeuLg4unfvTps2bdi7dy8+Pj6sXr2aggULplvfzJkzmTFjBvfu3aNatWosWLAADw8PwsPDGTduHBcvXgTgxx9/pFWrVsyfP59vv/0WIQT169dnwYIFGZ7Lli1bmDJlCrdv3+a7777j6aefZu7cuaxYsYK4uDhSUlJYuXIlo0aN4uLFi3h4eDBjxgzq1atHzZo12bt3LyVLlsRkMlGjRg327dtHXFwcQ4YMIS4ujt69ez9Q3zfffMOSJUtITEykb9++fPzxx1y+fDlb7aHR5EpCDsGOKeA3EOoPsFk12qK3kClTplC1alWCg4P55ptvCAoK4vvvv+fs2bMAzJkzh8DAQAICApg2bRqRkZEPlXHu3DnGjx/PiRMnKFq0KMuXL8+wvn79+nHo0CGOHDlC7dq1mT17NgCvvvoq7du358iRIwQFBVG3bl1OnDjBZ599xrZt2zhy5Ajff/99pudy+fJlDh48yPr16xk3btz94YtBQUEsW7aMnTt38tFHH+Hv78/Ro0f54osvGDFiBC4uLgwbNoxFixYB6oXRoEEDSpYsyWuvvcZLL73EsWPHKFu27P26Nm3axLlz5zh48CDBwcEEBgaya9eubLeHRpPrSIyFFWPAywd6fGvTqnKcRZ+V5W0vmjVr9sAknmnTprFy5UoAQkJCOHfuHN7e3g/kqVy5Mg0bNgSgcePGXL58OcPyjx8/zvvvv090dDRxcXE8+eSTAGzbto358+cD4OrqSpEiRZg/fz4DBgygRIkSABQvXjxT2QcOHIiLiwvVq1enSpUqnD59GoAnnnjift7du3ffV7ydOnUiMjKS27dvM2rUKHr37s3rr7/OnDlzeP755wHYs2fP/fTDhw/nvffeA5Si37RpE/7+/gDExcVx7tw5KlSokK320GhyHX9MgOgrMHI9uBexaVU5TtE7C4UKFbq/vWPHDrZs2cK+ffvw8PCgQ4cO6U7yKVCgwP1tV1dX4uPjMyx/5MiRrFq1igYNGjB37lx27NhhNdnTjk1P/W1+Thnh6+tL6dKl2bZtGwcPHrxv3adXLqjZrRMnTuTFF198YP/ly5ez1R4aTa7ixCoIXgjt3oGKrWxenXbdWIinpyexsbHpHouJiaFYsWJ4eHhw+vRp9u/f/9j1xcbGUrZsWZKSkh5Qpp07d+bHH38EICUlhZiYGDp16sTSpUvvu4uioqIyLXvp0qWYTCYuXLjAxYsXqVmz5kNp2rZte7/eHTt2UKJECby8vAAYPXo0w4YNY8CAAbi6ugLQunVrFi9eDPCAvE8++SRz5swhLi4OgLCwMG7cuPFIbaLR5ApiwmDta+DTGNq/Z5cqLVL0QohuQogzQojzQogJ6RwfJ4Q4JoQIFkLsFkLUMTs20ch3RgjxpDWFtyfe3t60bt2aevXq8c477zxwrFu3biQnJ1O7dm0mTJhAixYtHru+Tz/9lObNm9O6dWtq1ap1f//333/P9u3b8fPzo3Hjxpw8eZK6desyadIk2rdvT4MGDXjzzTczLbtChQo0a9aM7t2789NPP+Hu7v5QmsmTJxMYGEj9+vWZMGEC8+bNu3+sV69exMXF3XfbpMo1ffp0/Pz8CAsLu7+/a9euDBkyhJYtW+Ln58czzzyT4QtTo8n1mEywahykJEG/meBqp2B9UspM/wBX4AJQBcgPHAHqpEnjZbbdC9hobNcx0hcAKhvluGZWX+PGjWVaTp48+dA+jeM4dOiQbNOmjd3q09dfk2vY/b2UH3lJGTjP6kUDATIDvWqJRd8MOC+lvCilvAcsBh4YPyelNB/vWAiQxnZvYLGUMlFKeQk4b5SnyaFMmTKF/v378+WXXzpaFI0mZ3HtCGz9BGr3BP/hdq3aks5YHyDE7Hco0DxtIiHEeOBNlNXfySyvucM61NiXNu9YYCwot0JeYvz48ezZs+eBfa+99toDbpFH4fPPP2fp0qUP7BswYACTJk16rHInTJjAhAkPee80Gk1m3LsLy0dDoRLQc5qaBWtHrDbqRko5HZguhBgCvA88l428M4AZAE2aNJFZJM9VTJ8+3SblTpo06bGVukajsRKbP4CbZ2H4KvDIfPizLbDEdRMG+Jr9Lm/sy4jFQJ9HzKvRaDS5izMbVbCylq9A1Y4OEcESRX8IqC6EqCyEyA88C6wxTyCEqG72swdwztheAzwrhCgghKgMVAcOPr7YGo1GkwOIuwGrx0NpP+j8ocPEyNJ1I6VMFkK8AvyJGoEzR0p5QgjxCaqXdw3wihCiC5AE3MJw2xjplgAngWRgvJQyxUbnotFoNM6DlErJ34uD/jMhX4Gs89gIi3z0UsoNwIY0+z40234tk7yfA58/qoAajUaTIzk0C85tgqe+hVK1HSqKnhlrIxwRj/3y5cvUq1fP5vVMnjyZb7+1bRAmjSZHc+MUbHofqneFpqMdLY1W9HmRlBTtPdNobEZyohpKmb8w9J5u96GU6ZHzgpr9MQGuH7NumWX8oPuUTJNMmDABX19fxo8fDyirNl++fGzfvp1bt26RlJTEZ5999lAs9vRIjdmeXr704spnFIM+PZKTkxk6dOj9EMbz58/Hw8ODSpUqMWjQIDZv3sy7776LlJIvvvgCKSU9evTgq6++Ys6cORw9epSpU6cCKib+yZMn+c9//sPnn3/OvHnzKFWqFL6+vjRu3BiACxcuMH78eCIiIvDw8GDmzJnUqlWLkSNH4uXlRUBAANevX+frr7/mmWeesex6aDQ5ma2fQPhxGPw7FC7laGkAbdFbzKBBg1iyZMn930uWLOG5555j5cqVBAUFsX37dt56663UMBCZ4u7unm6+jOLKpxeDPiPOnDnDyy+/zKlTp/Dy8npgVSxvb2+CgoJo164d7733Htu2bSM4OJhDhw6xatUqBg4cyNq1a0lKSgLgl19+YdSoUQQGBrJ48WKCg4PZsGEDhw4dul/m2LFj+e9//0tgYCDffvstL7/88v1j165dY/fu3axbt05PstLkDS5sh33/U+6amt0cLc19cp5Fn4XlbSv8/f25ceMGV69eJSIigmLFilGmTBneeOMNdu3ahYuLC2FhYYSHh1OmTJlMy5JS8q9//euhfNu2bUs3rnx6MegzwtfXl9atWwMwbNgwpk2bxttvvw2olxXAoUOH6NChAyVLlgRg6NCh7Nq1iz59+tCpUyfWrVtH7dq1SUpKws/Pj6lTp9K3b188PDwAFdQM1JfJ3r17GTDgn5VxEhMT72/36dMHFxcX6tSpQ3h4uIUtrdHkUO5GwaqXoERNeOJTR0vzADlP0TuQAQMGsGzZMq5fv86gQYNYtGgRERERBAYG4ubmRqVKldKNQ5+WR81nCRnFmgfL4s2PHj2aL774glq1amUZhsFkMlG0aFGCg4PTPW4eb96SLx2NJsciJaz5P7hzE4YsgfwejpboAbTrJhsMGjSIxYsXs2zZMgYMGEBMTAylSpXCzc2N7du3c+XKFYvKyShfRnHl04tBnxF///03+/btA+DXX3+lTZs2D6Vp1qwZO3fu5ObNm6SkpPDbb7/Rvn17AJo3b05ISAi//vorgwcPBqBdu3asWrWK+Ph4YmNjWbt2LQBeXl5Urlz5fkwdKSVHjhyxqA00mlzF4QVwep2aFFW2vqOleQit6LNB3bp1iY2NxcfHh7JlyzJ06FACAgLw8/Nj/vz5D8SNz4yM8mUUVz69GPQZUbNmTaZPn07t2rW5desWL7300kNpypYty5QpU+jYsSMNGjSgcePGD3QiDxw4kNatW1OsWDEAGjVqxKBBg2jQoAHdu3enadOm99MuWrSI2bNn06BBA+rWrcvq1astagONJtcQeUENEqncToU5eESWB4ayYL9lxmJ2Ec72Sd2kSRMZEBDwwL5Tp05Ru7ZjJxzkJZ5++mneeOMNOnfu7GhRAH39NU5MShLM7gpRF+GlvVDkoeC8FvHHsWuM/zWIllW9mT+qOa4u2R+SKYQIlFI2Se+Ytug194mOjqZGjRoULFjQaZS8RuPU7PwKrgZBz+8fWclvP32DVxcfxr9CMWYMb/JISj4rdGesDTl27BjDhz+4wECBAgU4cODAY5UbGRmZriLeunUr3t7ej1xu0aJFOXv27OOIptHkHa7shb/+DQ2HQd0+WadPh30XIhm3MJAapT2ZM7IphQrYRiXnGEUvpXxoRImz4+fnl+GIlMfB29vbJuU6I87mWtRoAEiIgRUvQtGKjzzk+/Dftxg97xC+xT2YP6oZRQrabv3YHOG6cXd3JzIyUj/0eQwpJZGRkekuXq7ROJT1b8PtMOg/Cwp4Zjv7yau3eW7OQbwLF2DR6OZ4F7ZtZMscYdGXL1+e0NBQIiIiHC2Kxs64u7tTvnx5R4uh0fzD0aVwbAl0nATl0+37zJTzN+IYPvsAhQrkY9Ho5pT2sr0hkyMUvZubG5UrV3a0GBqNJq9z6wqsfxN8m0ObN7OdPSTqLsNmHUAIWDi6Ob7F7TOxKkcoeo1Go3E4phRY+aKaBdtvBrhmT32G305g6KwDxCelsHhsC6qWtF8oc63oNRqNxhJ2fwd/74O+M6BYpWxljYxLZOisA0TGJbJoTAtql/WyjYwZoBW9RqPRZEVoIOyYAvX6Q/2B2coaE5/E8NkHCYm6y7xRzWjoW9RGQmZMjhh1o9FoNA4jMQ5WjAbPstDju2wtJHInMZnnfznIuRux/DS8MS2qPPo8l8dBW/QajUaTGX9OhKhLMHIdFLTcGk9ISmHM/ACCQ6KZPqQRHWs6bhESbdFrNBpNRpxcA0Hzoc0bUOnhSLAZkZRiYvyiIPZeiOTbAQ3o7lfWhkJmjVb0Go1Gkx63r8LaV6GcP3SYaHG2FJPk9d+D2Xr6Bp/2qUe/Ro6fB6IVvUaj0aTFZFKrRSUnQr9ZkC+/hdkkE5YfZf3Ra/zrqVoMb1HRxoJahvbRazQaTVr2/wAXd6iolCWqWZRFSskn606yNDCUVztXZ2y7qraVMRtoi16j0WjMuX4Mtn4MNXtAo+cszvbtpjPM3XuZF9pU5o0u1W0oYPbRil6j0WhSSYqH5aOhYDHo9V+Lh1JO336e6dsvMLhZBd7vUdvpIu1q141Go9GksvlDiDgNw1ZAIcvGvM/dc4lv/jxD74bl+KxPPadT8qAteo1Go1Gc3QQHZ0CLl6GaZSusLQkIYfLak3StU5pvBzSwyepQ1kAreo1Go4mLgNUvQ6m60Pkji7KsO3qVCcuP0rZ6Cf47xB83V+dVp9p1o9Fo8jZSwppXIOE2jFgDblnHh996KpzXFwfTuKJa57VAPlc7CProaEWv0WjyLhFnYPdUOLsRun0FpetkmWXP+Zu8tCiIOuW8mD2yKQXzO7eSB63oNRpNXsNkgvNb4MCPcGEbuBaA5i9Bs7FZZg28cosx8wOo7F2Iec83w8vdduu8WhOt6DUaTd4gMRaCf4ODP0PkeRWNstP70Ph5KFQiy+zHw2IY+ctBSnkWYMHoZhQrZNlsWWdAK3qNRpO7iboEB2fC4QWQeBt8mkD/2VC7l8WhDc6FxzJizkG83N1YNKYFpTxz1oL1WtFrNJrch5Rw+S/Y/xOc2QAurlCnD7R4KdsLev8deZdhsw/gIgQLRzfHp2hBGwltO7Si12g0uYekeDi2FA78DOHHwcMb2r4FTV8Ar3LZLu5aTDxDZu0nMdnE72NbUrlEIRsIbXu0otdoNDmf21fh0CwI+AXio9R4+F7/A79nwO3RLPCbxjqv0XeT+HVMc2qW8bSy0PbDIkUvhOgGfA+4ArOklFPSHH8TGA0kAxHAKCnlFeNYCnDMSPq3lLKXlWTXaDR5nZBDavTMydVgSoGaT0GLcVCpbbaW/EtL9N17DJt1gKvR8Sx4oTn1y9t/nVQTe6sAACAASURBVFdrkqWiF0K4AtOBJ4BQ4JAQYo2U8qRZssNAEynlXSHES8DXwCDjWLyUsqGV5dZoNHmVlCSl2Pf/AGGBUMALmr0IzcZA8cqPXXxcYjLP/XKIixF3mD2yCU0rFbeC0I7FEou+GXBeSnkRQAixGOgN3Ff0UsrtZun3A8OsKaRGo9Fw5yYE/gKHZkPsNSheFbp/Aw0HQwHruFUSklIYPe8Qx8Ni+HFoI9pWL2mVch2NJYreBwgx+x0KNM8k/QvAH2a/3YUQASi3zhQp5aq0GYQQY4GxABUqVLBAJI1Gk2e4fly5Z44uhZREqNoJek6Dal3AxXrxZe4lmxi3MJADl6KYOqghXeuWsVrZjsaqnbFCiGFAE6C92e6KUsowIUQVYJsQ4piU8oJ5PinlDGAGQJMmTaQ1ZdJoNDkQUwqc+QMO/KSGSbp5gP9Q5aIpVcvq1SWnmHht8WF2nIngy35+9G7oY/U6HIklij4M8DX7Xd7Y9wBCiC7AJKC9lDIxdb+UMsz4f1EIsQPwBy6kza/RaDTER8PhhSpccPQVKOILT3wCjUaoxUBsgMkkeXf5Uf44fp0Pnq7D4Ga5z6tgiaI/BFQXQlRGKfhngSHmCYQQ/sDPQDcp5Q2z/cWAu1LKRCFECaA1qqNWo9Fo/uHmeWW9B/8KSXegQivo+qlazs/VdqPApZR8uOY4K4LCePOJGrzQ5vE7c52RLFtQSpkshHgF+BM1vHKOlPKEEOITIEBKuQb4BigMLDVWV0kdRlkb+FkIYULFvp+SZrSORqPJq0ipgort/xHObwbX/FCvPzQfB+VsP1AvdTHvhfv/5sX2Vfi/TpYtAp4TEVI6l0u8SZMmMiAgwNFiaDQaW7Pne7V0X6FSauZqk1FQuJRdqpZS8tXGM/y08wLPt67Eh0/XccolALODECJQSplufAc9M1aj0difq8Gw9VOo3VMFGMtXwK7VT91yjp92XmBYiwq5QslnhfOufaXRaHIn9+7C8tFQqKQaJmlnJT99+3m+33qOAY3L80kv51zM29poi16j0diXTZNUPPgRq8HDvrNOZ/11kW/+PEOfhuWY0r8+Lk66mLe10Ra9RqOxH6c3QMAcaPUKVGmfdXorMn/fZT5bf4qn/Mrw7YAGuOYRJQ9a0Ws0GnsRG64W4S7jB50+sGvViw/+zYerT9Cldmm+f9affK55S/XlrbPVaDSOQUpY/TLcu2P3ztcVQaFMXHmM9jVKMn2oP255TMmD9tFrNBp7cHCGWpD7qW+hZE27Vbv2yFXeXnqEVlW9+Xl4Ywrkc7Vb3c5E3nu1aTQa+xJ+EjZ9ANWfhKaj7VbtxuPXef33YJpULM7MEU1wd8ubSh60otdoNLYkKUENpXT3gt7TH2sxkOyw7XQ4//dbEPXLF2HO803xyJ+3nRda0duCu1GOlkCjcQ62fgI3TkDvH6CwfWK7/3UugnELg6hVxou5zzejcIG8reRBK3rrc3EHfF0Fzm1xtCQajWM5vxX2T4dmY6FGV7tUuf9iJGPmB1ClRCEWvNCMIgXd7FKvs6MVvbUJmANI2PyBiqmt0eRF7kTCqpehZC0VZtgOBF6JYtTcQ/gW82DR6OYU9chvl3pzAlrRW5M7N9WEkFJ14MZJOLLY0RJpNPZHSljzfxAfBf1mgltBm1d5JCSakXMOUdrLnUWjm+Nd2L5hFZwdreitydElYEqC/rOgXCPY/jkkxTtaKo3GvgTNgzProfNHULa+zas7cTWG4bMPULSQG7+OaU4pL3eb15nT0IreWkipVsbxaQyl66rP1dthavywRpNXuHkeNk6EKh2gxcs2r+7M9ViGzTqAp7sbv45uQdkitv96yIloRW8trgap0QX+w9Tvym2h2hPw17/1KBxN3iAlCVaMVrNe+/xo1YW70+NCRBxDZx0gfz4XFo1ujm9xD5vWl5PRit5aHF4I+QqqFXJS6TIZEm7D7u8cJZVGYz92fAlXD6vQw17lbFrVlcg7DJm5H4BFo1tQqUQhm9aX09GK3hrcuwvHlkGd3uBe5J/9ZepBg8FwYAZEhzhOPo3G1lzeA399B/7DoU4vm1YVeusuQ2Ye4F6yiUWjm1OtVGGb1pcb0IreGpxaC4m3/3HbmNPxX+r/9s/tK5NGYy/io2HFWCheGbpNsWlV12LiGTLzALEJSSx4oTk1y3jatL7cglb01uDwAihWGSq1efhYUV9o/qIaann9mP1l02hsiZSw/k2IvQb9ZkEB21nXN24nMHTmAaLu3GPBC82p51Mk60waQCv6xyfqIlz+C/yHZhzHo+2byqWz5WP7yqbR2JqjS+D4cug4Eco3tlk1kXGJDJ11gOu3E5g3qikNfIvarK7ciFb0j0vwryBcoMGQjNMULAZt34Lzm+HSLvvJptHYkluXYcPbUKEltHnTZtVE373HsNkHCbl1lzkjm9K4on2XH8wNaEX/OJhSlKKv2hmK+GSettlY8CoPmz8Ek8k+8mk0tiIlGVa8qLb7/gwutgkBfDshieGzD3IhIo6ZI5rQooq3TerJ7WhF/zhc2K4mRaXXCZsWN3foNEkNPzu50vayaTS2ZPd3ELIfenwHxSrapIq4xGRGzjnI6eu3+WlYI9pWt0/0y9yIVvSPw+EF4OENNZ+yLH39QVCqrgrdmnzPtrJpnJPjK+DGKUdL8XiEBsCOKeA3AOoPsEkVd+8lM2ruIY6ExvDfwY3oVKu0TerJK2hF/6jciYTT65XyzmdhlDwXV3jiY+XbDPzFpuJpnJDDi2DZ8/BTG9j+Rc582SfGqoVEvHzUsoA2ICEphTHzAwi4HMXUQQ3pVq+MTerJS2hF/6gcMwKYWeK2MadaF6jUFnZ+pWbNavIG14+pYYiV2kLdfur6/9wOQgMdLVn22DgBoq9Av5+hoPVHviQmpzBuYSB7L0Ty7YAG9Gxg2xm2eQWt6B8FKSFogYpQWbpu9vIKoQKe3Y2EvdNsI5/GuUiIgSUjwL0oPDMH+s+EIUvU/tld4M9Jana1HQmJusvNuESklJZnOrFKhfpo8yZUbGV1mZJSTLzy62F2nIngy75+9GtU3up15FX0GluPwrVgFcCsxyPGsPFppKy6fdPVYsmeufjT9OBMuH5UtZVrHlztR0pYPR5uXYGR66FwKbW/xpMwfj9s/gj2/Q9Or4Ne/4XK7Wwu0rbT4YyaGwBAgXwu+BQtSLmiBfEpWhCfYmbbRQtSpog7+fO5QEwYrH1NGTcdJlhdpuQUE68vDmbzyXA+6V2XZ5tVsHodeRmt6B+FoAWQz/3BAGbZpdP7cGqN6tTqOdV6sjkT57fAhncACa75oce/HS2R/dn3PxUio+vnULHlg8fci6hrX6+/WqhjXk9oPFJ98bnbZtZn1J17vLvsGLXKePJsU1/CouO5Gp1AaHQ8287cICI28YH0QkDpwm7M4DNqJifwS8kJeBwMe+Cl4OX+eC/wFJPk7aVHWH/sGu/3qM2IlpUeqzzNw2hFn12S4v8JYPY4PkrvqtBkFByaDS3HQ4nq1pPRGYgOgeVj1GpbldvCgZ+gRE1oPtbRktmPK/uUxV67p7rGGVG5Lby0F3Z8ob7yzm6Cp/8DNbtZVRwpJZNWHuN2fBILXmhG7bJeD6VJSErhekwCYdHx6u9WPNXPz6F++BG+yj+e2QEm7qWceCCPp3u++18A5dJ8FZQvVpCShQvg4pL+rHGTSTJxxVFWBV/lnSdrMrptFaues0ahFX12ObUWEmOy3wmbHu3eVROutkyGZxc9fnnOQvI9WDpSxScfOF8Fu4r+Gza+p15w1To7WkLbE3dDtUGxitB7esbhMVLJ7wFdP4M6fWHNK/DbIKj3DHT/CgqVsIpIq4Ov8sfx60zoXitdJQ/g7uZKpRKF/gn7e+0o7JsFtZ7mvUGf846Em3GJ918EV42XQVi0ejkEXLlFTHzSA2W6uQrKFilIuaLu+BT1wKeo+/2XwR/Hr7MkIJRXO1dnfMdqVjlPzcOIbHXG2IEmTZrIgIAAR4uRMfN6Kn/rq8HWWVhh59cqsuWoTVCh+eOX5wxseBcO/gwD5kHdPmpfYhzMeVIp/NFboGRNx8poS1KSYUEfNd589BYVrjo7JN9TE5J2fQvuXtD9a+XeyeplkQnXYuLp+p9d1Cztye8vtsQ1Awv7Ae7dhRkdVGTWl/aCh2WhB2ITkrgWk0DYrXhCzV4GV42XQ/jtBExmamdc+6q8160m4jHOTwNCiEApZZP0jmmLPjvcuqxi1XR833qr57QcD4dmqdAIozY+1sPsFBxbppR8i/H/KHlQUQ0HL4aZHeHXgTBmu8WKI8ex/XMV6K7Pj9lX8qDmZXSYALV7qY7c5S+odn36u0da0MNkkryz9CgpJsm/BzawTMkDbP4Abp6B4auyda083d3wdHejRun0QwgnpZi4HpPA1eh4TBJaVCmulbyN0cMrs8PhRYCAhoOtV2b+QuqhDtkPZzZYr1xHEHEG1rwKvi3UxLC0FPWFZ3+D29fg92E5c8JQVpzZqKzxRiOgYSaB7iyhdB31RdD1M7i4A6Y3h8C5aiRPNliw/wq7z9/k/R51qOht4UpMZzYqA6TlK1C1Y7ZFzww3Vxd8i3vQvIo3Lat6ayVvB7Sit5TUAGbVOkMRK4/v9R8B3tVUGOOUZOuWbS8S49RYcbeCMOCXjIdS+jZVPusre2D9G9lWWk7NrcuwciyUqQ/dv7FOmS6u0Or/4KU9ULaBGuI4r6cKj20BFyLi+PKPU3SoWZLBzXwtqzPuhvqSKF0POn/4GMJrnAWt6C3l4g64HWqdTti0uOaDzh+pz+TgHNgpKyWsex1unoVnZmftXqg/ANq9oybf7PuffWS0NUkJ6kUnUR3Qbu7WLd+7KoxYA09PhavB8EMrNULHlJJhluQUE28uOYK7mytf969vmeWcOu7/Xhz0n6UW+tbkeCxS9EKIbkKIM0KI80KIh2ZLCCHeFEKcFEIcFUJsFUJUNDv2nBDinPH3nDWFtyuHF0DB4pYHMMsutXtC+WZqgWU7z5J8bA7NgmNL1bKJVTpYlqfDv9QQ1U0fwJk/bCmdfdj4Hlw7An1/UqOMbIGLCzR5HsYfUBOr/vwXzO6aYZC0H3dc4EhINJ/38aOUl4UvnkOz4NwmeOJTKFXbisJrHEmWil4I4QpMB7oDdYDBQog6aZIdBppIKesDy4CvjbzFgY+A5kAz4CMhRDHriW8n7kaZBTCzkYWTGhoh9hrs/8E2ddiC0EDYOBGqd4U2b1mez8UF+vyk3BHLR8P147aT0dYE/6Z8561fh1o2MgTMKeIDQ35XS/dFXYSf2qrRW2Z9HsfDYvh+6zl6NyxHj/plLSv3xmnY9L66ls3G2Eh4jSOwxKJvBpyXUl6UUt4DFgO9zRNIKbdLKVPN0P1AqhP7SWCzlDJKSnkL2AxYdxaIPTi2FFLuqeUCbUnFluqLYc/3Kjqms3M3CpY+B55ljcUnsukJzO8Bg3+DAp7w27PKN5zTCD8B695Qwco6fWC/eoVQLrDxB6FOLzXSZ0YHCAsiISmFN34Pxrtwfj7pZeGon+RE9cLNX9iycf+aHIUlT6YPEGL2O9TYlxEvAKnf4hblFUKMFUIECCECIiIiLBDJjqQGMCvbEMr42b6+zh8p/+guK3Xm2QqTCVaMhbhwGDjv0YdKepVTyv7OTVg8VPm6cwoJt+H34Wqse//Zqq/F3hQuqQKlPfsbxEfBrM4Ezfo//r4RxTfPNKCIh4XhCbZ+AuHHlJJPjcejyTVYtTNWCDEMaAJkS0tJKWdIKZtIKZuULOlkq8hcO6IegEbD7VNfqVqqw/fQLIi6ZJ86H4W//q3WwO02RQVpexzK+SvfduhBFfMlJ4zEkVLNYL11GZ75BTwdvDBGrafg5f2EVx1Aq/BF7PV6n3b5z1qW98J21Sne5AWrh13QOAeWKPowwHxcVnlj3wMIIboAk4BeUsrE7OR1ag4vNAKYPWO/Ojv8C1zywbbP7FdndriwXbkK6g9S8XqsQd0+KtDbsSXwl20WtLAq+3+Ek6uhy0dQqbWjpQEgVhSiX+izvOX+CcU8XGHuU7DuzczXPbgbBateghI11Hh9Ta7EEkV/CKguhKgshMgPPAusMU8ghPAHfkYpeXNH659AVyFEMaMTtquxL2eQFK8UT+2eNllkIUO8ykLLl+H4MjWUzpmICVMzNUvWUoG3rOnLbfs2+A1UL7iTq61XrrX5e7+aNVrraWj1qqOluc+n605yLSaeoUNG4PLyPmjxMgTMgR9awrnND2eQEta+qtxm/WepPhNNriRLRS+lTAZeQSnoU8ASKeUJIcQnQoheRrJvgMLAUiFEsBBijZE3CvgU9bI4BHxi7MsZnF6vFofwt5PbxpzWr6nhnFs+sn/dGZEarCw5EQYtULN6rYkQKiZ7+Waw4kW1kLqzEReh2qCIr1N1Wm4+Gc6SgFBe7lCNRhWKqWvT7Ut4YbMKP7HoGdWmd80ev8MLVZC+zh+o0U+aXIsOapYZ83opH6y1Aphll30/wJ8TYdgK54j4uHGiGvr5zC9Qr5/t6om7ATM7gSkZxmx7pPguNsGUAgv6QsgBpUDL1ne0RABExiXy5NRdlPJ0Z9X41mqhEHOSE1WAtN3fqVWunvpGKfaf2kL5xjB8tWPub41VySyomb66GXHrClzaqTpGHfUQNH0BilZQVr3J5BgZUjmxUin55uNsq+RBjfoYvFgtRP3bYOeZQLbjS3VPPPWt0yh5KSX/WnmM2/HJ/GdQw4eVPKi5H50mwdgdKnzHsudhRkcVpqLPT1rJ5wH0Fc6I4F8BAQ2sGMAsu+QrAJ0+VAtLH1vqODlunoPVryiXyhOf2qfOMvWU3/jaEVg1zvEvurOb1JBX/2H2G4FlASuCwvjzRDhvP1mDmmXSjxZ5nzJ+MHordPlYxdDp/T81+UqT69GKPj1MKSrmTNWOKuKiI6nXX31mb/vMMWPM791RY8XzFYABc1UIXXtRszt0/VR1zO740n71puXWFVgxBkr7KWveSQiLjmfymhM0q1ycF9pYuDKTaz5o8zq8e1ENMtDkCbSiT49LOyEmxDGdsGlxcVEWWMzfamy9PZFSDc+LOK2sa0dYfy1fUVb0rq/hqAO+apIT1exfaVITw9wK2l+GdFAx5o9gkpJ/D8hGjPlUnKQTWWMftKJPj8MLoWAxqNXD0ZIoqnaEqp3U+PL4aPvVG/gLHF0MHSaq+h2BENDjP1CxjYqqGHLIvvVvnKhG//T5UUWQdBLm7bvM3guRfNizDr7F9bBITeZoRZ+Wu1Fwap0az+1MIVq7TIb4W7Bnqn3qCwuCP96Dal1USGFHki+/Gs7pVQ4WD1ELj9uDo0sgYLYaK1/7afvUaQHnb8Qx5Y/TdK5VioFNHOxa1OQItKJPy7FlkJLoVB1ugPLT+w1UMzJjbDy5+G4ULHkOCpWCfjOdY1SGR3EVsTE5UQVAS4y1bX03TqlFPiq0UvGHnISkFBNvLgnGI78rX/b306szaSzCCZ5gJ+PwAqVU7RHALLt0el/5ind8Ybs6TCZYOU6FSx4437nWdS1ZU61edeOUCqiWyaIbj0VirOqAzl/YWC3LeZZWnr79PEdDY/iirx+lPK28uIkm16IVvTnXjsD1o87RCZsexSpC0zFq6GcGi008Nru/g3N/qlmV5Rvbpo7HoVpnFUjtzAbYMtn65UupAqtFXVBRIT3LWL+OR+RoaDT/3Xaefv4+dPezMMa8RoNW9A9yeCG4FgA/OwYwyy7t3ob8nrZRchd3qmBl9Z6BpqOtX761aD5Wybd3mrpmWZCcYiI4JJprMfFZl33gZzU5rNMHULmtFYS1Dqkx5kt5FuCjXnUdLY4mh+E836SOJilBdb7V7qlG3DgrHsXVOOitH8PlPdaLnHj7qgpW5l0den7v/MPvun0Fkedh7etQrPJD7RB+O4GdZyPYeTaC3eduEhOfRIF8LrzauTpj21XBzTUdGyfkIGyaBDW6q9WinIivN57hQsQdFr7QnCIFLYwxr9EYaEWfyul1kBBtm8W/rU2Ll+DgTNj8IYze8vhKOSUJlj6vQg2MXK+CYDk7rvlgwDyY1QV+H8a957cQGFuUnWcj2HHmBqevq87aUp4F6FqnNG2ql2Dj8et88+cZ1gRf5Yt+fjSuaPZCv3NTBSvz8oG+PzpHB7TB3gs3mbPnEiNbVaJN9RKOFkeTA9GKPpXDC6FIBajc3tGSZI1bQeg4UfmST61Ri2w/DlsmQ8h+tUpSyZpWEdEehMTnJ7DOf3hiz2CuT+/J2MSPiXcpRJNKxXivWy061CxJrTKe90em9G7ow5aT4Xy4+jjP/LSXoc0r8G63Wnjld1HL6N25CS9scqovutsJSby95AhVShTivW61HC2OJoeiFT1A9N9wcQd0mOBUllymNBgC+6bDlo/VOrOuj/g5f3KNWl2o2Vjn7ptA+akPXIpi55kIdp69wYWIOwA87fku35sms73iXNxHLqdwwYxHo3SpU5oWVb35btNZ5u69xKYT4SyqtpXqF7crl1W5hvY6HYv4eM1JwmMTWf5SKwrmd3W0OJocilb0YAQwAxoOcawc2cE1n5pE9duzEDTv0TpPIy+o2aY+TaDr59aW8LGRUnLp5h3DHRPB/ouRJCabyJ/PhRZVvBnSvCLta5SkasmnEEFFKLH2VdjxEXT/KtNyCxfIx4c969DHvxzLFs+l6skf2VO4K5WqDMx0MWR7s/H4dZYHhfJq5+o09LXjwjeaXIdW9CYTHF4EVTqokMA5iRrd1ISeHV9B/Wez51u/d1eNFXfJZ/9gZZlwJzGZvRci2Xn2BjvPRhASpUbKVClRiMHNKtChZkmaV/Z+2Lpt/BzcPKu+TkrUUCGes6B+4Vj8UqYS5VmNV2KGkfifXbz5RA1GtqpEvvQ6a+3IzbhEJq08Rj0fL/6vUzWHyqLJ+WhFf2mnChjWxXlmP1qMEPDEJzC7i1JwHSZYlk9KWP8W3DgJw5Y5NEKnlJIz4bGGOyaCQ5ejSEqReOR3pVXVEoxtV5X21UtSwduCeC5PfKJCKm94B4pXUTGCMsIIViZSkvEe/TtrRFk+XH2cz9afYlVwGFP61aeeTxHrnWg2kFIyccUxYhOTWTywYfojhDSabKAV/eGFatWdWs4TyyRb+DaF2r1gzzS1UHfhUlnnCZoPR36F9hNULBs7ExOfxJ7zN+8r9+u3VfjlWmU8GdW6Mu1rlKRxpWIUyJdNn7SLq4qyOedJFXFy9FYoUT39tH9OgrBAGLgAvKviC8wZ2ZT1x67x8dqT9Prfbp5vXZk3n6hBoQL2fUyWBYay+WQ47/eoTfXSWcSY12gsIG8vJRh/C76tqT77n/rGPnXagpvnYXozaPI89Ph35mmvBsPsrmrc+dBlSjnaGJNJcvxqzH3FfjgkmhSTxNM9H+2ql6R9jZK0q1GSMkWsNKX/1hW1FKF7ETX8NG0Yh2PL1JyBlq/Akw/3TcTEJ/HVxtP8euBvfIoW5JPedelcu7R1ZMuCkKi7dP/+L+r5ePHr6Ba4ZDf8sCbPktlSgnlb0R+cCRvehhd35fzFkde9oSz18QczDqcbfwt+bq/WYn3xLyjkbTNxTCbJgUtRrDocxpZT4UTeuQdA/fJFaF9DKfeGvkVt5wv/ez/M6wm+zWH4yn9GJd04rV4CZfxg5LpMRysFXI5i4opjnLsRx1N+ZZjcsy6lvGwXX8ZkkgyeuZ8TV2+z8fW2lC+mww9rLCczRZ+3XTeHF0CZ+jlfyYNywxz5HbZ+ohbISIvJBCtfUjNgn//DZkr+9PXbrDwcxprgq1yLSaBQflc61y5Nx1olaVu9JCUK2yn0c4UW0Ou/sPJF9TJ/eqpaLWvJCMjvYQQry3xIapNKxVn/altm7LrAtG3n+evsTd7tXouhzSrYxNKes+cSBy5F8c0z9bWS11iVvKvorx1VQcy652CXjTmepaHVK7DzKwgNfDgg2d7v4ewf0P1r5de3Ilej41lz5CqrDodx+nos+VwE7WqUZOJTtXmidmnHjf9u8CxEnFGB2krUhLAAiDwHw1ep2PYWkD+fC690qk6P+uWYtPIYH6w6zsqgUL7sVz/rNVqzwbnwWL7+8wxP1CnNM43LW61cjQbysutmw7sQOBfeOu1coXgfh8RYmOavlNrIdf+ERrj0F8zvpWbQPvOLVeLYxMQnsfH4NVYeDuPApSikBP8KRenr70MPv7J428tyzwqTCZaOgFNr1e9OH6jAcI+AlJIVQWF8tv4ksQnJjG1XhVc7V8fd7fFeZEkpJvr+sIdr0Qn8+UY7+331aHIV2nWTlqQEOPq7WjUotyh5gAKe0P495ao4twlqPAmx12HZKPCuplwZj6HkE5NT2HEmglWHw9h6+gb3kk1UKVGI1zvXoI9/OSp6F7LiyVgJFxfo+7MKb+BZFtq8+chFCSHo37g8HWuV4vP1p/hhxwXWH7vG5338HisGzX+3ned42G1+GtZYK3mNTcibFv3x5Ur5DV/puLVQbUVKkhqBk88dxu6ABX3VmqdjtkGp2tkuzmSSBFy5xcrDYWw4do2Y+CRKFM7P0/XL0dffh/rli+SMVY6ktHpEzr3nbzJp1XEu3bxDX38f3u9RO9tfMsEh0fT/cS99Gvrw74G5oK9I4zD0qJu0LOirJta8djTnxLbJDidWqkiM5fyVku83E+oPzFYRZ8NjWXU4jNXBVwmLjqegmyvd6pWhd8NytKlWwuEzR52FhKQUpm8/z087L1CoQD7+9VRtBjQub9HLL/5eCj2m/UVisok/Xm+Ll7sOP6x5dLTrxpzoELiwXbk4cqOSB6jTB8o1gqtB0OQFi5V8+O0E1gRfZeXhME5eu42ri6Bt9RK882RNnqhT2u4Th3IC7m6uvNW1Jr0alGPiimO8u+woK4JC+aKvH1VKZh6S4quNp7l48w6/jmmulbzGpuS9JzcnBjDLLkJA7+lwdDF0/uk0xAAAEMZJREFUnJRp0tiEJDYev86q4DD2XohESmjgW5SPetbh6frlKOmpfcaWUL20J0tebMniQyF8+ccpuk39i/EdqzGuQ5V0Z/juPneTuXsvM6p1ZVpV1THmNbYlb7luTCaY1kDFQRmx2jZ15ADuJZvYdTaClcFhbDkZTmKyiYreHvRp6EMffx8ql3DCTtUcxI3YBD5Ze5J1R69RtWQhvuxXn2aV/+n0j4lPotvUXXjkd2X9q20fe9SORgPadfMPl/9Ssec758AAZo+JlJLAK7dYFRzGuqPXiL6bRPFC+Xm2qS+9/X3w9y2aMzpVcwClPN3535BG9G98g/dXHmfgz/t4tqkvE7vXpoiHGx+vOcGN2ERWvtxKK3mNXchbiv7wAhX/pFYPR0tiN87fiGN1cBirgsMIiYrH3c2FrnXK0NffhzbVS+jIiDakY81SbH6zHVO3nGP27ktsORVOrwY+rDgcxutdqlO/vI4xr7EPeUfRx99Sqyk1GqGW4svlBF65xeQ1JzgWFoOLgNbVSvBGlxp0rVuGwrpT1W545FcjcXo3VJ21c/ZcokH5IozvqGPMa+xH3nnijy+HlMScsfj3Y3I8LIaRcw7iVdCND56uQ88GZSnlabtgXJqsqVuuCCtfbs0fx6/RtFJx/SWlsSt5R9EHLYDSfrkjgFkmXIyI47k5B/F0z8eScS3xKZr7v15yCq4ugqfrWxZjR6OxJnnDrLh+DK4FQ6PhVp8d6UxcjY5n+OyDSGDB6OZayWs0GiCvKPrDC8E1P/gNcLQkNiMyLpHhsw9wOz6J+aOaUTWLyToajSbvkPtdN8mJKoBZrR65K4CZGbEJSYz85RCht+KZP6qZw9Y61Wg0zolFFr0QopsQ4owQ4rwQ4qEVqIUQ7YQQQUKIZCHEM2mOpQghgo2/NdYS3GLObFAjbvyH271qe5CQlMLoeQGcunabH4c1onkV260apdFociZZWvRCCFdgOvAEEAocEkKskVKeNEv2NzASSC/Qd7yUsqEVZH00ghaAV3mo0sFhItiKpBQT4xcFcfByFFMHNaRTLfusa6rRaHIWllj0zYDzUsqLUsp7wGKgt3kCKeVlKeVRwGQDGR+dmFC4sA38h9plEWx7YjJJ3ll6hK2nb/BJ73r0bujjaJE0Go2TYomi9wFCzH6HGvssxV0IESCE2C+E6JNeAiHEWCNNQERERDaKzoLg3wCZ6wKYSSmZvPYEq4Kv8s6TNRneoqKjRdJoNE6MPUbdVDQC7QwBpgohqqZNIKWcIaVsIqVsUrJkSevUajKpkAeV20GxStYp00n4z+azzN93hTFtK/Nyh4eaU6PRaB7AEkUfBvia/S5v7LMIKWWY8f8isAPwz4Z8j86V3RB9Bfz/v717D5KqPtM4/n24CAJyFxQYARXlqqADgiTejWhCXAxZiQsCwRB3YzRZt3aN5WWjrrFMsmvcaIgiYIHBJd7QqBhKNCFrBAYQBUYWBOWiEHAAkcvADO/+0Y0OBEPDDHOmzzyfKqq6z5lz+uUH80zP7/R5f9dWy8tVl/GzV/LgrBVcXVjArVd0cyMyMzukXIJ+HtBFUmdJxwDDgJw+PSOphaQG2cetgYHA0r99VBVZOAUaNMusC5sS04rWcM+LxVze8wTuvaqXQ97McnLIoI+IMuAG4BWgGJgWEUsk3SXp6wCS+kpaC3wT+LWkJdnDuwFFkhYBrwH3HfBpnaNj5xZYOh16DU1NA7MZi9dzy9Nv8+UurXlgWG/q1nHIm1lucrphKiJeAl46YNsdFR7PIzOlc+BxbwC9Klnj4Vv8NJTtyrQ8SIE/Ld/EjVMXcmZBc8YNP/ugKxaZmX2RdLZAWDgF2vaEE5P7+H5VWbh6M2MnF9G5dWMmjurrdVvN7LClL+g3LMksit1neN43MFu2fhujJs6jdZMGTB7Tj+aNjkm6JDPLQ+kL+n0NzM64OulKKmX1xzsY8dgcGtSrwxPXnUObpu4nb2ZHJl3zAGWlsOhJOP2KvG5g9pdPdjH8sTnsLt/LtO8OoKBlo6RLMrM8lq539Mtehp0leX0RdsuO3Yx4bC6bPi1l4qi+nNb2uKRLMrM8l66gXzgFmraHky9MupIjsr20jNGT5rFq03YevbaQPie1SLokM0uB9AT91nXw3quZvjZ52MCstKyc66fMZ9GaLTz4rT4MPLV10iWZWUqkZ46+USu46lEo6Jd0JYetrHwvN019i9nLN/HToWcwqOcJSZdkZimSnqCv3zBzJ2yeiQhuffYdZixZz+1f6843CwsOfZCZ2WFIz9RNHooI7n2pmGlFa7nxolMZ86XOSZdkZinkoE/Qw6+/x6OzVzFyQEd+eOlpSZdjZinloE/I5Dc/4KevLGNIn/bcObiHO1Ga2VHjoE/A9LfWccf0xVzSrQ33Dz2DOu5EaWZHkYO+ms16dwM3T1tEv04t+eU1Z1G/rv8JzOzocspUozkrP+Yfpyyg24lNGT+ykIb18+/z/maWfxz01WTxuq1c93gRHVocy6TRfTmuYf2kSzKzWsJBXw3e2/gpIyfMpemx9Zk85hxaNWmQdElmVos46I+ydVt2MmL8HCSYPKYf7ZqnY2lDM8sf6bkztgba9GkpI8bPYVtpGU+O7c/JxzdJuiQzq4X8jv4o+WTXHkZOmMuHW3cyYVRferRrlnRJZlZLOeiPgl17yrluUhHL1m/jV8PPpm+n/F0Exczyn6duqtiuPeX80xMLmPdBCQ8O68OFp7dJuiQzq+Uc9FXo1eIN/PsLS1hTspP/GNKTwWe2S7okMzMHfVVYu3kHP35hKTOXbqBLmyZM/U5/BpzSKumyzMwAB32llJaVM372Kv571nKEuOXyrnx7YGeOqedLH2ZWczjoj9D/rtjE7dMXs3Ljdgb1OIHbB3envT8jb2Y1kIP+MG34ZBf3vFjMC4s+pGOrRkwc3dcXXM2sRnPQ56isfC+P//kD/mvm/7G7fC8/uKQL159/ihuTmVmN56DPQdH7Jdz23GLeXb+N8087nruu7EHHVo2TLsvMLCcO+r/h409Lue/ld/nt/LW0a9aQccPP4rIeJ3g1KDPLKw76g9i7N5g6bzX3z1jG9tIyrj//FG68+FQaHePhMrP84+Q6wDtrt3Lb9MUsWrOF/ie35O4re9Kl7XFJl2VmdsQc9Flbd+7h579fxuQ3P6BV4wY8cHVvruzdztM0Zpb3an3QRwTPLFjHT14upmT7bkYO6MQ/f+U0mnoFKDNLiVod9MvWb+P26YuZu6qE3gXNmTS6Hz3bu52wmaVLrQz67aVl/OLV5Uz40yqaNKzHfVf14u8LC6hTx9M0ZpY+OTVlkTRI0jJJKyTdcpD950laIKlM0tAD9o2UtDz7Z2RVFX4kIoIX3/6Ii3/+Bx7540qGnt2BWTdfwLB+JznkzSy1DvmOXlJd4CHgUmAtME/S8xGxtMKXrQZGAf9ywLEtgTuBQiCA+dljN1dN+blbtWk7d0xfzOzlm+h+YlMe+oezOLtji+ouw8ys2uUyddMPWBERKwEkPQlcCXwW9BHxfnbf3gOOvQyYGREl2f0zgUHA1EpXnqNde8p5+LUVjPvDShrUq8Odg7szon9H6tV1h0kzqx1yCfr2wJoKz9cC5+R4/oMd2z7HYytt1rsbuPP5zEIgf9e7Hbde0Y02TRtW18ubmdUINeJirKSxwFiAk046qdLnq7gQyKltmvCb75zDuae0rvR5zczyUS5Bvw4oqPC8Q3ZbLtYBFxxw7OsHflFEPAI8AlBYWBg5nvuv7C7by6OzV362EMi/DerKmC95IRAzq91yCfp5QBdJnckE9zDgmhzP/wpwr6R9Vz2/AvzosKvMwZqSHYyaOJf3Nm7nsh5tuWNwDy8EYmZGDkEfEWWSbiAT2nWBCRGxRNJdQFFEPC+pL/As0AIYLOnHEdEjIkok3U3mhwXAXfsuzFa1tk0b0qlVY277ancu7OqFQMzM9lHEEc+UHBWFhYVRVFSUdBlmZnlF0vyIKDzYPk9em5mlnIPezCzlHPRmZinnoDczSzkHvZlZyjnozcxSzkFvZpZyDnozs5SrcTdMSdoIfFCJU7QGNlVROfnOY7E/j8f+PB6fS8NYdIyI4w+2o8YFfWVJKvqiu8NqG4/F/jwe+/N4fC7tY+GpGzOzlHPQm5mlXBqD/pGkC6hBPBb783jsz+PxuVSPRerm6M3MbH9pfEdvZmYVOOjNzFIuNUEvaZCkZZJWSLol6XqSJKlA0muSlkpaIummpGtKmqS6khZK+l3StSRNUnNJT0l6V1KxpAFJ15QkST/Mfp8sljRVUsOka6pqqQh6SXWBh4DLge7AtyR1T7aqRJUBN0dEd6A/8L1aPh4ANwHFSRdRQ/wCmBERXYEzqcXjIqk9cCNQGBE9ySyXOizZqqpeKoIe6AesiIiVEbEbeBK4MuGaEhMRH0XEguzjbWS+kdsnW1VyJHUAvgqMT7qWpElqBpwHPAYQEbsjYkuyVSWuHnCspHpAI+DDhOupcmkJ+vbAmgrP11KLg60iSZ2APsCcZCtJ1APAvwJ7ky6kBugMbAQmZqeyxktqnHRRSYmIdcDPgNXAR8DWiPh9slVVvbQEvR2EpCbA08APIuKTpOtJgqSvAX+JiPlJ11JD1APOAn4VEX2A7UCtvaYlqQWZ3/47A+2AxpKGJ1tV1UtL0K8DCio875DdVmtJqk8m5J+IiGeSridBA4GvS3qfzJTeRZKmJFtSotYCayNi3294T5EJ/trqEmBVRGyMiD3AM8C5CddU5dIS9POALpI6SzqGzMWU5xOuKTGSRGYOtjgi/jPpepIUET+KiA4R0YnM/4tZEZG6d2y5ioj1wBpJp2c3XQwsTbCkpK0G+ktqlP2+uZgUXpyul3QBVSEiyiTdALxC5qr5hIhYknBZSRoIjADekfRWdtutEfFSgjVZzfF94Insm6KVwOiE60lMRMyR9BSwgMyn1RaSwnYIboFgZpZyaZm6MTOzL+CgNzNLOQe9mVnKOejNzFLOQW9mlnIOejOzlHPQm1VCthGWWY3moLfUkvScpPnZXuNjs9sGSVogaZGkV7PbmkiaKOkdSW9L+kZ2+6cVzjVU0qTs40mSxkmaA9wvqZ+kP2ebhL2x767TbA/8n2X7nL8t6fuSLpL0XIXzXirp2eobFauN/G7E0uzbEVEi6VhgnqTpwKPAeRGxSlLL7NfdTqZrYS/4rNHVoXQAzo2IcklNgS9n79C+BLgX+AYwFugE9M7uawlsBh6WdHxEbCRzV+qEqvsrm/01B72l2Y2ShmQfF5AJ3j9GxCqAiCjJ7ruECotNRMTmHM7924gozz5uBjwuqQsQQP0K5x0XEWUVX0/SZGC4pInAAODaI/z7meXEQW+pJOkCMkE7ICJ2SHodeAvoehinqdgf5MDl5bZXeHw38FpEDMn2/3/9EOedCLwA7CLzA6PsMGoyO2yeo7e0agZszoZ8VzJLKjYEzpPUGaDC1M1M4Hv7DqwwdbNBUjdJdYAhfLFmfN4We1SF7TOB7+67YLvv9SLiQzKrGN1GJvTNjioHvaXVDKCepGLgPuBNMisrjQWekbQI+J/s194DtMheNF0EXJjdfgvwO+ANMqsPfZH7gZ9IWsj+vyWPJ9MG9+3sea+psO8JYE1EpK4lrtU87l5plgBJvwQWRsRjSddi6eegN6tmkuaTmeO/NCJKk67H0s9Bb2aWcp6jNzNLOQe9mVnKOejNzFLOQW9mlnIOejOzlPt/J68Xz6guI6gAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds_results = train(mode=\"non_implicit\", solver='direct')"
      ],
      "metadata": {
        "id": "vvyyEM_yaeqY",
        "outputId": "394ba631-cf21-4df7-e4ad-7f92cc605d87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM TRAINING IMAGES:::2500\n",
            "NUM TEST IMAGES:::250\n",
            "TRAINING MODE:::non_implicit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> epoch 1, batch 1 :: batch_loss = 2.3379976749420166, batch_acc = 0.1015625, batch_res = 0.5847600698471069 \n",
            "> epoch 1, batch 2 :: batch_loss = 2.3374249935150146, batch_acc = 0.109375, batch_res = 0.5406241416931152 \n",
            "> epoch 1, batch 3 :: batch_loss = 2.331136703491211, batch_acc = 0.125, batch_res = 0.5126231908798218 \n",
            "> epoch 1, batch 4 :: batch_loss = 2.3317480087280273, batch_acc = 0.171875, batch_res = 0.47496968507766724 \n",
            "> epoch 1, batch 5 :: batch_loss = 2.3058223724365234, batch_acc = 0.1484375, batch_res = 0.4473017752170563 \n",
            "> epoch 1, batch 6 :: batch_loss = 2.302008628845215, batch_acc = 0.1171875, batch_res = 0.43288472294807434 \n",
            "> epoch 1, batch 7 :: batch_loss = 2.303187370300293, batch_acc = 0.1171875, batch_res = 0.4120209813117981 \n",
            "> epoch 1, batch 8 :: batch_loss = 2.3013863563537598, batch_acc = 0.1015625, batch_res = 0.4050143361091614 \n",
            "> epoch 1, batch 9 :: batch_loss = 2.3102943897247314, batch_acc = 0.0703125, batch_res = 0.3903750479221344 \n",
            "> epoch 1, batch 10 :: batch_loss = 2.298142433166504, batch_acc = 0.140625, batch_res = 0.37675046920776367 \n",
            "> epoch 1, batch 11 :: batch_loss = 2.2899117469787598, batch_acc = 0.125, batch_res = 0.38081619143486023 \n",
            "> epoch 1, batch 12 :: batch_loss = 2.313086986541748, batch_acc = 0.1015625, batch_res = 0.3864286243915558 \n",
            "> epoch 1, batch 13 :: batch_loss = 2.2974793910980225, batch_acc = 0.1875, batch_res = 0.39258068799972534 \n",
            "> epoch 1, batch 14 :: batch_loss = 2.300891876220703, batch_acc = 0.125, batch_res = 0.39421021938323975 \n",
            "> epoch 1, batch 15 :: batch_loss = 2.287135124206543, batch_acc = 0.1015625, batch_res = 0.3962180018424988 \n",
            "> epoch 1, batch 16 :: batch_loss = 2.296354055404663, batch_acc = 0.0859375, batch_res = 0.41864562034606934 \n",
            "> epoch 1, batch 17 :: batch_loss = 2.292194366455078, batch_acc = 0.1015625, batch_res = 0.4086776077747345 \n",
            "> epoch 1, batch 18 :: batch_loss = 2.3170464038848877, batch_acc = 0.078125, batch_res = 0.40094518661499023 \n",
            "> epoch 1, batch 19 :: batch_loss = 2.3046064376831055, batch_acc = 0.0625, batch_res = 0.4199674725532532 \n",
            "> epoch 1, batch 20 :: batch_loss = 2.2819020748138428, batch_acc = 0.1617647111415863, batch_res = 0.42963045835494995 \n",
            "\tTRAIN epoch = 1 / loss = 2.3075900077819824 / acc = 0.11559999734163284 / res = 0.43028759956359863\n",
            "\tVAL epoch = 1 / loss = 2.2807037830352783 / acc = 0.13600000739097595 / res = 0.45658251643180847\n",
            "> epoch 2, batch 1 :: batch_loss = 2.288792133331299, batch_acc = 0.1328125, batch_res = 0.44917821884155273 \n",
            "> epoch 2, batch 2 :: batch_loss = 2.3047003746032715, batch_acc = 0.109375, batch_res = 0.480235755443573 \n",
            "> epoch 2, batch 3 :: batch_loss = 2.2932188510894775, batch_acc = 0.1171875, batch_res = 0.48313653469085693 \n",
            "> epoch 2, batch 4 :: batch_loss = 2.3036842346191406, batch_acc = 0.078125, batch_res = 0.49971479177474976 \n",
            "> epoch 2, batch 5 :: batch_loss = 2.2795679569244385, batch_acc = 0.125, batch_res = 0.49813392758369446 \n",
            "> epoch 2, batch 6 :: batch_loss = 2.289017915725708, batch_acc = 0.1484375, batch_res = 0.5170324444770813 \n",
            "> epoch 2, batch 7 :: batch_loss = 2.2771782875061035, batch_acc = 0.1640625, batch_res = 0.5210187435150146 \n",
            "> epoch 2, batch 8 :: batch_loss = 2.2913131713867188, batch_acc = 0.21875, batch_res = 0.5207697153091431 \n",
            "> epoch 2, batch 9 :: batch_loss = 2.2863848209381104, batch_acc = 0.1875, batch_res = 0.5128363370895386 \n",
            "> epoch 2, batch 10 :: batch_loss = 2.2776942253112793, batch_acc = 0.1953125, batch_res = 0.48924750089645386 \n",
            "> epoch 2, batch 11 :: batch_loss = 2.2820701599121094, batch_acc = 0.1875, batch_res = 0.47799330949783325 \n",
            "> epoch 2, batch 12 :: batch_loss = 2.2860279083251953, batch_acc = 0.1796875, batch_res = 0.46695899963378906 \n",
            "> epoch 2, batch 13 :: batch_loss = 2.2826547622680664, batch_acc = 0.1328125, batch_res = 0.46254026889801025 \n",
            "> epoch 2, batch 14 :: batch_loss = 2.2939183712005615, batch_acc = 0.15625, batch_res = 0.4607929587364197 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3d6eB0U2bQcY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}