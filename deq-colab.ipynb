{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "920f83e2-0b3a-4177-d8e4-fc5d81ec6e6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "# jax.config.update(\"jax_check_tracer_leaks\", True)\n",
        "# import os\n",
        "# os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"1\"\n",
        "# jax.checking_leaks = True\n",
        "# jax.check_tracer_leaks = True\n",
        "\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(g, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False, name=\"unknown\"):\n",
        "    bsz, total_hsize, seq_len = x0.shape\n",
        "    # g = lambda y: f(y) - y\n",
        "    dev = x0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    x_est = x0           # (bsz, 2d, L')\n",
        "    gx = g(x_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_xest, lowest_gx = 0, x_est, gx\n",
        "\n",
        "    while nstep < threshold:\n",
        "        x_est, gx, delta_x, delta_gx, ite = line_search_jax(update, x_est, gx, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gx)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gx + x_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_xest, lowest_gx = jnp.copy(x_est), jnp.copy(gx)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_x)\n",
        "        u = (delta_x - matvec_jax(part_Us, part_VTs, delta_gx)) / jnp.einsum('bij, bij -> b', vT, delta_gx)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_xest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_xest\n",
        "\n",
        "\n",
        "def newton_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    # note: f might ignore x0 (i.e. with backward pass)\n",
        "    orig_shape = z0.shape\n",
        "    def g(y):\n",
        "      # this reshaping is to enable solving with Jacobian\n",
        "      return (f(y.reshape(*orig_shape),x0)-y.reshape(*orig_shape)).reshape(-1)\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    z = z0.reshape(-1)\n",
        "    gz = g(z)\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    nstep = 0\n",
        "\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      # solve system\n",
        "      jgz = jac_g(z)\n",
        "      # print(\"gz\",gz.shape,jnp.linalg.norm(gz))\n",
        "      # print(\"jgz\",jgz.shape,jnp.linalg.norm(jgz))\n",
        "      delta_z = jnp.linalg.solve(jgz,-gz)\n",
        "      # print(\"delta_z\",delta_z.shape,jnp.linalg.norm(delta_z))\n",
        "      z = z + delta_z\n",
        "      # need to compute gx here to decide whether to stop\n",
        "      gz = g(z)\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      nstep += 1\n",
        "\n",
        "    print(\"gz_norm\",gz_norm)\n",
        "    z = z.reshape(*orig_shape).astype(jnp.float32)\n",
        "\n",
        "    # assert False\n",
        "\n",
        "    return z\n",
        "\n",
        "def direct_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    nstep = 0\n",
        "    z_old = z0\n",
        "    z_new = f(z0,x0)\n",
        "    gz = z_new-z_old\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    min_gz_norm, min_z = gz_norm, z_new\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      z_old = z_new\n",
        "      z_new = f(z_old,x0)\n",
        "      gz = z_new-z_old\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      if gz_norm < min_gz_norm:\n",
        "        min_gz_norm, min_z = gz_norm, z_new\n",
        "      nstep += 1\n",
        "    print(\"min_gz_norm\",min_gz_norm,\"nstep\",nstep)\n",
        "    return min_z\n"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    \"\"\"\n",
        "\n",
        "    AKA the green block in the diagram\n",
        "    \"\"\"\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3)  # can also be (5, 5), modify later\n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "''' \n",
        "    assert statement we'll need    \n",
        "    assert that the number of branches == len(input_channel_vector)\n",
        "    assert also that num_branches == len(kernel_size_vector)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "EzcfTWz6agE_",
        "outputId": "1b81967f-c6c7-4860-8e91-3305dc7e8665"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    assert statement we'll need    \\n    assert that the number of branches == len(input_channel_vector)\\n    assert also that num_branches == len(kernel_size_vector)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def _downsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3, 3), strides=(2,2), padding=1, use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            #down_block.append(module)\n",
        "        seq = nn.Sequential(down_block)\n",
        "        return seq\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        #self.downsample_fn = self._downsample()\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            \n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(conv_down)\n",
        "            #down_block.append(group_down)\n",
        "                 \n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(module)\n",
        "            \n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "        #self.layers = down_block\n",
        "        #print('seq', self.layers)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        '''\n",
        "        z = z_plus\n",
        "        for i, lyr in enumerate(self.layers[:-1]):\n",
        "            print(i, z.shape)\n",
        "            z = lyr(z)\n",
        "            z = nn.relu(z)\n",
        "            print(i, z.shape)\n",
        "            #z = nn.sigmoid(z)  # nn.silu(z)  # jnp.tanh(z)  # nn.sigmoid(z)\n",
        "        out = self.layers[-1](z)\n",
        "        '''\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cringy_reshape(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        raise ValueError\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps image to initial latent representation\n",
        "    AKA the grey part in the diagram\n",
        "    \"\"\"\n",
        "\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    training: bool = True\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def setup(self):\n",
        "        # self.num_branches = len(self.branches)\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.channels[1], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        x = self.relu(self.bn2(self.conv2(x), use_running_average=True))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A tool for using the \n",
        "    \"\"\"\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def _make_layer(self, inplanes, planes):\n",
        "          downsample = False\n",
        "          if inplanes != planes * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(inplanes, planes, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        incre_modules = []\n",
        "        for i, channels  in enumerate(self.channels):\n",
        "            incre_mod = self._make_layer(self.channels[i], self.output_channels[i])\n",
        "            incre_modules.append(incre_mod)\n",
        "        self.incre_modules = incre_modules\n",
        "        downsamp_modules = []\n",
        "        for i in range(len(self.channels)-1):\n",
        "            in_channels = self.output_channels[i] * self.expansion\n",
        "            out_channels = self.output_channels[i+1] * self.expansion\n",
        "            downsamp_module = nn.Sequential([nn.Conv(out_channels, kernel_size=(3,3), strides=(2,2), use_bias=True),\n",
        "                                            #nn.BatchNorm(),\n",
        "                                            nn.relu])\n",
        "            downsamp_modules.append(downsamp_module)\n",
        "        self.downsamp_modules = downsamp_modules\n",
        "\n",
        "        self.final_layer = nn.Sequential([nn.Conv(self.final_chansize, kernel_size=(1,1)),\n",
        "                                         #nn.BatchNorm(),\n",
        "                                         nn.relu])\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, z_list):\n",
        "        z = self.incre_modules[0](z_list[0])\n",
        "        for i in range(len(self.downsamp_modules)):\n",
        "            z = self.incre_modules[i+1](z_list[i+1]) + self.downsamp_modules[i](z)\n",
        "        z = self.final_layer(z)\n",
        "        z = nn.avg_pool(z, window_shape=z.shape[1:3])\n",
        "        z = jnp.reshape(z, (z.shape[0], -1))\n",
        "        z = self.classifier(z)\n",
        "        return z"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    # image = np.tile(image, (1,1,1,24))\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data, train_ds.targets)\n",
        "    test_images, test_labels = transform(test_ds.data, test_ds.targets)\n",
        "    print(f\"MUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"MUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 z: jnp.ndarray,\n",
        "                 x: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, z, x, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray,\n",
        "                      x: jnp.ndarray):\n",
        "    z = rootfind(solver_fn, f_fn, threshold, eps, weights, z, x)\n",
        "    return z, (weights, z, x)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    weights, z, x = res\n",
        "    (_, vjp_fun) = jax.vjp(f_fn, weights, z, x)\n",
        "    def z_fn(z,x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "        return Jz_T + grad\n",
        "    z0 = jnp.zeros_like(grad)\n",
        "    x0 = None # dummy, z_fn does not use x\n",
        "    g = solver_fn(z_fn, z0, x0, threshold, eps)\n",
        "    return (None, g, None)\n",
        "\n",
        "rootfind.defvjp(_rootfind_fwd, _rootfind_bwd)"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQFF(nn.Module):\n",
        "    \"\"\"\n",
        "    The f_{\\theta}(z,x) function that is repeatedly applied\n",
        "    AKA the yellow block in the diagram\n",
        "    \"\"\"\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    #features: Tuple[int] = (16, 4)\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    \n",
        "    # branches: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "\n",
        "\n",
        "    #  TODO HERE\n",
        "    # UnfilteredStackTrace: jax.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (64, 32640, 1) and (64, 32, 32, 24)\n",
        "   \n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # self.downsample = DownSample(channels=self.channels,\n",
        "        #                              num_groups=self.num_groups)\n",
        "        # self.upsample = UpSample(channels=self.channels,\n",
        "        #                          num_groups=self.num_groups)\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    # array.append(z_plus[i])\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                 #channel_dimension=channel_dimensions)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                # channel_dimension=channel_dimensions)\n",
        "                    # array.append(nn.Module(sampled))\n",
        "                    array.append(sampled)\n",
        "            # fuse_layers.append(nn.Module(array))\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.relu,\n",
        "                                          nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  #kernel_init=self.kernel_init, \n",
        "                                                  use_bias=False),\n",
        "                                          nn.GroupNorm(num_groups=self.num_groups//2)]))\n",
        "                                                       #group_size=self.channels[i])]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, z, x, shape_tuple):\n",
        "        \n",
        "        batch_size = z.shape[0]\n",
        "        z_list = cringy_reshape(z,shape_tuple)\n",
        "        x_list = cringy_reshape(x,shape_tuple)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](z_list[i], i, x_list[i])) # z, branch, x\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "          # stick z back into into one vector\n",
        "        fuse_outputs = jnp.concatenate([fo.reshape(batch_size,-1) for fo in fuse_outputs],axis=1)\n",
        "        assert fuse_outputs.shape[1] == z.shape[1]\n",
        "        return fuse_outputs\n"
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdeq_inputs(x,num_branches):\n",
        "\n",
        "  batch_size = x.shape[0]\n",
        "  x_list = [x]\n",
        "  for i in range(1, num_branches):\n",
        "      bs, H, W, y = x_list[-1].shape\n",
        "      new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "      x_list.append(new_item)\n",
        "  z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "  shape_list = [el.shape for el in z_list]\n",
        "  # make them (batched) vectors\n",
        "  x_vec = jnp.concatenate([x.reshape(batch_size,-1) for x in x_list],axis=1)\n",
        "  z_vec = jnp.concatenate([z.reshape(batch_size,-1) for z in z_list],axis=1)\n",
        "  # i'm not sure if tuple is actually important but I like it for non-mutability\n",
        "  shape_tuple = tuple(shape_list)\n",
        "  return x_vec, z_vec, shape_tuple\n",
        "\n",
        "\n",
        "def mdeq_fn(x,encoder,decoder,deqff,all_weights,solver_fn):\n",
        "    # TODO move threshold and eps out of this function\n",
        "\n",
        "    threshold = 7\n",
        "    eps = 1e-3\n",
        "    encoder_weights = all_weights[\"encoder\"]\n",
        "    decoder_weights = all_weights[\"decoder\"]\n",
        "    deqff_weights = all_weights[\"mdeqff\"]\n",
        "    batch_size = x.shape[0]\n",
        "    # transform the input image\n",
        "    x = encoder.apply(encoder_weights,x)\n",
        "    # construct inputs (lots of padding and concatenation)\n",
        "    x, z, shape_tuple = create_mdeq_inputs(x,deqff.num_branches)\n",
        "    # the root function can only take 3 ndarrays as input\n",
        "    # this is why it needs the shape_tuple\n",
        "    def deqff_root(_weights,_z,_x):\n",
        "      # note: it's safe to pass the shape_tuple here (no tracers)\n",
        "      return deqff.apply(_weights,_z,_x,shape_tuple)\n",
        "    # apply rootfinder with custom vjp\n",
        "    z = rootfind(solver_fn,deqff_root,threshold,eps,deqff_weights,z,x)\n",
        "    # reshape back to list\n",
        "    z_list = cringy_reshape(z,shape_tuple)\n",
        "    log_probs = decoder.apply(decoder_weights,z_list)\n",
        "    return log_probs"
      ],
      "metadata": {
        "id": "5hI7s8DE1kqX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    '''\n",
        "    extra thing: warm-up using gradient descent in pytorch code of official repo\n",
        "    --> check impact of that and maybe also cost etc (eg if only one layer etc)\n",
        "    '''\n",
        "\n",
        "    max_itr = 7 \n",
        "    print_interval = 5\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    image_size = train_images.shape[1]\n",
        "    batch_size = 64\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "    print(num_images,image_size)\n",
        "\n",
        "    solver_fn = direct_jax\n",
        "\n",
        "    num_groups = 8\n",
        "    channels = [24, 24]\n",
        "    num_branches = 2\n",
        "\n",
        "    # instantiation\n",
        "    encoder = Encoder(channels=channels)\n",
        "    decoder = Classifier() # not sure about what to pass\n",
        "    mdeqff = MDEQFF(num_branches=num_branches, channels=channels, num_groups=num_groups)\n",
        "\n",
        "    # weight initialization\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng, _ = jax.random.split(prng, 2)\n",
        "    x_dummy = jnp.ones((batch_size, image_size, image_size, 24))\n",
        "    print(\"x_dummy\",x_dummy.shape)\n",
        "    x_dummy_2, encoder_weights = encoder.init_with_output(prng,x_dummy)\n",
        "    print(\"x_dummy_2\",x_dummy_2.shape)\n",
        "    x_dummy_3, z_dummy, shape_tuple = create_mdeq_inputs(x_dummy_2,num_branches)\n",
        "    print(\"x_dummy_3\",x_dummy_3.shape)\n",
        "    print(\"z_dummy\",z_dummy.shape)\n",
        "    z_dummy_2, mdeqff_weights = mdeqff.init_with_output(prng,z_dummy,x_dummy_3,shape_tuple)\n",
        "    print(\"z_dummy_2\",z_dummy_2.shape)\n",
        "    z_dummy_3 = cringy_reshape(z_dummy_2,shape_tuple)\n",
        "    # z_dummy_3 is a list\n",
        "    o_dummy, classifier_weights = decoder.init_with_output(prng,z_dummy_3)\n",
        "    print(\"o_dummy\",o_dummy.shape)\n",
        "\n",
        "    # collect weights\n",
        "    weights = {'encoder': encoder_weights, 'mdeqff': mdeqff_weights ,'decoder': classifier_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true):\n",
        "        logits = mdeq_fn(x_batch,encoder,decoder,mdeqff,weights,solver_fn)\n",
        "        return loss_fn(logits, y_true)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true):\n",
        "        (loss_vals, acc), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler():\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "        return shuffled_indices\n",
        "\n",
        "    max_epoch = 7 \n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    for epoch in range(max_epoch):\n",
        "        idxs = list_shuffler()\n",
        "        start, end = 0, 0\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab,...]\n",
        "            x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "  \n",
        "            weights, opt_state, batch_loss, batch_acc = step(weights=weights,\n",
        "                                                 opt_state=opt_state,\n",
        "                                                 x_batch=x_batch,\n",
        "                                                 y_true=y_true)\n",
        "            loss_vals.append(batch_loss)\n",
        "            acc_vals.append(batch_acc)\n",
        "\n",
        "\n",
        "            print(f\"batch_loss :: {batch_loss} // batch_acc :: {batch_acc}\")\n",
        "            # loss_vals, grads = jax.value_and_grad(loss, has_aux=False)(optimizer.target, x_batch, y_true)\n",
        "            # optimizer = optimizer.apply_gradient(grads)\n",
        "\n",
        "        epoch_loss = jnp.average(jnp.array(loss_vals))\n",
        "        epoch_acc = jnp.average(jnp.array(acc_vals))\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"\\tat epoch\", epoch, \"have loss\", epoch_loss, \"and acc\", epoch_acc)\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss) "
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breakdown of code overall:\n",
        "\n",
        "\n",
        "*   MDEQ modul\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "7RvQXU3CROAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e041ca4-1950-4af9-d051-144705c7a6d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUM TRAINING IMAGES:::60000\n",
            "MUM TEST IMAGES:::10000\n",
            "60000 28\n",
            "x_dummy (64, 28, 28, 24)\n",
            "x_dummy_2 (64, 28, 28, 24)\n",
            "x_dummy_3 (64, 23520)\n",
            "z_dummy (64, 23520)\n",
            "z_dummy_2 (64, 23520)\n",
            "o_dummy (64, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min_gz_norm 901.9565 nstep 7\n",
            "min_gz_norm 0.00508863 nstep 7\n",
            "batch_loss :: 2.3215670585632324 // batch_acc :: 0.09375\n",
            "min_gz_norm 903.1876 nstep 7\n",
            "min_gz_norm 0.0050234566 nstep 7\n",
            "batch_loss :: 2.29575252532959 // batch_acc :: 0.0\n",
            "min_gz_norm 902.65436 nstep 7\n",
            "min_gz_norm 0.0048512626 nstep 7\n",
            "batch_loss :: 2.370596408843994 // batch_acc :: 0.0625\n",
            "min_gz_norm 922.34485 nstep 7\n",
            "min_gz_norm 0.004836891 nstep 7\n",
            "batch_loss :: 2.391103506088257 // batch_acc :: 0.109375\n",
            "min_gz_norm 915.18146 nstep 7\n",
            "min_gz_norm 0.0047249347 nstep 7\n",
            "batch_loss :: 2.3160321712493896 // batch_acc :: 0.15625\n",
            "min_gz_norm 907.3576 nstep 7\n",
            "min_gz_norm 0.0046726917 nstep 7\n",
            "batch_loss :: 2.321805000305176 // batch_acc :: 0.046875\n",
            "min_gz_norm 905.88983 nstep 7\n",
            "min_gz_norm 0.0046172813 nstep 7\n",
            "batch_loss :: 2.2845427989959717 // batch_acc :: 0.140625\n",
            "min_gz_norm 914.445 nstep 7\n",
            "min_gz_norm 0.0045628203 nstep 7\n",
            "batch_loss :: 2.327014684677124 // batch_acc :: 0.03125\n",
            "min_gz_norm 919.1274 nstep 7\n",
            "min_gz_norm 0.0045262747 nstep 7\n",
            "batch_loss :: 2.269575834274292 // batch_acc :: 0.171875\n",
            "min_gz_norm 909.19934 nstep 7\n",
            "min_gz_norm 0.004581565 nstep 7\n",
            "batch_loss :: 2.3284242153167725 // batch_acc :: 0.09375\n",
            "min_gz_norm 911.01965 nstep 7\n",
            "min_gz_norm 0.0044098976 nstep 7\n",
            "batch_loss :: 2.2905592918395996 // batch_acc :: 0.1875\n",
            "min_gz_norm 902.5504 nstep 7\n",
            "min_gz_norm 0.0044478336 nstep 7\n",
            "batch_loss :: 2.302428960800171 // batch_acc :: 0.09375\n",
            "min_gz_norm 908.3709 nstep 7\n",
            "min_gz_norm 0.0045376746 nstep 7\n",
            "batch_loss :: 2.327618360519409 // batch_acc :: 0.0625\n",
            "min_gz_norm 903.9469 nstep 7\n",
            "min_gz_norm 0.0044996147 nstep 7\n",
            "batch_loss :: 2.331935405731201 // batch_acc :: 0.046875\n",
            "min_gz_norm 901.0459 nstep 7\n",
            "min_gz_norm 0.004492803 nstep 7\n",
            "batch_loss :: 2.321732997894287 // batch_acc :: 0.046875\n",
            "min_gz_norm 912.9194 nstep 7\n",
            "min_gz_norm 0.0044106264 nstep 7\n",
            "batch_loss :: 2.2955870628356934 // batch_acc :: 0.0625\n",
            "min_gz_norm 901.99457 nstep 7\n",
            "min_gz_norm 0.004396126 nstep 7\n",
            "batch_loss :: 2.268981456756592 // batch_acc :: 0.171875\n",
            "min_gz_norm 913.62036 nstep 7\n",
            "min_gz_norm 0.004510964 nstep 7\n",
            "batch_loss :: 2.2590084075927734 // batch_acc :: 0.25\n",
            "min_gz_norm 912.30774 nstep 7\n",
            "min_gz_norm 0.0045064534 nstep 7\n",
            "batch_loss :: 2.2655904293060303 // batch_acc :: 0.25\n",
            "min_gz_norm 904.9605 nstep 7\n",
            "min_gz_norm 0.0045742895 nstep 7\n",
            "batch_loss :: 2.3134286403656006 // batch_acc :: 0.234375\n",
            "min_gz_norm 909.8211 nstep 7\n",
            "min_gz_norm 0.004510819 nstep 7\n",
            "batch_loss :: 2.2521262168884277 // batch_acc :: 0.28125\n",
            "min_gz_norm 909.04895 nstep 7\n",
            "min_gz_norm 0.0045625283 nstep 7\n",
            "batch_loss :: 2.278416395187378 // batch_acc :: 0.15625\n",
            "min_gz_norm 904.73694 nstep 7\n",
            "min_gz_norm 0.0045922715 nstep 7\n",
            "batch_loss :: 2.381507396697998 // batch_acc :: 0.078125\n",
            "min_gz_norm 904.251 nstep 7\n",
            "min_gz_norm 0.0045809424 nstep 7\n",
            "batch_loss :: 2.36071515083313 // batch_acc :: 0.078125\n",
            "min_gz_norm 906.7048 nstep 7\n",
            "min_gz_norm 0.004592992 nstep 7\n",
            "batch_loss :: 2.2729172706604004 // batch_acc :: 0.15625\n",
            "min_gz_norm 919.97815 nstep 7\n",
            "min_gz_norm 0.004748548 nstep 7\n",
            "batch_loss :: 2.266237735748291 // batch_acc :: 0.21875\n",
            "min_gz_norm 897.36475 nstep 7\n",
            "min_gz_norm 0.0045727207 nstep 7\n",
            "batch_loss :: 2.2935991287231445 // batch_acc :: 0.21875\n",
            "min_gz_norm 895.8393 nstep 7\n",
            "min_gz_norm 0.0045665703 nstep 7\n",
            "batch_loss :: 2.272167205810547 // batch_acc :: 0.234375\n",
            "min_gz_norm 913.6991 nstep 7\n",
            "min_gz_norm 0.004766511 nstep 7\n",
            "batch_loss :: 2.2903246879577637 // batch_acc :: 0.171875\n",
            "min_gz_norm 906.73285 nstep 7\n",
            "min_gz_norm 0.0047628134 nstep 7\n",
            "batch_loss :: 2.250058889389038 // batch_acc :: 0.15625\n",
            "min_gz_norm 907.8445 nstep 7\n",
            "min_gz_norm 0.0048131207 nstep 7\n",
            "batch_loss :: 2.296276569366455 // batch_acc :: 0.03125\n",
            "min_gz_norm 914.7383 nstep 7\n",
            "min_gz_norm 0.0048306966 nstep 7\n",
            "batch_loss :: 2.2658803462982178 // batch_acc :: 0.046875\n",
            "min_gz_norm 902.34515 nstep 7\n",
            "min_gz_norm 0.004652653 nstep 7\n",
            "batch_loss :: 2.2660837173461914 // batch_acc :: 0.28125\n",
            "min_gz_norm 907.9174 nstep 7\n",
            "min_gz_norm 0.0049436353 nstep 7\n",
            "batch_loss :: 2.2596797943115234 // batch_acc :: 0.203125\n",
            "min_gz_norm 908.86945 nstep 7\n",
            "min_gz_norm 0.005065467 nstep 7\n",
            "batch_loss :: 2.2564315795898438 // batch_acc :: 0.109375\n",
            "min_gz_norm 906.90045 nstep 7\n",
            "min_gz_norm 0.0049770335 nstep 7\n",
            "batch_loss :: 2.290065288543701 // batch_acc :: 0.0625\n",
            "min_gz_norm 913.79065 nstep 7\n",
            "min_gz_norm 0.0050760396 nstep 7\n",
            "batch_loss :: 2.3053512573242188 // batch_acc :: 0.078125\n",
            "min_gz_norm 891.29193 nstep 7\n",
            "min_gz_norm 0.00496027 nstep 7\n",
            "batch_loss :: 2.3016066551208496 // batch_acc :: 0.140625\n",
            "min_gz_norm 906.98975 nstep 7\n",
            "min_gz_norm 0.0052376157 nstep 7\n",
            "batch_loss :: 2.2174532413482666 // batch_acc :: 0.203125\n",
            "min_gz_norm 898.93805 nstep 7\n",
            "min_gz_norm 0.0050374605 nstep 7\n",
            "batch_loss :: 2.276348352432251 // batch_acc :: 0.140625\n",
            "min_gz_norm 909.91296 nstep 7\n",
            "min_gz_norm 0.005060878 nstep 7\n",
            "batch_loss :: 2.3140037059783936 // batch_acc :: 0.140625\n",
            "min_gz_norm 896.9105 nstep 7\n",
            "min_gz_norm 0.0051523303 nstep 7\n",
            "batch_loss :: 2.2579684257507324 // batch_acc :: 0.234375\n",
            "min_gz_norm 916.8961 nstep 7\n",
            "min_gz_norm 0.00507679 nstep 7\n",
            "batch_loss :: 2.2162182331085205 // batch_acc :: 0.390625\n",
            "min_gz_norm 917.06573 nstep 7\n",
            "min_gz_norm 0.00523115 nstep 7\n",
            "batch_loss :: 2.285806655883789 // batch_acc :: 0.21875\n",
            "min_gz_norm 905.3725 nstep 7\n",
            "min_gz_norm 0.0053397026 nstep 7\n",
            "batch_loss :: 2.2351465225219727 // batch_acc :: 0.3125\n",
            "min_gz_norm 909.54364 nstep 7\n",
            "min_gz_norm 0.0052464935 nstep 7\n",
            "batch_loss :: 2.261259078979492 // batch_acc :: 0.1875\n",
            "min_gz_norm 897.80865 nstep 7\n",
            "min_gz_norm 0.005164936 nstep 7\n",
            "batch_loss :: 2.266885280609131 // batch_acc :: 0.15625\n",
            "min_gz_norm 909.30927 nstep 7\n",
            "min_gz_norm 0.0053985477 nstep 7\n",
            "batch_loss :: 2.2361810207366943 // batch_acc :: 0.28125\n",
            "min_gz_norm 905.55554 nstep 7\n",
            "min_gz_norm 0.0054527344 nstep 7\n",
            "batch_loss :: 2.2728822231292725 // batch_acc :: 0.1875\n",
            "min_gz_norm 909.7855 nstep 7\n",
            "min_gz_norm 0.005810263 nstep 7\n",
            "batch_loss :: 2.323899745941162 // batch_acc :: 0.140625\n",
            "min_gz_norm 904.45215 nstep 7\n",
            "min_gz_norm 0.0058157775 nstep 7\n",
            "batch_loss :: 2.234187602996826 // batch_acc :: 0.21875\n",
            "min_gz_norm 902.5024 nstep 7\n",
            "min_gz_norm 0.0059555406 nstep 7\n",
            "batch_loss :: 2.251328468322754 // batch_acc :: 0.203125\n",
            "min_gz_norm 904.87885 nstep 7\n",
            "min_gz_norm 0.0059091523 nstep 7\n",
            "batch_loss :: 2.2802233695983887 // batch_acc :: 0.203125\n",
            "min_gz_norm 912.8159 nstep 7\n",
            "min_gz_norm 0.006019299 nstep 7\n",
            "batch_loss :: 2.2447023391723633 // batch_acc :: 0.265625\n",
            "min_gz_norm 912.8186 nstep 7\n",
            "min_gz_norm 0.005849162 nstep 7\n",
            "batch_loss :: 2.2386114597320557 // batch_acc :: 0.21875\n",
            "min_gz_norm 894.1568 nstep 7\n",
            "min_gz_norm 0.0061221714 nstep 7\n",
            "batch_loss :: 2.238818407058716 // batch_acc :: 0.203125\n",
            "min_gz_norm 902.25714 nstep 7\n",
            "min_gz_norm 0.0065013235 nstep 7\n",
            "batch_loss :: 2.1705121994018555 // batch_acc :: 0.34375\n",
            "min_gz_norm 900.3613 nstep 7\n",
            "min_gz_norm 0.0062782406 nstep 7\n",
            "batch_loss :: 2.2615292072296143 // batch_acc :: 0.15625\n",
            "min_gz_norm 912.7797 nstep 7\n",
            "min_gz_norm 0.006544362 nstep 7\n",
            "batch_loss :: 2.225184202194214 // batch_acc :: 0.3125\n",
            "min_gz_norm 916.04895 nstep 7\n",
            "min_gz_norm 0.006366956 nstep 7\n",
            "batch_loss :: 2.2541329860687256 // batch_acc :: 0.15625\n",
            "min_gz_norm 899.2228 nstep 7\n",
            "min_gz_norm 0.006664325 nstep 7\n",
            "batch_loss :: 2.220673084259033 // batch_acc :: 0.25\n",
            "min_gz_norm 909.9874 nstep 7\n",
            "min_gz_norm 0.0066297106 nstep 7\n",
            "batch_loss :: 2.2443437576293945 // batch_acc :: 0.171875\n",
            "min_gz_norm 926.20123 nstep 7\n",
            "min_gz_norm 0.00736094 nstep 7\n",
            "batch_loss :: 2.1989710330963135 // batch_acc :: 0.28125\n",
            "min_gz_norm 911.13336 nstep 7\n",
            "min_gz_norm 0.006830346 nstep 7\n",
            "batch_loss :: 2.22859263420105 // batch_acc :: 0.171875\n",
            "min_gz_norm 914.8196 nstep 7\n",
            "min_gz_norm 0.007844335 nstep 7\n",
            "batch_loss :: 2.1960244178771973 // batch_acc :: 0.265625\n",
            "min_gz_norm 905.52167 nstep 7\n",
            "min_gz_norm 0.0076114936 nstep 7\n",
            "batch_loss :: 2.2063450813293457 // batch_acc :: 0.21875\n",
            "min_gz_norm 910.2234 nstep 7\n",
            "min_gz_norm 0.0069702533 nstep 7\n",
            "batch_loss :: 2.2082598209381104 // batch_acc :: 0.3125\n",
            "min_gz_norm 913.74365 nstep 7\n",
            "min_gz_norm 0.007268523 nstep 7\n",
            "batch_loss :: 2.115119695663452 // batch_acc :: 0.3125\n",
            "min_gz_norm 916.3788 nstep 7\n",
            "min_gz_norm 0.007545404 nstep 7\n",
            "batch_loss :: 2.1707358360290527 // batch_acc :: 0.265625\n",
            "min_gz_norm 901.9735 nstep 7\n",
            "min_gz_norm 0.007058405 nstep 7\n",
            "batch_loss :: 2.276273488998413 // batch_acc :: 0.15625\n",
            "min_gz_norm 903.7471 nstep 7\n",
            "min_gz_norm 0.0074714855 nstep 7\n",
            "batch_loss :: 2.168105125427246 // batch_acc :: 0.203125\n",
            "min_gz_norm 910.3285 nstep 7\n",
            "min_gz_norm 0.007579015 nstep 7\n",
            "batch_loss :: 2.2104978561401367 // batch_acc :: 0.203125\n",
            "min_gz_norm 905.7375 nstep 7\n",
            "min_gz_norm 0.007934976 nstep 7\n",
            "batch_loss :: 2.1359784603118896 // batch_acc :: 0.3125\n",
            "min_gz_norm 914.3784 nstep 7\n",
            "min_gz_norm 0.00962042 nstep 7\n",
            "batch_loss :: 2.1092171669006348 // batch_acc :: 0.34375\n",
            "min_gz_norm 905.84485 nstep 7\n",
            "min_gz_norm 0.008516088 nstep 7\n",
            "batch_loss :: 2.184810161590576 // batch_acc :: 0.28125\n",
            "min_gz_norm 915.4092 nstep 7\n",
            "min_gz_norm 0.009832883 nstep 7\n",
            "batch_loss :: 2.147047281265259 // batch_acc :: 0.25\n",
            "min_gz_norm 910.4034 nstep 7\n",
            "min_gz_norm 0.011050241 nstep 7\n",
            "batch_loss :: 2.126603841781616 // batch_acc :: 0.234375\n",
            "min_gz_norm 913.119 nstep 7\n",
            "min_gz_norm 0.009813519 nstep 7\n",
            "batch_loss :: 2.228670597076416 // batch_acc :: 0.140625\n",
            "min_gz_norm 912.02563 nstep 7\n",
            "min_gz_norm 0.011046515 nstep 7\n",
            "batch_loss :: 2.170196533203125 // batch_acc :: 0.234375\n",
            "min_gz_norm 901.63727 nstep 7\n",
            "min_gz_norm 0.012124899 nstep 7\n",
            "batch_loss :: 2.142533302307129 // batch_acc :: 0.234375\n",
            "min_gz_norm 896.23114 nstep 7\n",
            "min_gz_norm 0.009830662 nstep 7\n",
            "batch_loss :: 2.148982286453247 // batch_acc :: 0.3125\n",
            "min_gz_norm 910.8257 nstep 7\n",
            "min_gz_norm 0.013094158 nstep 7\n",
            "batch_loss :: 1.9989700317382812 // batch_acc :: 0.4375\n",
            "min_gz_norm 912.2119 nstep 7\n",
            "min_gz_norm 0.011216434 nstep 7\n",
            "batch_loss :: 2.0967745780944824 // batch_acc :: 0.328125\n",
            "min_gz_norm 911.96246 nstep 7\n",
            "min_gz_norm 0.0106068915 nstep 7\n",
            "batch_loss :: 2.1542396545410156 // batch_acc :: 0.1875\n",
            "min_gz_norm 912.7631 nstep 7\n",
            "min_gz_norm 0.00992612 nstep 7\n",
            "batch_loss :: 2.1693482398986816 // batch_acc :: 0.1875\n",
            "min_gz_norm 912.578 nstep 7\n",
            "min_gz_norm 0.013721108 nstep 7\n",
            "batch_loss :: 2.106550931930542 // batch_acc :: 0.296875\n",
            "min_gz_norm 914.43787 nstep 7\n",
            "min_gz_norm 0.014148399 nstep 7\n",
            "batch_loss :: 2.0833044052124023 // batch_acc :: 0.4375\n",
            "min_gz_norm 911.1084 nstep 7\n",
            "min_gz_norm 0.014567417 nstep 7\n",
            "batch_loss :: 2.1084463596343994 // batch_acc :: 0.28125\n",
            "min_gz_norm 909.90546 nstep 7\n",
            "min_gz_norm 0.015447164 nstep 7\n",
            "batch_loss :: 2.1103897094726562 // batch_acc :: 0.28125\n",
            "min_gz_norm 902.607 nstep 7\n",
            "min_gz_norm 0.0152497655 nstep 7\n",
            "batch_loss :: 2.1277403831481934 // batch_acc :: 0.28125\n",
            "min_gz_norm 910.44305 nstep 7\n",
            "min_gz_norm 0.015274033 nstep 7\n",
            "batch_loss :: 1.913832426071167 // batch_acc :: 0.375\n",
            "min_gz_norm 903.851 nstep 7\n",
            "min_gz_norm 0.013527602 nstep 7\n",
            "batch_loss :: 2.0332131385803223 // batch_acc :: 0.375\n",
            "min_gz_norm 908.827 nstep 7\n",
            "min_gz_norm 0.0135437315 nstep 7\n",
            "batch_loss :: 2.034257173538208 // batch_acc :: 0.265625\n",
            "min_gz_norm 912.3195 nstep 7\n",
            "min_gz_norm 0.014673086 nstep 7\n",
            "batch_loss :: 2.0514235496520996 // batch_acc :: 0.234375\n",
            "min_gz_norm 914.9696 nstep 7\n",
            "min_gz_norm 0.015216642 nstep 7\n",
            "batch_loss :: 2.015578269958496 // batch_acc :: 0.34375\n",
            "min_gz_norm 902.7606 nstep 7\n",
            "min_gz_norm 0.025681715 nstep 7\n",
            "batch_loss :: 2.1039822101593018 // batch_acc :: 0.28125\n",
            "min_gz_norm 899.3761 nstep 7\n",
            "min_gz_norm 0.017119907 nstep 7\n",
            "batch_loss :: 2.0408883094787598 // batch_acc :: 0.3125\n",
            "min_gz_norm 906.00867 nstep 7\n",
            "min_gz_norm 0.012430389 nstep 7\n",
            "batch_loss :: 2.1346423625946045 // batch_acc :: 0.296875\n",
            "min_gz_norm 917.12805 nstep 7\n",
            "min_gz_norm 0.016373927 nstep 7\n",
            "batch_loss :: 1.9833325147628784 // batch_acc :: 0.328125\n",
            "min_gz_norm 903.4957 nstep 7\n",
            "min_gz_norm 0.017653465 nstep 7\n",
            "batch_loss :: 2.016514301300049 // batch_acc :: 0.34375\n",
            "min_gz_norm 904.36743 nstep 7\n",
            "min_gz_norm 0.017946841 nstep 7\n",
            "batch_loss :: 2.022658109664917 // batch_acc :: 0.265625\n",
            "min_gz_norm 905.201 nstep 7\n",
            "min_gz_norm 0.020639697 nstep 7\n",
            "batch_loss :: 2.014991283416748 // batch_acc :: 0.359375\n",
            "min_gz_norm 892.6604 nstep 7\n",
            "min_gz_norm 0.01770497 nstep 7\n",
            "batch_loss :: 1.8499003648757935 // batch_acc :: 0.34375\n",
            "min_gz_norm 905.50397 nstep 7\n",
            "min_gz_norm 0.022432543 nstep 7\n",
            "batch_loss :: 2.018970012664795 // batch_acc :: 0.25\n",
            "min_gz_norm 900.5411 nstep 7\n",
            "min_gz_norm 0.019289965 nstep 7\n",
            "batch_loss :: 2.0670437812805176 // batch_acc :: 0.171875\n",
            "min_gz_norm 907.4454 nstep 7\n",
            "min_gz_norm 0.01817719 nstep 7\n",
            "batch_loss :: 1.9083540439605713 // batch_acc :: 0.3125\n",
            "min_gz_norm 920.3489 nstep 7\n",
            "min_gz_norm 0.020473087 nstep 7\n",
            "batch_loss :: 1.9877134561538696 // batch_acc :: 0.234375\n",
            "min_gz_norm 912.37915 nstep 7\n",
            "min_gz_norm 0.019499741 nstep 7\n",
            "batch_loss :: 1.8822914361953735 // batch_acc :: 0.359375\n",
            "min_gz_norm 906.87646 nstep 7\n",
            "min_gz_norm 0.017723521 nstep 7\n",
            "batch_loss :: 1.9193387031555176 // batch_acc :: 0.265625\n",
            "min_gz_norm 906.0307 nstep 7\n",
            "min_gz_norm 0.023278942 nstep 7\n",
            "batch_loss :: 1.949609398841858 // batch_acc :: 0.21875\n",
            "min_gz_norm 914.83777 nstep 7\n",
            "min_gz_norm 0.019434987 nstep 7\n",
            "batch_loss :: 1.9398727416992188 // batch_acc :: 0.265625\n",
            "min_gz_norm 906.5976 nstep 7\n",
            "min_gz_norm 0.025531862 nstep 7\n",
            "batch_loss :: 2.004182815551758 // batch_acc :: 0.234375\n",
            "min_gz_norm 902.5564 nstep 7\n",
            "min_gz_norm 0.01952114 nstep 7\n",
            "batch_loss :: 1.9341789484024048 // batch_acc :: 0.3125\n",
            "min_gz_norm 917.782 nstep 7\n",
            "min_gz_norm 0.022611711 nstep 7\n",
            "batch_loss :: 1.8752199411392212 // batch_acc :: 0.359375\n",
            "min_gz_norm 905.92725 nstep 7\n",
            "min_gz_norm 0.017156638 nstep 7\n",
            "batch_loss :: 1.8645031452178955 // batch_acc :: 0.4375\n",
            "min_gz_norm 914.16095 nstep 7\n",
            "min_gz_norm 0.02574777 nstep 7\n",
            "batch_loss :: 1.847846269607544 // batch_acc :: 0.234375\n",
            "min_gz_norm 908.2095 nstep 7\n",
            "min_gz_norm 0.024189135 nstep 7\n",
            "batch_loss :: 1.9758846759796143 // batch_acc :: 0.21875\n",
            "min_gz_norm 910.44055 nstep 7\n",
            "min_gz_norm 0.021936934 nstep 7\n",
            "batch_loss :: 1.9723284244537354 // batch_acc :: 0.21875\n",
            "min_gz_norm 908.26843 nstep 7\n",
            "min_gz_norm 0.019241175 nstep 7\n",
            "batch_loss :: 1.8823707103729248 // batch_acc :: 0.359375\n",
            "min_gz_norm 913.9798 nstep 7\n",
            "min_gz_norm 0.020079508 nstep 7\n",
            "batch_loss :: 2.0561013221740723 // batch_acc :: 0.25\n",
            "min_gz_norm 900.1119 nstep 7\n",
            "min_gz_norm 0.017953344 nstep 7\n",
            "batch_loss :: 1.9844844341278076 // batch_acc :: 0.328125\n",
            "min_gz_norm 905.0464 nstep 7\n",
            "min_gz_norm 0.021958247 nstep 7\n",
            "batch_loss :: 1.8340270519256592 // batch_acc :: 0.453125\n",
            "min_gz_norm 911.7007 nstep 7\n",
            "min_gz_norm 0.021976333 nstep 7\n",
            "batch_loss :: 1.8867435455322266 // batch_acc :: 0.3125\n",
            "min_gz_norm 918.5275 nstep 7\n",
            "min_gz_norm 0.027834171 nstep 7\n",
            "batch_loss :: 1.8667898178100586 // batch_acc :: 0.3125\n",
            "min_gz_norm 909.5879 nstep 7\n",
            "min_gz_norm 0.02488387 nstep 7\n",
            "batch_loss :: 1.9509984254837036 // batch_acc :: 0.28125\n",
            "min_gz_norm 901.92145 nstep 7\n",
            "min_gz_norm 0.019896528 nstep 7\n",
            "batch_loss :: 1.9930822849273682 // batch_acc :: 0.1875\n",
            "min_gz_norm 909.20624 nstep 7\n",
            "min_gz_norm 0.03025313 nstep 7\n",
            "batch_loss :: 1.9487075805664062 // batch_acc :: 0.203125\n",
            "min_gz_norm 918.743 nstep 7\n",
            "min_gz_norm 0.03068387 nstep 7\n",
            "batch_loss :: 1.7665774822235107 // batch_acc :: 0.390625\n",
            "min_gz_norm 906.95667 nstep 7\n",
            "min_gz_norm 0.023413239 nstep 7\n",
            "batch_loss :: 1.9176788330078125 // batch_acc :: 0.34375\n",
            "min_gz_norm 916.8763 nstep 7\n",
            "min_gz_norm 0.028792005 nstep 7\n",
            "batch_loss :: 1.8964309692382812 // batch_acc :: 0.375\n",
            "min_gz_norm 910.5953 nstep 7\n",
            "min_gz_norm 0.023870816 nstep 7\n",
            "batch_loss :: 1.756117343902588 // batch_acc :: 0.375\n",
            "min_gz_norm 901.8539 nstep 7\n",
            "min_gz_norm 0.027410787 nstep 7\n",
            "batch_loss :: 1.8803521394729614 // batch_acc :: 0.390625\n",
            "min_gz_norm 909.42114 nstep 7\n",
            "min_gz_norm 0.022754945 nstep 7\n",
            "batch_loss :: 1.8194944858551025 // batch_acc :: 0.3125\n",
            "min_gz_norm 912.92365 nstep 7\n",
            "min_gz_norm 0.023559412 nstep 7\n",
            "batch_loss :: 1.8729686737060547 // batch_acc :: 0.3125\n",
            "min_gz_norm 904.6449 nstep 7\n",
            "min_gz_norm 0.025461162 nstep 7\n",
            "batch_loss :: 1.8596470355987549 // batch_acc :: 0.328125\n",
            "min_gz_norm 913.9033 nstep 7\n",
            "min_gz_norm 0.027593134 nstep 7\n",
            "batch_loss :: 1.9773004055023193 // batch_acc :: 0.265625\n",
            "min_gz_norm 898.5829 nstep 7\n",
            "min_gz_norm 0.022619082 nstep 7\n",
            "batch_loss :: 1.7374553680419922 // batch_acc :: 0.40625\n",
            "min_gz_norm 901.4537 nstep 7\n",
            "min_gz_norm 0.021133613 nstep 7\n",
            "batch_loss :: 1.7220077514648438 // batch_acc :: 0.40625\n",
            "min_gz_norm 904.9673 nstep 7\n",
            "min_gz_norm 0.024300901 nstep 7\n",
            "batch_loss :: 1.8409736156463623 // batch_acc :: 0.265625\n",
            "min_gz_norm 920.884 nstep 7\n",
            "min_gz_norm 0.02623784 nstep 7\n",
            "batch_loss :: 1.6790896654129028 // batch_acc :: 0.4375\n",
            "min_gz_norm 910.45795 nstep 7\n",
            "min_gz_norm 0.031906806 nstep 7\n",
            "batch_loss :: 1.825398564338684 // batch_acc :: 0.390625\n",
            "min_gz_norm 914.6791 nstep 7\n",
            "min_gz_norm 0.021934902 nstep 7\n",
            "batch_loss :: 1.7622462511062622 // batch_acc :: 0.34375\n",
            "min_gz_norm 917.1927 nstep 7\n",
            "min_gz_norm 0.030997269 nstep 7\n",
            "batch_loss :: 1.7206735610961914 // batch_acc :: 0.453125\n",
            "min_gz_norm 909.7777 nstep 7\n",
            "min_gz_norm 0.03233552 nstep 7\n",
            "batch_loss :: 1.9321789741516113 // batch_acc :: 0.421875\n",
            "min_gz_norm 898.8918 nstep 7\n",
            "min_gz_norm 0.034769278 nstep 7\n",
            "batch_loss :: 1.7518646717071533 // batch_acc :: 0.390625\n",
            "min_gz_norm 906.491 nstep 7\n",
            "min_gz_norm 0.026078705 nstep 7\n",
            "batch_loss :: 1.689188838005066 // batch_acc :: 0.3125\n",
            "min_gz_norm 914.1754 nstep 7\n",
            "min_gz_norm 0.027318187 nstep 7\n",
            "batch_loss :: 1.5679171085357666 // batch_acc :: 0.40625\n",
            "min_gz_norm 905.517 nstep 7\n",
            "min_gz_norm 0.02405432 nstep 7\n",
            "batch_loss :: 1.8115065097808838 // batch_acc :: 0.3125\n",
            "min_gz_norm 903.8891 nstep 7\n",
            "min_gz_norm 0.02995731 nstep 7\n",
            "batch_loss :: 1.8375482559204102 // batch_acc :: 0.3125\n",
            "min_gz_norm 896.6375 nstep 7\n",
            "min_gz_norm 0.02804422 nstep 7\n",
            "batch_loss :: 1.79282546043396 // batch_acc :: 0.328125\n",
            "min_gz_norm 912.33514 nstep 7\n",
            "min_gz_norm 0.031276215 nstep 7\n",
            "batch_loss :: 1.7450475692749023 // batch_acc :: 0.421875\n",
            "min_gz_norm 915.93176 nstep 7\n",
            "min_gz_norm 0.036584694 nstep 7\n",
            "batch_loss :: 1.9029560089111328 // batch_acc :: 0.40625\n",
            "min_gz_norm 912.3698 nstep 7\n",
            "min_gz_norm 0.024433738 nstep 7\n",
            "batch_loss :: 1.6512023210525513 // batch_acc :: 0.46875\n",
            "min_gz_norm 888.56604 nstep 7\n",
            "min_gz_norm 0.03720386 nstep 7\n",
            "batch_loss :: 1.7590949535369873 // batch_acc :: 0.390625\n",
            "min_gz_norm 909.691 nstep 7\n",
            "min_gz_norm 0.02674456 nstep 7\n",
            "batch_loss :: 1.8305491209030151 // batch_acc :: 0.328125\n",
            "min_gz_norm 908.9288 nstep 7\n",
            "min_gz_norm 0.0316487 nstep 7\n",
            "batch_loss :: 1.7875690460205078 // batch_acc :: 0.25\n",
            "min_gz_norm 901.1175 nstep 7\n",
            "min_gz_norm 0.040062565 nstep 7\n",
            "batch_loss :: 1.682373285293579 // batch_acc :: 0.53125\n",
            "min_gz_norm 900.89923 nstep 7\n",
            "min_gz_norm 0.033562336 nstep 7\n",
            "batch_loss :: 1.6778161525726318 // batch_acc :: 0.40625\n",
            "min_gz_norm 912.1473 nstep 7\n",
            "min_gz_norm 0.04011105 nstep 7\n",
            "batch_loss :: 1.567046046257019 // batch_acc :: 0.46875\n",
            "min_gz_norm 916.4784 nstep 7\n",
            "min_gz_norm 0.03310941 nstep 7\n",
            "batch_loss :: 1.6103732585906982 // batch_acc :: 0.484375\n",
            "min_gz_norm 911.9723 nstep 7\n",
            "min_gz_norm 0.04077047 nstep 7\n",
            "batch_loss :: 1.6748172044754028 // batch_acc :: 0.421875\n",
            "min_gz_norm 913.90436 nstep 7\n",
            "min_gz_norm 0.025792537 nstep 7\n",
            "batch_loss :: 1.4818437099456787 // batch_acc :: 0.421875\n",
            "min_gz_norm 905.3508 nstep 7\n",
            "min_gz_norm 0.034353413 nstep 7\n",
            "batch_loss :: 1.8196231126785278 // batch_acc :: 0.4375\n",
            "min_gz_norm 912.71796 nstep 7\n",
            "min_gz_norm 0.029684847 nstep 7\n",
            "batch_loss :: 1.8760731220245361 // batch_acc :: 0.390625\n",
            "min_gz_norm 912.5235 nstep 7\n",
            "min_gz_norm 0.026745558 nstep 7\n",
            "batch_loss :: 1.5626144409179688 // batch_acc :: 0.578125\n",
            "min_gz_norm 914.7209 nstep 7\n",
            "min_gz_norm 0.045333873 nstep 7\n",
            "batch_loss :: 1.6600584983825684 // batch_acc :: 0.453125\n",
            "min_gz_norm 901.7452 nstep 7\n",
            "min_gz_norm 0.037359405 nstep 7\n",
            "batch_loss :: 1.777857780456543 // batch_acc :: 0.421875\n",
            "min_gz_norm 896.3222 nstep 7\n",
            "min_gz_norm 0.0393882 nstep 7\n",
            "batch_loss :: 1.708139181137085 // batch_acc :: 0.421875\n",
            "min_gz_norm 906.6215 nstep 7\n",
            "min_gz_norm 0.032347612 nstep 7\n",
            "batch_loss :: 1.751187801361084 // batch_acc :: 0.375\n",
            "min_gz_norm 912.70306 nstep 7\n",
            "min_gz_norm 0.049776193 nstep 7\n",
            "batch_loss :: 1.8547461032867432 // batch_acc :: 0.28125\n",
            "min_gz_norm 904.7681 nstep 7\n",
            "min_gz_norm 0.035497483 nstep 7\n",
            "batch_loss :: 1.6622164249420166 // batch_acc :: 0.484375\n",
            "min_gz_norm 913.8902 nstep 7\n",
            "min_gz_norm 0.057665538 nstep 7\n",
            "batch_loss :: 1.5992422103881836 // batch_acc :: 0.484375\n",
            "min_gz_norm 910.8137 nstep 7\n",
            "min_gz_norm 0.03801799 nstep 7\n",
            "batch_loss :: 1.7404093742370605 // batch_acc :: 0.375\n",
            "min_gz_norm 910.0102 nstep 7\n",
            "min_gz_norm 0.039068416 nstep 7\n",
            "batch_loss :: 1.6895982027053833 // batch_acc :: 0.390625\n",
            "min_gz_norm 900.7105 nstep 7\n",
            "min_gz_norm 0.044536915 nstep 7\n",
            "batch_loss :: 1.7167913913726807 // batch_acc :: 0.375\n",
            "min_gz_norm 903.6218 nstep 7\n",
            "min_gz_norm 0.048658483 nstep 7\n",
            "batch_loss :: 1.7856526374816895 // batch_acc :: 0.359375\n",
            "min_gz_norm 912.1661 nstep 7\n",
            "min_gz_norm 0.040258277 nstep 7\n",
            "batch_loss :: 1.664876937866211 // batch_acc :: 0.4375\n",
            "min_gz_norm 912.7412 nstep 7\n",
            "min_gz_norm 0.034618113 nstep 7\n",
            "batch_loss :: 1.6605907678604126 // batch_acc :: 0.375\n",
            "min_gz_norm 904.7659 nstep 7\n",
            "min_gz_norm 0.05030682 nstep 7\n",
            "batch_loss :: 1.6765284538269043 // batch_acc :: 0.40625\n",
            "min_gz_norm 903.1433 nstep 7\n",
            "min_gz_norm 0.047535904 nstep 7\n",
            "batch_loss :: 1.6985615491867065 // batch_acc :: 0.359375\n",
            "min_gz_norm 907.47864 nstep 7\n",
            "min_gz_norm 0.03748965 nstep 7\n",
            "batch_loss :: 1.5765866041183472 // batch_acc :: 0.40625\n",
            "min_gz_norm 902.68054 nstep 7\n",
            "min_gz_norm 0.037109077 nstep 7\n",
            "batch_loss :: 1.7393460273742676 // batch_acc :: 0.46875\n",
            "min_gz_norm 910.79407 nstep 7\n",
            "min_gz_norm 0.042652756 nstep 7\n",
            "batch_loss :: 1.7457871437072754 // batch_acc :: 0.34375\n",
            "min_gz_norm 892.4506 nstep 7\n",
            "min_gz_norm 0.037242897 nstep 7\n",
            "batch_loss :: 1.6168935298919678 // batch_acc :: 0.390625\n",
            "min_gz_norm 903.69415 nstep 7\n",
            "min_gz_norm 0.048380353 nstep 7\n",
            "batch_loss :: 1.6205015182495117 // batch_acc :: 0.46875\n",
            "min_gz_norm 891.3861 nstep 7\n",
            "min_gz_norm 0.035331756 nstep 7\n",
            "batch_loss :: 1.6460998058319092 // batch_acc :: 0.46875\n",
            "min_gz_norm 916.96155 nstep 7\n",
            "min_gz_norm 0.04266087 nstep 7\n",
            "batch_loss :: 1.6644233465194702 // batch_acc :: 0.46875\n",
            "min_gz_norm 901.76544 nstep 7\n",
            "min_gz_norm 0.033446506 nstep 7\n",
            "batch_loss :: 1.49391508102417 // batch_acc :: 0.484375\n",
            "min_gz_norm 911.01324 nstep 7\n",
            "min_gz_norm 0.050716925 nstep 7\n",
            "batch_loss :: 1.7068548202514648 // batch_acc :: 0.34375\n",
            "min_gz_norm 892.3215 nstep 7\n",
            "min_gz_norm 0.038743373 nstep 7\n",
            "batch_loss :: 1.3630833625793457 // batch_acc :: 0.46875\n",
            "min_gz_norm 908.86005 nstep 7\n",
            "min_gz_norm 0.04745904 nstep 7\n",
            "batch_loss :: 1.7740442752838135 // batch_acc :: 0.3125\n",
            "min_gz_norm 915.4566 nstep 7\n",
            "min_gz_norm 0.04760219 nstep 7\n",
            "batch_loss :: 1.5666298866271973 // batch_acc :: 0.453125\n",
            "min_gz_norm 893.53925 nstep 7\n",
            "min_gz_norm 0.04268843 nstep 7\n",
            "batch_loss :: 1.7683943510055542 // batch_acc :: 0.375\n",
            "min_gz_norm 905.74225 nstep 7\n",
            "min_gz_norm 0.036646858 nstep 7\n",
            "batch_loss :: 1.5315203666687012 // batch_acc :: 0.484375\n",
            "min_gz_norm 909.0391 nstep 7\n",
            "min_gz_norm 0.040612407 nstep 7\n",
            "batch_loss :: 1.440592646598816 // batch_acc :: 0.640625\n",
            "min_gz_norm 897.0795 nstep 7\n",
            "min_gz_norm 0.043515597 nstep 7\n",
            "batch_loss :: 1.6913373470306396 // batch_acc :: 0.421875\n",
            "min_gz_norm 910.5983 nstep 7\n",
            "min_gz_norm 0.04514945 nstep 7\n",
            "batch_loss :: 1.727337121963501 // batch_acc :: 0.390625\n",
            "min_gz_norm 903.11304 nstep 7\n",
            "min_gz_norm 0.03795713 nstep 7\n",
            "batch_loss :: 1.5858564376831055 // batch_acc :: 0.5\n",
            "min_gz_norm 893.90405 nstep 7\n",
            "min_gz_norm 0.04240175 nstep 7\n",
            "batch_loss :: 1.632348656654358 // batch_acc :: 0.375\n",
            "min_gz_norm 894.8447 nstep 7\n",
            "min_gz_norm 0.04373089 nstep 7\n",
            "batch_loss :: 1.809814214706421 // batch_acc :: 0.328125\n",
            "min_gz_norm 912.4332 nstep 7\n",
            "min_gz_norm 0.047615685 nstep 7\n",
            "batch_loss :: 1.511809229850769 // batch_acc :: 0.515625\n",
            "min_gz_norm 907.6134 nstep 7\n",
            "min_gz_norm 0.037836395 nstep 7\n",
            "batch_loss :: 1.4676997661590576 // batch_acc :: 0.546875\n",
            "min_gz_norm 910.3504 nstep 7\n",
            "min_gz_norm 0.0491962 nstep 7\n",
            "batch_loss :: 1.4749730825424194 // batch_acc :: 0.59375\n",
            "min_gz_norm 910.8394 nstep 7\n",
            "min_gz_norm 0.03912469 nstep 7\n",
            "batch_loss :: 1.621069312095642 // batch_acc :: 0.5\n",
            "min_gz_norm 918.93567 nstep 7\n",
            "min_gz_norm 0.045663908 nstep 7\n",
            "batch_loss :: 1.5500797033309937 // batch_acc :: 0.5\n",
            "min_gz_norm 898.4051 nstep 7\n",
            "min_gz_norm 0.037311777 nstep 7\n",
            "batch_loss :: 1.3599188327789307 // batch_acc :: 0.609375\n",
            "min_gz_norm 908.3172 nstep 7\n",
            "min_gz_norm 0.041438732 nstep 7\n",
            "batch_loss :: 1.6207551956176758 // batch_acc :: 0.390625\n",
            "min_gz_norm 904.8612 nstep 7\n",
            "min_gz_norm 0.04482081 nstep 7\n",
            "batch_loss :: 1.5620200634002686 // batch_acc :: 0.5625\n",
            "min_gz_norm 901.17505 nstep 7\n",
            "min_gz_norm 0.04423339 nstep 7\n",
            "batch_loss :: 1.5597350597381592 // batch_acc :: 0.515625\n",
            "min_gz_norm 911.5767 nstep 7\n",
            "min_gz_norm 0.043782543 nstep 7\n",
            "batch_loss :: 1.6558175086975098 // batch_acc :: 0.421875\n",
            "min_gz_norm 915.3372 nstep 7\n",
            "min_gz_norm 0.03697608 nstep 7\n",
            "batch_loss :: 1.3585219383239746 // batch_acc :: 0.515625\n",
            "min_gz_norm 915.20905 nstep 7\n",
            "min_gz_norm 0.05210548 nstep 7\n",
            "batch_loss :: 1.622694969177246 // batch_acc :: 0.390625\n",
            "min_gz_norm 916.4407 nstep 7\n",
            "min_gz_norm 0.056827564 nstep 7\n",
            "batch_loss :: 1.5371921062469482 // batch_acc :: 0.5\n",
            "min_gz_norm 915.2159 nstep 7\n",
            "min_gz_norm 0.05089022 nstep 7\n",
            "batch_loss :: 1.5981345176696777 // batch_acc :: 0.4375\n",
            "min_gz_norm 913.5172 nstep 7\n",
            "min_gz_norm 0.046562288 nstep 7\n",
            "batch_loss :: 1.663313388824463 // batch_acc :: 0.34375\n",
            "min_gz_norm 907.61633 nstep 7\n",
            "min_gz_norm 0.05143996 nstep 7\n",
            "batch_loss :: 1.5419421195983887 // batch_acc :: 0.5\n",
            "min_gz_norm 903.8194 nstep 7\n",
            "min_gz_norm 0.04872257 nstep 7\n",
            "batch_loss :: 1.631562352180481 // batch_acc :: 0.34375\n",
            "min_gz_norm 915.296 nstep 7\n",
            "min_gz_norm 0.04022261 nstep 7\n",
            "batch_loss :: 1.6147136688232422 // batch_acc :: 0.359375\n",
            "min_gz_norm 909.44946 nstep 7\n",
            "min_gz_norm 0.045852546 nstep 7\n",
            "batch_loss :: 1.5521960258483887 // batch_acc :: 0.421875\n",
            "min_gz_norm 907.4943 nstep 7\n",
            "min_gz_norm 0.04668366 nstep 7\n",
            "batch_loss :: 1.463053584098816 // batch_acc :: 0.5625\n",
            "min_gz_norm 909.29816 nstep 7\n",
            "min_gz_norm 0.042310167 nstep 7\n",
            "batch_loss :: 1.5319972038269043 // batch_acc :: 0.453125\n",
            "min_gz_norm 904.8233 nstep 7\n",
            "min_gz_norm 0.048987206 nstep 7\n",
            "batch_loss :: 1.63901686668396 // batch_acc :: 0.421875\n",
            "min_gz_norm 907.5138 nstep 7\n",
            "min_gz_norm 0.04228935 nstep 7\n",
            "batch_loss :: 1.4613406658172607 // batch_acc :: 0.53125\n",
            "min_gz_norm 910.2843 nstep 7\n",
            "min_gz_norm 0.050140996 nstep 7\n",
            "batch_loss :: 1.5593693256378174 // batch_acc :: 0.53125\n",
            "min_gz_norm 901.2118 nstep 7\n",
            "min_gz_norm 0.053856283 nstep 7\n",
            "batch_loss :: 1.576762318611145 // batch_acc :: 0.453125\n",
            "min_gz_norm 919.39606 nstep 7\n",
            "min_gz_norm 0.053188942 nstep 7\n",
            "batch_loss :: 1.6490733623504639 // batch_acc :: 0.421875\n",
            "min_gz_norm 905.01636 nstep 7\n",
            "min_gz_norm 0.047383863 nstep 7\n",
            "batch_loss :: 1.5117818117141724 // batch_acc :: 0.5\n",
            "min_gz_norm 892.1047 nstep 7\n",
            "min_gz_norm 0.04651897 nstep 7\n",
            "batch_loss :: 1.5383193492889404 // batch_acc :: 0.40625\n",
            "min_gz_norm 904.7387 nstep 7\n",
            "min_gz_norm 0.05280771 nstep 7\n",
            "batch_loss :: 1.6593263149261475 // batch_acc :: 0.390625\n",
            "min_gz_norm 897.50464 nstep 7\n",
            "min_gz_norm 0.061435204 nstep 7\n",
            "batch_loss :: 1.476635456085205 // batch_acc :: 0.4375\n",
            "min_gz_norm 901.0725 nstep 7\n",
            "min_gz_norm 0.044045582 nstep 7\n",
            "batch_loss :: 1.4742380380630493 // batch_acc :: 0.484375\n",
            "min_gz_norm 915.04724 nstep 7\n",
            "min_gz_norm 0.056359384 nstep 7\n",
            "batch_loss :: 1.420820713043213 // batch_acc :: 0.484375\n",
            "min_gz_norm 901.89777 nstep 7\n",
            "min_gz_norm 0.047606245 nstep 7\n",
            "batch_loss :: 1.4844062328338623 // batch_acc :: 0.484375\n",
            "min_gz_norm 899.50085 nstep 7\n",
            "min_gz_norm 0.048310082 nstep 7\n",
            "batch_loss :: 1.5169728994369507 // batch_acc :: 0.375\n",
            "min_gz_norm 902.9004 nstep 7\n",
            "min_gz_norm 0.04379254 nstep 7\n",
            "batch_loss :: 1.569057822227478 // batch_acc :: 0.421875\n",
            "min_gz_norm 905.8961 nstep 7\n",
            "min_gz_norm 0.04905441 nstep 7\n",
            "batch_loss :: 1.6562565565109253 // batch_acc :: 0.375\n",
            "min_gz_norm 904.2699 nstep 7\n",
            "min_gz_norm 0.053248852 nstep 7\n",
            "batch_loss :: 1.7376294136047363 // batch_acc :: 0.4375\n",
            "min_gz_norm 907.2989 nstep 7\n",
            "min_gz_norm 0.05160422 nstep 7\n",
            "batch_loss :: 1.4006445407867432 // batch_acc :: 0.484375\n",
            "min_gz_norm 903.78033 nstep 7\n",
            "min_gz_norm 0.04839802 nstep 7\n",
            "batch_loss :: 1.628956913948059 // batch_acc :: 0.484375\n",
            "min_gz_norm 900.9478 nstep 7\n",
            "min_gz_norm 0.048654415 nstep 7\n",
            "batch_loss :: 1.4350974559783936 // batch_acc :: 0.578125\n",
            "min_gz_norm 902.38696 nstep 7\n",
            "min_gz_norm 0.049835104 nstep 7\n",
            "batch_loss :: 1.3876227140426636 // batch_acc :: 0.5\n",
            "min_gz_norm 906.85834 nstep 7\n",
            "min_gz_norm 0.062315147 nstep 7\n",
            "batch_loss :: 1.635683536529541 // batch_acc :: 0.40625\n",
            "min_gz_norm 919.5571 nstep 7\n",
            "min_gz_norm 0.05527304 nstep 7\n",
            "batch_loss :: 1.4793461561203003 // batch_acc :: 0.546875\n",
            "min_gz_norm 899.286 nstep 7\n",
            "min_gz_norm 0.05572111 nstep 7\n",
            "batch_loss :: 1.5844006538391113 // batch_acc :: 0.40625\n",
            "min_gz_norm 900.80145 nstep 7\n",
            "min_gz_norm 0.049910665 nstep 7\n",
            "batch_loss :: 1.3939604759216309 // batch_acc :: 0.609375\n",
            "min_gz_norm 916.0674 nstep 7\n",
            "min_gz_norm 0.043282952 nstep 7\n",
            "batch_loss :: 1.331300973892212 // batch_acc :: 0.53125\n",
            "min_gz_norm 901.8611 nstep 7\n",
            "min_gz_norm 0.059806157 nstep 7\n",
            "batch_loss :: 1.6104130744934082 // batch_acc :: 0.484375\n",
            "min_gz_norm 916.4718 nstep 7\n",
            "min_gz_norm 0.063246675 nstep 7\n",
            "batch_loss :: 1.6262822151184082 // batch_acc :: 0.515625\n",
            "min_gz_norm 901.86017 nstep 7\n",
            "min_gz_norm 0.051600136 nstep 7\n",
            "batch_loss :: 1.4998078346252441 // batch_acc :: 0.5\n",
            "min_gz_norm 906.805 nstep 7\n",
            "min_gz_norm 0.04787261 nstep 7\n",
            "batch_loss :: 1.3501570224761963 // batch_acc :: 0.546875\n",
            "min_gz_norm 899.5847 nstep 7\n",
            "min_gz_norm 0.04467603 nstep 7\n",
            "batch_loss :: 1.3994028568267822 // batch_acc :: 0.515625\n",
            "min_gz_norm 904.60394 nstep 7\n",
            "min_gz_norm 0.056291837 nstep 7\n",
            "batch_loss :: 1.588292121887207 // batch_acc :: 0.484375\n",
            "min_gz_norm 917.06854 nstep 7\n",
            "min_gz_norm 0.046938352 nstep 7\n",
            "batch_loss :: 1.514949083328247 // batch_acc :: 0.4375\n",
            "min_gz_norm 901.5937 nstep 7\n",
            "min_gz_norm 0.052654855 nstep 7\n",
            "batch_loss :: 1.525305986404419 // batch_acc :: 0.484375\n",
            "min_gz_norm 906.0686 nstep 7\n",
            "min_gz_norm 0.05152947 nstep 7\n",
            "batch_loss :: 1.3453469276428223 // batch_acc :: 0.5\n",
            "min_gz_norm 904.2272 nstep 7\n",
            "min_gz_norm 0.04595209 nstep 7\n",
            "batch_loss :: 1.3697820901870728 // batch_acc :: 0.625\n",
            "min_gz_norm 906.42065 nstep 7\n",
            "min_gz_norm 0.04338526 nstep 7\n",
            "batch_loss :: 1.41725492477417 // batch_acc :: 0.5625\n",
            "min_gz_norm 910.4463 nstep 7\n",
            "min_gz_norm 0.048647538 nstep 7\n",
            "batch_loss :: 1.2530136108398438 // batch_acc :: 0.65625\n",
            "min_gz_norm 913.4741 nstep 7\n",
            "min_gz_norm 0.049942043 nstep 7\n",
            "batch_loss :: 1.5384057760238647 // batch_acc :: 0.5\n",
            "min_gz_norm 898.04004 nstep 7\n",
            "min_gz_norm 0.052897766 nstep 7\n",
            "batch_loss :: 1.551520586013794 // batch_acc :: 0.453125\n",
            "min_gz_norm 905.2725 nstep 7\n",
            "min_gz_norm 0.053210746 nstep 7\n",
            "batch_loss :: 1.2109836339950562 // batch_acc :: 0.625\n",
            "min_gz_norm 900.50604 nstep 7\n",
            "min_gz_norm 0.04278272 nstep 7\n",
            "batch_loss :: 1.2800464630126953 // batch_acc :: 0.53125\n",
            "min_gz_norm 916.51575 nstep 7\n",
            "min_gz_norm 0.047450714 nstep 7\n",
            "batch_loss :: 1.2209217548370361 // batch_acc :: 0.59375\n",
            "min_gz_norm 900.2195 nstep 7\n",
            "min_gz_norm 0.051237464 nstep 7\n",
            "batch_loss :: 1.414860725402832 // batch_acc :: 0.453125\n",
            "min_gz_norm 900.8332 nstep 7\n",
            "min_gz_norm 0.04449469 nstep 7\n",
            "batch_loss :: 1.2993746995925903 // batch_acc :: 0.5625\n",
            "min_gz_norm 901.2245 nstep 7\n",
            "min_gz_norm 0.049424417 nstep 7\n",
            "batch_loss :: 1.5656356811523438 // batch_acc :: 0.421875\n",
            "min_gz_norm 902.40393 nstep 7\n",
            "min_gz_norm 0.05291047 nstep 7\n",
            "batch_loss :: 1.3675122261047363 // batch_acc :: 0.515625\n",
            "min_gz_norm 904.89014 nstep 7\n",
            "min_gz_norm 0.04693822 nstep 7\n",
            "batch_loss :: 1.4728065729141235 // batch_acc :: 0.46875\n",
            "min_gz_norm 902.8828 nstep 7\n",
            "min_gz_norm 0.04784085 nstep 7\n",
            "batch_loss :: 1.543229103088379 // batch_acc :: 0.34375\n",
            "min_gz_norm 911.1486 nstep 7\n",
            "min_gz_norm 0.05511939 nstep 7\n",
            "batch_loss :: 1.3836867809295654 // batch_acc :: 0.578125\n"
          ]
        }
      ]
    }
  ]
}