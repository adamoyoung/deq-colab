{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "9c705060-e3f3-4293-e4de-a00271798e3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "jax.config.update(\"jax_enable_x64\", True)\n",
        "import jax.lax as lax\n",
        "from jax import random\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 243,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", ls=False, name=\"unknown\"):\n",
        "    bsz, total_hsize, seq_len = x0.shape\n",
        "    g = lambda y: f(y) - y\n",
        "    dev = x0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    x_est = x0           # (bsz, 2d, L')\n",
        "    gx = g(x_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_xest, lowest_gx = 0, x_est, gx\n",
        "\n",
        "    while nstep < threshold:\n",
        "        x_est, gx, delta_x, delta_gx, ite = line_search_jax(update, x_est, gx, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gx)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gx + x_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_xest, lowest_gx = lax.stop_gradient(x_est.copy()), lax.stop_gradient(gx.copy())\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_x)\n",
        "        u = (delta_x - matvec_jax(part_Us, part_VTs, delta_gx)) / jnp.einsum('bij, bij -> b', vT, delta_gx)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    return {\"result\": lowest_xest,\n",
        "            \"lowest\": lowest_dict[stop_mode],\n",
        "            \"nstep\": lowest_step_dict[stop_mode],\n",
        "            \"prot_break\": prot_break,\n",
        "            \"abs_trace\": trace_dict['abs'],\n",
        "            \"rel_trace\": trace_dict['rel'],\n",
        "            \"eps\": eps,\n",
        "            \"threshold\": threshold}\n",
        "\n",
        "\n",
        "def newton_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", name=\"unknown\"):\n",
        "\n",
        "    g = lambda y: f(y) - y\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    x = x0\n",
        "    gx = g(x)\n",
        "    gx_norm = jnp.linalg.norm(gx)\n",
        "    nstep = 0\n",
        "    # print(gx_norm)\n",
        "\n",
        "    while nstep < threshold:\n",
        "      # solve system\n",
        "      delta_x = jnp.linalg.solve(jac_g(x),-g(x))\n",
        "      x = x + delta_x\n",
        "      gx = g(x)\n",
        "      gx_norm = jnp.linalg.norm(gx)\n",
        "      nstep += 1\n",
        "      # print(gx_norm)\n",
        "\n",
        "    return x, gx, gx_norm"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3)  # can also be (5, 5), modify later\n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "''' \n",
        "    assert statement we'll need    \n",
        "    assert that the number of branches == len(input_channel_vector)\n",
        "    assert also that num_branches == len(kernel_size_vector)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "EzcfTWz6agE_",
        "outputId": "29feb255-4dcd-49ae-dcfd-d939a0d9869c"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    assert statement we'll need    \\n    assert that the number of branches == len(input_channel_vector)\\n    assert also that num_branches == len(kernel_size_vector)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def _downsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3, 3), strides=(2,2), padding=1, use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            #down_block.append(module)\n",
        "        print(\"down_block\", down_block)\n",
        "        seq = nn.Sequential(down_block)\n",
        "        print('seq', seq)\n",
        "        return seq\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        #self.downsample_fn = self._downsample()\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        print(\"num_samples\", num_samples)\n",
        "        for n in range(num_samples):\n",
        "            print(\"nnnnnnn\", n)\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            \n",
        "            print(\"inter_chan\", inter_chan)\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(conv_down)\n",
        "            #down_block.append(group_down)\n",
        "                 \n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(module)\n",
        "            \n",
        "        print(\"down_block\", down_block)\n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "        #self.layers = down_block\n",
        "        #print('seq', self.layers)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        print(\"zPlus\", z_plus.shape)\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        '''\n",
        "        z = z_plus\n",
        "        for i, lyr in enumerate(self.layers[:-1]):\n",
        "            print(i, z.shape)\n",
        "            z = lyr(z)\n",
        "            z = nn.relu(z)\n",
        "            print(i, z.shape)\n",
        "            #z = nn.sigmoid(z)  # nn.silu(z)  # jnp.tanh(z)  # nn.sigmoid(z)\n",
        "        out = self.layers[-1](z)\n",
        "        '''\n",
        "        print(\"out\", out.shape)\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        print(\"uptype\", type(self.upsample_fn))\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class f_theta(nn.Module):\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    features: Tuple[int] = (16, 4)\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    \n",
        "    # branches: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "\n",
        "\n",
        "    #  TODO HERE\n",
        "    # UnfilteredStackTrace: jax.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (64, 32640, 1) and (64, 32, 32, 24)\n",
        "    def cringy_reshape(self, in_vec, shape_list):\n",
        "        start = 0\n",
        "        out_vec = []\n",
        "        in_vec = jnp.array(in_vec)\n",
        "        for size in shape_list:\n",
        "            my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "            end = start+my_elems\n",
        "            my_chunk = in_vec[:, start:end]\n",
        "            start += my_elems\n",
        "            my_chunk = jnp.reshape(my_chunk, size)\n",
        "            out_vec.append(my_chunk)\n",
        "\n",
        "        return out_vec\n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # self.downsample = DownSample(channels=self.channels,\n",
        "        #                              num_groups=self.num_groups)\n",
        "        # self.upsample = UpSample(channels=self.channels,\n",
        "        #                          num_groups=self.num_groups)\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    # array.append(z_plus[i])\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                 #channel_dimension=channel_dimensions)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                # channel_dimension=channel_dimensions)\n",
        "                    # array.append(nn.Module(sampled))\n",
        "                    array.append(sampled)\n",
        "            # fuse_layers.append(nn.Module(array))\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.relu,\n",
        "                                          nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  #kernel_init=self.kernel_init, \n",
        "                                                  use_bias=False),\n",
        "                                          nn.GroupNorm(num_groups=self.num_groups//2)]))\n",
        "                                                       #group_size=self.channels[i])]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, x, injection, shape_list):\n",
        "        x = self.cringy_reshape(x, shape_list)\n",
        "        #print('preshape injection', injection.shape)\n",
        "        #injection = self.cringy_reshape(injection, shape_list)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        \n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](x[i], i, injection[i])) # z, branch, x\n",
        "\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "          intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "          for j in range(self.num_branches):\n",
        "            if i == j:\n",
        "              intermediate_i += branch_outputs[j]\n",
        "            else:\n",
        "              print(\"i,j\", i, j)\n",
        "              if self.fuse_branches[i][j] is not None:\n",
        "                  print(i,j)\n",
        "                  print('bshape', branch_outputs[j].shape)\n",
        "                  temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                  print(\"temp\", type(temp))\n",
        "                  print(\"inter\", type(intermediate_i))\n",
        "                  intermediate_i += temp\n",
        "                  print(\"inter\", type(intermediate_i))\n",
        "              else:\n",
        "                  raise Exception(\"Should not happen.\")\n",
        "              #print('mimimi', self.fuse_branches[i][j])\n",
        "              #intermediate_i += self.fuse_branches[i][j](branch_outputs[j])\n",
        "          fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "\n",
        "        return fuse_outputs\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQModel(nn.Module):\n",
        "    solver_fn: Callable\n",
        "\n",
        "    num_groups: int = 8\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "    branches: List[int] = field(default_factory=lambda:[1, 1, 1])\n",
        "    training: bool = True\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "    features: Tuple[int] = (16, 4)\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.branches)\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3,3), strides=(1,1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3,3), strides=(1,1))\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.model = f_theta(num_branches=len(self.channels), channels=self.channels, num_groups=self.num_groups)\n",
        "                                         \n",
        "    def __call__(self, x):\n",
        "        #x = self.transform(x)\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=False))\n",
        "        x = self.relu(self.bn2(self.conv2(x), use_running_average=False))\n",
        "        \n",
        "        x_list = [x]\n",
        "        for i in range(1, self.num_branches):\n",
        "            bs, H, W, _ = x_list[-1].shape\n",
        "            new_item = jnp.zeros((bs, H//2, W//2, self.channels[i]))\n",
        "            x_list.append(new_item)\n",
        "        z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "        shape_list = [el.shape for el in z_list]\n",
        "        \n",
        "        bsz = x.shape[0]\n",
        "        # func = lambda z: self.model(x=z, injection=x_list, shape_list=shape_list)\n",
        "        def make_vec(in_vec):\n",
        "            return jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in in_vec], axis=1)\n",
        "        def func(z):\n",
        "            out = self.model(x=z, injection=x_list, shape_list=shape_list)\n",
        "            return make_vec(out) \n",
        "        # z_vec = jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in z_list], axis=1)\n",
        "        z_vec = make_vec(z_list) \n",
        "        result = self.solver_fn(func, z_vec, threshold=3)\n",
        "        z_vec = result['result']\n",
        "        output = z_vec\n",
        "        if self.training:\n",
        "            output = func(z_vec)\n",
        "            #output = func(z_vec.requires_grad_())\n",
        "        # jac_loss = jac_loss_estimate(output, z1) # comes from the follow-up paper\n",
        "        jac_loss = None\n",
        "        \n",
        "        y_list = output # TO DO -- for now without dropout!\n",
        "\n",
        "        return y_list, jac_loss"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    label = jax.nn.one_hot(label, num_classes=num_classes)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.CIFAR10(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.CIFAR10(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:1000], train_ds.targets[:1000])\n",
        "    test_images, test_labels = transform(test_ds.data[:200], test_ds.targets[:200])\n",
        "    return train_images, train_labels, test_images, test_labels"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 250,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    '''\n",
        "    extra thing: warm-up using gradient descent in pytorch code of official repo\n",
        "    --> check impact of that and maybe also cost etc (eg if only one layer etc)\n",
        "    '''\n",
        "\n",
        "    max_itr = 1000\n",
        "    print_interval = 100\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    train_size = train_images.shape[0]\n",
        "    batch_size = 64\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    solver_fn = broyden_jax\n",
        "\n",
        "    my_deq = MDEQModel(solver_fn=solver_fn)\n",
        "    print(my_deq)\n",
        "\n",
        "    # def cross_entropy_loss(*, logits, labels):\n",
        "    def cross_entropy_loss(logits, labels):\n",
        "        ''' \n",
        "        should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "        if getting funny results maybe remove log of logits\n",
        "        '''\n",
        "        one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "        return -jnp.mean(jnp.sum(one_hot_labels * jnp.log(logits), axis=-1))\n",
        "\n",
        "    png = jax.random.PRNGKey(0)\n",
        "    png, key = jax.random.split(png, 2)\n",
        "    dummy_input = jnp.ones((batch_size, 32, 32, 3))\n",
        "\n",
        "    print('calling model.init')\n",
        "    weights = my_deq.init(rngs=jax.random.PRNGKey(0), x=dummy_input)\n",
        "    print('called it')\n",
        "\n",
        "\n",
        "    optimizer = optax.adamw(learning_rate=0.001, weight_decay=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "    def loss(weights, x_batch, y_true):\n",
        "        y_batch = my_deq(weights, x_batch)\n",
        "        return loss_fn(y_batch, y_true)\n",
        "  \n",
        "    def step(weights, opt_state, x_batch, y_true):\n",
        "        print('learning how to walk')\n",
        "        loss_vals, grad = jax.value_and_grad(loss, has_aux=False)(weights, x_batch, y_true)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "        return weights, opt_state, loss_vals\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=train_size, dtype=jnp.int32)\n",
        "            print('idxs', idxs)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler():\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "\n",
        "        return shuffled_indices\n",
        "\n",
        "\n",
        "    # g = generator(batch_size=batch_size)\n",
        "    # print('g', g)\n",
        "\n",
        "    \n",
        "    for itr in range(max_itr):\n",
        "        idxs = list_shuffler()\n",
        "        start, end = 0, 0\n",
        "           \n",
        "        # batch_idxs = next(g)\n",
        "        while end < len(idxs):\n",
        "            print('start', start, 'end', end)\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            print('start', start, 'end', end)\n",
        "            x_batch = train_images[idxs_to_grab]\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "            print('start', start, 'end', end)\n",
        "\n",
        "            weights, opt_state, loss_vals = step(weights=weights,\n",
        "                                                opt_state=opt_state,\n",
        "                                                x_batch=x_batch,\n",
        "                                                y_true=y_true)\n",
        "            \n",
        "            if itr % print_interval == 0:\n",
        "                print(\"\\tat step\", itr, \"have loss\", loss_vals)\n",
        "\n",
        "            if loss_vals < 1e-5:\n",
        "                break\n",
        "        "
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "FQh6WBQmokr8",
        "outputId": "a67b4279-3a36-4561-f160-5693dfd452ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "MDEQModel(\n",
            "    # attributes\n",
            "    solver_fn = broyden_jax\n",
            "    num_groups = 8\n",
            "    channels = [24, 24, 24]\n",
            "    branches = [1, 1, 1]\n",
            "    training = True\n",
            "    features = (16, 4)\n",
            ")\n",
            "calling model.init\n",
            "i,j 0 1\n",
            "0 1\n",
            "bshape (64, 16, 16, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 2\n",
            "0 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 0\n",
            "1 0\n",
            "bshape (64, 32, 32, 24)\n",
            "num_samples 1\n",
            "nnnnnnn 0\n",
            "inter_chan 24\n",
            "down_block [Conv(), GroupNorm(\n",
            "    # attributes\n",
            "    num_groups = 8\n",
            "    group_size = None\n",
            "    epsilon = 1e-06\n",
            "    dtype = float32\n",
            "    param_dtype = float32\n",
            "    use_bias = True\n",
            "    use_scale = True\n",
            "    bias_init = zeros\n",
            "    scale_init = ones\n",
            ")]\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 16, 16, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 2\n",
            "1 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 0\n",
            "2 0\n",
            "bshape (64, 32, 32, 24)\n",
            "num_samples 2\n",
            "nnnnnnn 0\n",
            "inter_chan 24\n",
            "nnnnnnn 1\n",
            "inter_chan 24\n",
            "down_block [Conv(), GroupNorm(\n",
            "    # attributes\n",
            "    num_groups = 8\n",
            "    group_size = None\n",
            "    epsilon = 1e-06\n",
            "    dtype = float32\n",
            "    param_dtype = float32\n",
            "    use_bias = True\n",
            "    use_scale = True\n",
            "    bias_init = zeros\n",
            "    scale_init = ones\n",
            "), <jax._src.custom_derivatives.custom_jvp object at 0x7f3a331c6850>, Conv(), GroupNorm(\n",
            "    # attributes\n",
            "    num_groups = 8\n",
            "    group_size = None\n",
            "    epsilon = 1e-06\n",
            "    dtype = float32\n",
            "    param_dtype = float32\n",
            "    use_bias = True\n",
            "    use_scale = True\n",
            "    bias_init = zeros\n",
            "    scale_init = ones\n",
            ")]\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 1\n",
            "2 1\n",
            "bshape (64, 16, 16, 24)\n",
            "num_samples 1\n",
            "nnnnnnn 0\n",
            "inter_chan 24\n",
            "down_block [Conv(), GroupNorm(\n",
            "    # attributes\n",
            "    num_groups = 8\n",
            "    group_size = None\n",
            "    epsilon = 1e-06\n",
            "    dtype = float32\n",
            "    param_dtype = float32\n",
            "    use_bias = True\n",
            "    use_scale = True\n",
            "    bias_init = zeros\n",
            "    scale_init = ones\n",
            ")]\n",
            "zPlus (64, 16, 16, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 1\n",
            "0 1\n",
            "bshape (64, 16, 16, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 2\n",
            "0 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 0\n",
            "1 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 16, 16, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 2\n",
            "1 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 0\n",
            "2 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 1\n",
            "2 1\n",
            "bshape (64, 16, 16, 24)\n",
            "zPlus (64, 16, 16, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 1\n",
            "0 1\n",
            "bshape (64, 16, 16, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 2\n",
            "0 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 0\n",
            "1 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 16, 16, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 2\n",
            "1 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 0\n",
            "2 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 1\n",
            "2 1\n",
            "bshape (64, 16, 16, 24)\n",
            "zPlus (64, 16, 16, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 1\n",
            "0 1\n",
            "bshape (64, 16, 16, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 2\n",
            "0 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 0\n",
            "1 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 16, 16, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 2\n",
            "1 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 0\n",
            "2 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 1\n",
            "2 1\n",
            "bshape (64, 16, 16, 24)\n",
            "zPlus (64, 16, 16, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 1\n",
            "0 1\n",
            "bshape (64, 16, 16, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 0 2\n",
            "0 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 0\n",
            "1 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 16, 16, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 1 2\n",
            "1 2\n",
            "bshape (64, 8, 8, 24)\n",
            "uptype <class 'flax.linen.combinators.Sequential'>\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 0\n",
            "2 0\n",
            "bshape (64, 32, 32, 24)\n",
            "zPlus (64, 32, 32, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "i,j 2 1\n",
            "2 1\n",
            "bshape (64, 16, 16, 24)\n",
            "zPlus (64, 16, 16, 24)\n",
            "out (64, 8, 8, 24)\n",
            "temp <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "inter <class 'jaxlib.xla_extension.DeviceArray'>\n",
            "called it\n",
            "start 0 end 0\n",
            "start 0 end 64\n",
            "start 64 end 64\n",
            "learning how to walk\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-252-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                 \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                 y_true=y_true)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(weights, opt_state, x_batch, y_true)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning how to walk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/traceback_util.py\u001b[0m in \u001b[0;36mreraise_with_filtered_traceback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36mvalue_and_grad_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    951\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m       \u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvjp_py\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_vjp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_partial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mdyn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_axes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_axes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    953\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/api.py\u001b[0m in \u001b[0;36m_vjp\u001b[0;34m(fun, has_aux, reduce_axes, *primals)\u001b[0m\n\u001b[1;32m   2408\u001b[0m     out_primal, out_vjp = ad.vjp(\n\u001b[0;32m-> 2409\u001b[0;31m         flat_fun, primals_flat, reduce_axes=reduce_axes)\n\u001b[0m\u001b[1;32m   2410\u001b[0m     \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mvjp\u001b[0;34m(traceable, primals, has_aux, reduce_axes)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mout_primals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinearize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mprimals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/ad.py\u001b[0m in \u001b[0;36mlinearize\u001b[0;34m(traceable, *primals, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflatten_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_to_jaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjvpfun_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m   \u001b[0mout_primals_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_tangents_pvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_pvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/_src/profiler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mTraceAnnotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdecorator_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/interpreters/partial_eval.py\u001b[0m in \u001b[0;36mtrace_to_jaxpr\u001b[0;34m(fun, pvals, instantiate)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[0mfun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_to_subjaxpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstantiate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mjaxpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mout_pvals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jax/linear_util.py\u001b[0m in \u001b[0;36mcall_wrapped\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m       \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(weights, x_batch, y_true)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_deq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/transforms.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mforce\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlinen_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_named_call\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_setup\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mprewrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m     \u001b[0mfn_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36mwrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_wrapped_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/flax/linen/module.py\u001b[0m in \u001b[0;36m_call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_stack\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnfilteredStackTrace\u001b[0m: TypeError: __call__() takes 2 positional arguments but 3 were given\n\nThe stack trace below excludes JAX-internal frames.\nThe preceding is the original exception that occurred, unmodified.\n\n--------------------",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-252-2da0ffaf5447>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m                                                 \u001b[0mopt_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                                 \u001b[0mx_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                                 y_true=y_true)\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mprint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(weights, opt_state, x_batch, y_true)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'learning how to walk'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-251-df4003c06b56>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(weights, x_batch, y_true)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_deq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __call__() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breakdown of code overall:\n",
        "\n",
        "\n",
        "*   MDEQ modul\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "7RvQXU3CROAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}