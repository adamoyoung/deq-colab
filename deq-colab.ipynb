{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "bf8fb35c-b647-4180-816a-f2eaa9c70cfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "# jax.config.update(\"jax_enable_x64\", True)\n",
        "# jax.config.update(\"jax_check_tracer_leaks\", True)\n",
        "# import os\n",
        "# os.environ[\"JAX_CHECK_TRACER_LEAKS\"] = \"1\"\n",
        "# jax.checking_leaks = True\n",
        "# jax.check_tracer_leaks = True\n",
        "\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(g, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False, name=\"unknown\"):\n",
        "    bsz, total_hsize, seq_len = x0.shape\n",
        "    # g = lambda y: f(y) - y\n",
        "    dev = x0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    x_est = x0           # (bsz, 2d, L')\n",
        "    gx = g(x_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_xest, lowest_gx = 0, x_est, gx\n",
        "\n",
        "    while nstep < threshold:\n",
        "        x_est, gx, delta_x, delta_gx, ite = line_search_jax(update, x_est, gx, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gx)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gx + x_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    # lowest_xest, lowest_gx = lax.stop_gradient(x_est.copy()), lax.stop_gradient(gx.copy())\n",
        "                    # lowest_xest, lowest_gx = lax.stop_gradient(x_est), lax.stop_gradient(gx)\n",
        "                    # lowest_xest, lowest_gx = lax.stop_gradient(jnp.copy(x_est)), lax.stop_gradient(jnp.copy(gx))\n",
        "                    lowest_xest, lowest_gx = jnp.copy(x_est), jnp.copy(gx)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_x)\n",
        "        u = (delta_x - matvec_jax(part_Us, part_VTs, delta_gx)) / jnp.einsum('bij, bij -> b', vT, delta_gx)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gx)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_xest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_xest\n",
        "\n",
        "\n",
        "def newton_jax(g, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, name=\"unknown\"):\n",
        "\n",
        "    # g = lambda y: f(y) - y\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    x = x0\n",
        "    gx = g(x)\n",
        "    gx_norm = jnp.linalg.norm(gx)\n",
        "    nstep = 0\n",
        "    # print(gx_norm)\n",
        "\n",
        "    while nstep < threshold:\n",
        "      # solve system\n",
        "      delta_x = jnp.linalg.solve(jac_g(x),-g(x))\n",
        "      x = x + delta_x\n",
        "      gx = g(x)\n",
        "      gx_norm = jnp.linalg.norm(gx)\n",
        "      nstep += 1\n",
        "      # print(gx_norm)\n",
        "\n",
        "    if result_dict:\n",
        "        return {'result': x, 'gradient': gx, 'gx_norm': gx_norm}\n",
        "    else:\n",
        "        return x"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Talking about MDEQ model..."
      ],
      "metadata": {
        "id": "jHtzFU33GEoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3)  # can also be (5, 5), modify later\n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=1)#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    \n",
        "''' \n",
        "    assert statement we'll need    \n",
        "    assert that the number of branches == len(input_channel_vector)\n",
        "    assert also that num_branches == len(kernel_size_vector)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "EzcfTWz6agE_",
        "outputId": "de3a384f-0119-4c7c-c548-ca7d85209965"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" \\n    assert statement we'll need    \\n    assert that the number of branches == len(input_channel_vector)\\n    assert also that num_branches == len(kernel_size_vector)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def _downsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res\n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3, 3), strides=(2,2), padding=1, use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            #down_block.append(module)\n",
        "        seq = nn.Sequential(down_block)\n",
        "        return seq\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        #self.downsample_fn = self._downsample()\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        for n in range(num_samples):\n",
        "            inter_chan = self.in_chan if n < num_samples-1 else self.out_chan\n",
        "            \n",
        "            conv_down = nn.Conv(features=inter_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "                               #, kernel_init=self.kernel_init, use_bias=False)\n",
        "            group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "                                      #group_size=inter_chan)\n",
        "            relu_down = nn.relu\n",
        "            # module_list = [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(conv_down)\n",
        "            #down_block.append(group_down)\n",
        "                 \n",
        "            if n < num_samples - 1:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down, relu_down]\n",
        "                down_block += [conv_down, group_down, relu_down]\n",
        "            else:\n",
        "                # module = nn.Sequential([conv_down,\n",
        "                # module = [conv_down, group_down]\n",
        "                down_block += [conv_down, group_down]\n",
        "            \n",
        "            #down_block.append(module)\n",
        "            \n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "        #self.layers = down_block\n",
        "        #print('seq', self.layers)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        '''\n",
        "        z = z_plus\n",
        "        for i, lyr in enumerate(self.layers[:-1]):\n",
        "            print(i, z.shape)\n",
        "            z = lyr(z)\n",
        "            z = nn.relu(z)\n",
        "            print(i, z.shape)\n",
        "            #z = nn.sigmoid(z)  # nn.silu(z)  # jnp.tanh(z)  # nn.sigmoid(z)\n",
        "        out = self.layers[-1](z)\n",
        "        '''\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cringy_reshape(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        print(\"We don't really except this to happen :thinking_fade:\")\n",
        "        return in_vec\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "\n",
        "def make_batch_vec(in_vec, bsz):\n",
        "    return jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in in_vec], axis=1)\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class f_theta(nn.Module):\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    #features: Tuple[int] = (16, 4)\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    \n",
        "    # branches: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "\n",
        "\n",
        "    #  TODO HERE\n",
        "    # UnfilteredStackTrace: jax.core.InconclusiveDimensionOperation: Cannot divide evenly the sizes of shapes (64, 32640, 1) and (64, 32, 32, 24)\n",
        "   \n",
        "\n",
        "    def setup(self):\n",
        "\n",
        "        # self.downsample = DownSample(channels=self.channels,\n",
        "        #                              num_groups=self.num_groups)\n",
        "        # self.upsample = UpSample(channels=self.channels,\n",
        "        #                          num_groups=self.num_groups)\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    # array.append(z_plus[i])\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                 #channel_dimension=channel_dimensions)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                        #(z_plus=z_plus, branches=(i, j),\n",
        "                                                # channel_dimension=channel_dimensions)\n",
        "                    # array.append(nn.Module(sampled))\n",
        "                    array.append(sampled)\n",
        "            # fuse_layers.append(nn.Module(array))\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.relu,\n",
        "                                          nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  #kernel_init=self.kernel_init, \n",
        "                                                  use_bias=False),\n",
        "                                          nn.GroupNorm(num_groups=self.num_groups//2)]))\n",
        "                                                       #group_size=self.channels[i])]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, x, injection, shape_list):\n",
        "        injection = injection\n",
        "        if not isinstance(x, list):\n",
        "            x = cringy_reshape(x, shape_list)\n",
        "        #print('preshape injection', injection.shape)\n",
        "        #injection = self.cringy_reshape(injection, shape_list)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        \n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](x[i], i, injection[i])) # z, branch, x\n",
        "\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "                #print('mimimi', self.fuse_branches[i][j])\n",
        "                #intermediate_i += self.fuse_branches[i][j](branch_outputs[j])\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "\n",
        "        return fuse_outputs\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InitLayer(nn.Module):\n",
        "    # solver_fn: Callable\n",
        "\n",
        "    num_groups: int = 8\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    branches: List[int] = field(default_factory=lambda:[1, 1])\n",
        "    #channels: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "    #branches: List[int] = field(default_factory=lambda:[1, 1, 1])\n",
        "    training: bool = True\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "    features: Tuple[int] = (16, 4)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.branches)\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        # self.model = f_theta(num_branches=len(self.channels), channels=self.channels, num_groups=self.num_groups)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        # x = self.relu(self.bn2(self.conv2(x), use_running_average=True))\n",
        "        \n",
        "        x_list = [x]\n",
        "        # for i in range(1, self.num_branches):\n",
        "        #     bs, H, W, _ = x_list[-1].shape\n",
        "        #     new_item = jnp.zeros((bs, H//2, W//2, self.channels[i]))\n",
        "        #     x_list.append(new_item)\n",
        "        for i in range(1, self.num_branches):\n",
        "            bs, H, W, y = x_list[-1].shape\n",
        "            new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "            x_list.append(new_item)\n",
        "        z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "        shape_list = [el.shape for el in z_list]\n",
        "        # bsz = x.shape[0]\n",
        "        # func = lambda z: self.model(x=z, injection=x_list, shape_list=shape_list)\n",
        "        # def make_batch_vec(in_vec, bsz):\n",
        "        #     return jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in in_vec], axis=1)\n",
        "            \n",
        "        # def f_fn(z):\n",
        "        #     out = self.model(x=make_vec(z), injection=x_list, shape_list=shape_list)\n",
        "        #     print('outshapes', [o.shape for o in out])\n",
        "        #     # return make_vec(out) - z\n",
        "        #     return make_vec(out)\n",
        "\n",
        "        return x_list, z_list, shape_list\n",
        "\n",
        "        \n",
        "        # z_vec = jnp.concatenate([elem.reshape(bsz, -1, 1) for elem in z_list], axis=1)\n",
        "        #z_vec = make_vec(z_list) \n",
        "\n",
        "        # z_vec = self._solve_mdeq(f_fn, x0, threshold=3)\n",
        "        # forward, backward = self.solver_fn[0], self.solver_fn[1]\n",
        "\n",
        "        # result = self.solver_fn(func, z_vec, threshold=3) # TODO have threshold as param\n",
        "         \n",
        "        # z_vec = result['result']\n",
        "        # output = z_vec\n",
        "        '''\n",
        "        if self.training:\n",
        "            output = func(z_vec)\n",
        "        '''\n",
        "            #output = func(z_vec.requires_grad_())\n",
        "        # jac_loss = jac_loss_estimate(output, z1) # comes from the follow-up paper\n",
        "        \n",
        "        # y_list = cringy_reshape(output, shape_list) # TO DO -- for now without dropout!\n",
        "        \n",
        "        # return z_vec, f_fn, shape_list "
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    #channels: List[int] = field(default_factory=lambda:[24, 24, 24])\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    #output_channels: List[int] = field(default_factory=lambda:[4, 8, 16])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def _make_layer(self, inplanes, planes):\n",
        "          downsample = False\n",
        "          if inplanes != planes * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(inplanes, planes, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        incre_modules = []\n",
        "        for i, channels  in enumerate(self.channels):\n",
        "            incre_mod = self._make_layer(self.channels[i], self.output_channels[i])\n",
        "            incre_modules.append(incre_mod)\n",
        "        self.incre_modules = incre_modules\n",
        "        downsamp_modules = []\n",
        "        for i in range(len(self.channels)-1):\n",
        "            in_channels = self.output_channels[i] * self.expansion\n",
        "            out_channels = self.output_channels[i+1] * self.expansion\n",
        "            downsamp_module = nn.Sequential([nn.Conv(out_channels, kernel_size=(3,3), strides=(2,2), use_bias=True),\n",
        "                                            #nn.BatchNorm(),\n",
        "                                            nn.relu])\n",
        "            downsamp_modules.append(downsamp_module)\n",
        "        self.downsamp_modules = downsamp_modules\n",
        "\n",
        "        self.final_layer = nn.Sequential([nn.Conv(self.final_chansize, kernel_size=(1,1)),\n",
        "                                         #nn.BatchNorm(),\n",
        "                                         nn.relu])\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y_list):\n",
        "        y = self.incre_modules[0](y_list[0])\n",
        "        for i in range(len(self.downsamp_modules)):\n",
        "            y = self.incre_modules[i+1](y_list[i+1]) + self.downsamp_modules[i](y)\n",
        "        y = self.final_layer(y)\n",
        "        y = nn.avg_pool(y, window_shape=y.shape[1:3])\n",
        "        y = jnp.reshape(y, (y.shape[0], -1))\n",
        "        y = self.classifier(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading MNIST data..."
      ],
      "metadata": {
        "id": "KHZ0JMhUGWFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def transform(image, label, num_classes=10):\n",
        "#     image = jnp.float32(image) / 255.\n",
        "#     label = jnp.array(label)\n",
        "#     return image, label\n",
        "# \n",
        "# def load_data():\n",
        "#     test_ds = torchvision.datasets.CIFAR10(root=\"data\", train=False,download=True)\n",
        "#     train_ds = torchvision.datasets.CIFAR10(root=\"data\", train=True,download=True)\n",
        "# \n",
        "#     train_images, train_labels = transform(train_ds.data[:5000], train_ds.targets[:5000])\n",
        "#     test_images, test_labels = transform(test_ds.data[:200], test_ds.targets[:200])\n",
        "#     return train_images, train_labels, test_images, test_labels\n",
        "\n",
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    # image = np.tile(image, (1,1,1,24))\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    #test_ds = torchvision.datasets.CIFAR10(root=\"data\", train=False,download=True)\n",
        "    #train_ds = torchvision.datasets.CIFAR10(root=\"data\", train=True,download=True)\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data, train_ds.targets)\n",
        "    test_images, test_labels = transform(test_ds.data, test_ds.targets)\n",
        "    print(f\"MUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"MUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assembling stuff: \n",
        "*   implicit differentiation stuff \n",
        "*   ... \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2QtSjI-bGcAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Rootfinder forward\n",
        "'''\n",
        "\n",
        "# def broyden_jax(f, x0, threshold, eps=1e-3, stop_mode=\"rel\", ls=False, name=\"unknown\"):\n",
        "\n",
        "# @partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind_fwd(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 x0: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    #print('kn forward', x0.shape)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, x0, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _fwd_rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray):\n",
        "    z = rootfind_fwd(solver_fn, f_fn, threshold, eps, weights, z)\n",
        "    return z, (weights, z)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _fwd_rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    # weights, z = res\n",
        "    \n",
        "    return None, grad \n",
        "    # return None, None, None, None, None, grad \n",
        "\n",
        "rootfind_fwd.defvjp(_fwd_rootfind_fwd, _fwd_rootfind_bwd)\n",
        "\n",
        "'''\n",
        "Rootfinder backward\n",
        "'''\n",
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,))\n",
        "def rootfind_bwd(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 x0: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, x0, threshold, eps))\n",
        "\n",
        "def _fwd_rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray):\n",
        "    return z, (weights, z)\n",
        "\n",
        "def _bwd_rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,\n",
        "                      grad):\n",
        "    weights, z = res\n",
        "    _, vjp_fun = jax.vjp(f_fn, weights, z)\n",
        "\n",
        "    def x_fn(x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        Jw_T, Jz_T = vjp_fun(x)\n",
        "        return Jz_T + grad\n",
        "\n",
        "    g_0 = jnp.zeros_like(grad)\n",
        "    g = solver_fn(x_fn, g_0, threshold, eps)\n",
        "    print('backward grad norm', jnp.linalg.norm(g))\n",
        "\n",
        "    return None, g\n",
        "    # return None, None, None, None, None, g \n",
        "\n",
        "rootfind_bwd.defvjp(_fwd_rootfind_bwd, _bwd_rootfind_bwd)\n",
        "\n"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_mdeq(solver_fn, f_fn, shape_list, weights, x0, threshold, eps):\n",
        "    g_fn = lambda weights, z: f_fn(weights, z) - z\n",
        "    \n",
        "    x0 = make_batch_vec(x0, shape_list[0][0])\n",
        "    # z_vec_star = rootfind_fwd(solver_fn, f_fn, threshold, eps, weights, cringy_reshape(x0, shape_list))\n",
        "    ##z_vec_star = rootfind_fwd(solver_fn, g_fn, threshold, eps, weights, x0)\n",
        "    ##z_vec_star = f_fn(weights, z_vec_star)\n",
        "    z_vec_star = f_fn(weights, x0)\n",
        "    z_vec_star = rootfind_bwd(solver_fn, g_fn, threshold, eps, weights, z_vec_star)\n",
        "    \n",
        "    return z_vec_star # TODO cringy reshape\n"
      ],
      "metadata": {
        "id": "C8dQEJiqG5DO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_fn(solver_fn: Callable, head: nn.Module, init: nn.Module, f_th: nn.Module, weights: dict, images: jnp.ndarray):\n",
        "    '''\n",
        "    mdeq: lambda function from below (taking weights and images as arguments)\n",
        "    '''\n",
        "\n",
        "    # model_fn return x_list, z_list, shape_list\n",
        "    init_fn = lambda init_weights, images: init.apply(init_weights, images)\n",
        "    # def f_fn(mdeq_weights, images):\n",
        "    #     z, _ = mdeq_fn(mdeq_weights, images)\n",
        "    #     return z\n",
        "\n",
        "\n",
        "    threshold = 7\n",
        "    eps = 1e-3\n",
        "\n",
        "    x_list, z_list, shape_list = init_fn(weights['init'], images) # take care of shape_list\n",
        "\n",
        "    #  f_theta_fn takes x, injection, shape_list and returns list of fused layers (list of lists)\n",
        "    def f_theta_fn(weights, z):\n",
        "        out = f_th.apply(weights, z, x_list, shape_list)\n",
        "        return  make_batch_vec(out, shape_list[0][0])\n",
        "\n",
        "    # print(\"shape out of fn: \", z.shape)\n",
        "    z_star = solve_mdeq(solver_fn=solver_fn, f_fn=f_theta_fn, shape_list=shape_list, weights=weights['f_theta'], x0=z_list, threshold=threshold, eps=eps)\n",
        "    # f_th_val = \n",
        "    # blib = jax.lax.stop_gradient(jnp.linalg.norm((f_theta_fn(weights['f_theta'], z_star) - cringy_reshape(z_star, shape_list)), ord=2))\n",
        "    # print('current residual', blib)\n",
        "    y_batch = cringy_reshape(z_star, shape_list)\n",
        "    # y_batch = mdeq(weights['mdeq'], images)\n",
        "    logits = head.apply(weights['head'], y_batch)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "SJKf2DGbNArr"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def cross_entropy_loss(*, logits, labels):\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rspDYzYaPRln"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    '''\n",
        "    extra thing: warm-up using gradient descent in pytorch code of official repo\n",
        "    --> check impact of that and maybe also cost etc (eg if only one layer etc)\n",
        "    '''\n",
        "\n",
        "    max_itr = 7 \n",
        "    print_interval = 5\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    train_size = train_images.shape[1]\n",
        "    batch_size = 128\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    solver_fn = broyden_jax\n",
        "\n",
        "    my_init = InitLayer()\n",
        "    my_f_theta = f_theta(num_branches=len(my_init.channels), channels=my_init.channels, num_groups=my_init.num_groups)\n",
        "\n",
        "    png = jax.random.PRNGKey(0)\n",
        "    # png, _ = jax.random.split(png, 2)\n",
        "    # init_dummy_input = jnp.ones((batch_size, train_size, train_size, 3))\n",
        "    init_dummy_input = jnp.ones((batch_size, train_size, train_size, 24))\n",
        "    f_theta_dummy_input = jnp.ones((batch_size, train_size, train_size, 24))\n",
        "    cls_dummy_input = [jnp.ones((batch_size, train_size, train_size, 24)),\n",
        "                       jnp.ones((batch_size, train_size//2, train_size//2, 24))]\n",
        "                       #jnp.ones((64, 8, 8, 24)),]\n",
        "    (dummy_x, dummy_inj, dummy_shape), init_weights = my_init.init_with_output(rngs=png, x=init_dummy_input)\n",
        "    dummy_x = make_batch_vec(dummy_x, batch_size)\n",
        "    # png, _ = jax.random.split(png, 2)\n",
        "    f_theta_weights = my_f_theta.init(rngs=png, x=dummy_x, injection=dummy_inj, shape_list=dummy_shape)\n",
        "    # png, _ = jax.random.split(png, 2)\n",
        "    head = Classifier()\n",
        "    cls_weights = head.init(png, cls_dummy_input)\n",
        "    weights = {'init': init_weights, 'f_theta': f_theta_weights ,'head': cls_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true):\n",
        "        logits = forward_fn(solver_fn, head, my_init, my_f_theta, weights, x_batch)\n",
        "        return loss_fn(logits, y_true)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true):\n",
        "        (loss_vals, acc), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler():\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "\n",
        "        return shuffled_indices\n",
        "\n",
        "    max_epoch = 7 \n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    for epoch in range(max_epoch):\n",
        "        idxs = list_shuffler()\n",
        "        start, end = 0, 0\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab]\n",
        "            x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "  \n",
        "            weights, opt_state, batch_loss, batch_acc = step(weights=weights,\n",
        "                                                 opt_state=opt_state,\n",
        "                                                 x_batch=x_batch,\n",
        "                                                 y_true=y_true)\n",
        "            loss_vals.append(batch_loss)\n",
        "            acc_vals.append(batch_acc)\n",
        "\n",
        "\n",
        "            print(f\"batch_loss :: {batch_loss} // batch_acc :: {batch_acc}\")\n",
        "            # loss_vals, grads = jax.value_and_grad(loss, has_aux=False)(optimizer.target, x_batch, y_true)\n",
        "            # optimizer = optimizer.apply_gradient(grads)\n",
        "            \n",
        "        epoch_loss = jnp.average(jnp.array(loss_vals))\n",
        "        epoch_acc = jnp.average(jnp.array(acc_vals))\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(\"\\tat epoch\", epoch, \"have loss\", epoch_loss, \"and acc\", epoch_acc)\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss) "
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Breakdown of code overall:\n",
        "\n",
        "\n",
        "*   MDEQ modul\n",
        "*   List item\n",
        "\n"
      ],
      "metadata": {
        "id": "7RvQXU3CROAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a30f86c-6952-4418-eef2-d3a871926535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUM TRAINING IMAGES:::60000\n",
            "MUM TEST IMAGES:::10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "backward grad norm 0.009178664\n",
            "batch_loss :: 2.385399341583252 // batch_acc :: 0.109375\n",
            "backward grad norm 0.006238869\n",
            "batch_loss :: 2.339388370513916 // batch_acc :: 0.078125\n",
            "backward grad norm 0.004676696\n",
            "batch_loss :: 2.3055286407470703 // batch_acc :: 0.1015625\n",
            "backward grad norm 0.0041114874\n",
            "batch_loss :: 2.30641508102417 // batch_acc :: 0.09375\n",
            "backward grad norm 0.0038465\n",
            "batch_loss :: 2.3015241622924805 // batch_acc :: 0.1875\n",
            "backward grad norm 0.003851834\n",
            "batch_loss :: 2.2941813468933105 // batch_acc :: 0.109375\n",
            "backward grad norm 0.0038835213\n",
            "batch_loss :: 2.325500011444092 // batch_acc :: 0.0703125\n",
            "backward grad norm 0.0039079958\n",
            "batch_loss :: 2.3097846508026123 // batch_acc :: 0.1171875\n",
            "backward grad norm 0.0036431765\n",
            "batch_loss :: 2.287147045135498 // batch_acc :: 0.1171875\n",
            "backward grad norm 0.003745798\n",
            "batch_loss :: 2.2917869091033936 // batch_acc :: 0.1015625\n",
            "backward grad norm 0.003724966\n",
            "batch_loss :: 2.265313148498535 // batch_acc :: 0.1484375\n",
            "backward grad norm 0.00390008\n",
            "batch_loss :: 2.350203037261963 // batch_acc :: 0.0625\n",
            "backward grad norm 0.003863869\n",
            "batch_loss :: 2.281564235687256 // batch_acc :: 0.3046875\n",
            "backward grad norm 0.0036586402\n",
            "batch_loss :: 2.2941102981567383 // batch_acc :: 0.203125\n",
            "backward grad norm 0.0038695382\n",
            "batch_loss :: 2.2970051765441895 // batch_acc :: 0.3046875\n",
            "backward grad norm 0.0038459082\n",
            "batch_loss :: 2.284687042236328 // batch_acc :: 0.1328125\n",
            "backward grad norm 0.003856939\n",
            "batch_loss :: 2.2807776927948 // batch_acc :: 0.1015625\n",
            "backward grad norm 0.0038388972\n",
            "batch_loss :: 2.2665627002716064 // batch_acc :: 0.171875\n",
            "backward grad norm 0.0038994628\n",
            "batch_loss :: 2.280184507369995 // batch_acc :: 0.109375\n",
            "backward grad norm 0.003978137\n",
            "batch_loss :: 2.2541615962982178 // batch_acc :: 0.1484375\n",
            "backward grad norm 0.004088375\n",
            "batch_loss :: 2.2758612632751465 // batch_acc :: 0.09375\n",
            "backward grad norm 0.0039061867\n",
            "batch_loss :: 2.2545371055603027 // batch_acc :: 0.1484375\n",
            "backward grad norm 0.0038069459\n",
            "batch_loss :: 2.277317762374878 // batch_acc :: 0.09375\n",
            "backward grad norm 0.004096906\n",
            "batch_loss :: 2.2540078163146973 // batch_acc :: 0.078125\n",
            "backward grad norm 0.003881687\n",
            "batch_loss :: 2.210606098175049 // batch_acc :: 0.1328125\n",
            "backward grad norm 0.0040492266\n",
            "batch_loss :: 2.184312343597412 // batch_acc :: 0.2890625\n",
            "backward grad norm 0.004871742\n",
            "batch_loss :: 2.1746249198913574 // batch_acc :: 0.25\n",
            "backward grad norm 0.035919596\n",
            "batch_loss :: 2.170318365097046 // batch_acc :: 0.2578125\n",
            "backward grad norm 0.00604623\n",
            "batch_loss :: 2.2846972942352295 // batch_acc :: 0.1171875\n",
            "backward grad norm 0.005425447\n",
            "batch_loss :: 2.3218111991882324 // batch_acc :: 0.0703125\n",
            "backward grad norm 0.005117301\n",
            "batch_loss :: 2.260092258453369 // batch_acc :: 0.15625\n",
            "backward grad norm 0.0052949567\n",
            "batch_loss :: 2.2743661403656006 // batch_acc :: 0.109375\n",
            "backward grad norm 0.005825052\n",
            "batch_loss :: 2.2781105041503906 // batch_acc :: 0.046875\n",
            "backward grad norm 0.0059282295\n",
            "batch_loss :: 2.257204532623291 // batch_acc :: 0.296875\n",
            "backward grad norm 0.006351884\n",
            "batch_loss :: 2.285783290863037 // batch_acc :: 0.140625\n",
            "backward grad norm 0.0066638156\n",
            "batch_loss :: 2.262586832046509 // batch_acc :: 0.109375\n",
            "backward grad norm 0.007177199\n",
            "batch_loss :: 2.1883621215820312 // batch_acc :: 0.140625\n",
            "backward grad norm 0.00721572\n",
            "batch_loss :: 2.265881061553955 // batch_acc :: 0.09375\n",
            "backward grad norm 0.007759057\n",
            "batch_loss :: 2.247706890106201 // batch_acc :: 0.1328125\n",
            "backward grad norm 0.0075140437\n",
            "batch_loss :: 2.2001590728759766 // batch_acc :: 0.2578125\n",
            "backward grad norm 0.0086037675\n",
            "batch_loss :: 2.1971778869628906 // batch_acc :: 0.234375\n",
            "backward grad norm 0.008866446\n",
            "batch_loss :: 2.2177767753601074 // batch_acc :: 0.21875\n",
            "backward grad norm 0.00824843\n",
            "batch_loss :: 2.194349765777588 // batch_acc :: 0.1953125\n",
            "backward grad norm 0.010891646\n",
            "batch_loss :: 2.1658477783203125 // batch_acc :: 0.234375\n",
            "backward grad norm 0.0131196715\n",
            "batch_loss :: 2.092750072479248 // batch_acc :: 0.2578125\n",
            "backward grad norm 0.013316376\n",
            "batch_loss :: 1.947477102279663 // batch_acc :: 0.296875\n",
            "backward grad norm 0.1492775\n",
            "batch_loss :: 2.2513294219970703 // batch_acc :: 0.1484375\n",
            "backward grad norm 0.056940652\n",
            "batch_loss :: 2.0619282722473145 // batch_acc :: 0.3125\n",
            "backward grad norm 0.04520932\n",
            "batch_loss :: 2.214719772338867 // batch_acc :: 0.1484375\n",
            "backward grad norm 0.10270275\n",
            "batch_loss :: 2.31630802154541 // batch_acc :: 0.109375\n",
            "backward grad norm 0.045010976\n",
            "batch_loss :: 2.214909315109253 // batch_acc :: 0.0546875\n",
            "backward grad norm 0.019817917\n",
            "batch_loss :: 2.1924586296081543 // batch_acc :: 0.1015625\n",
            "backward grad norm 0.017801277\n",
            "batch_loss :: 2.1197586059570312 // batch_acc :: 0.234375\n",
            "backward grad norm 0.012008268\n",
            "batch_loss :: 2.1041321754455566 // batch_acc :: 0.2734375\n",
            "backward grad norm 0.011953923\n",
            "batch_loss :: 2.133471965789795 // batch_acc :: 0.234375\n",
            "backward grad norm 0.011394782\n",
            "batch_loss :: 2.0803046226501465 // batch_acc :: 0.21875\n",
            "backward grad norm 0.011359971\n",
            "batch_loss :: 2.122337579727173 // batch_acc :: 0.15625\n",
            "backward grad norm 0.0096708415\n",
            "batch_loss :: 1.9967072010040283 // batch_acc :: 0.1953125\n",
            "backward grad norm 0.010689898\n",
            "batch_loss :: 2.067965269088745 // batch_acc :: 0.2265625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fHT71ffxr6en"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}