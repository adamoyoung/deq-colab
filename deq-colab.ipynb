{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "b682a215-9105-41ad-8b7f-533a1a5084c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, z0, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False):\n",
        "    bsz, total_hsize = z0.shape\n",
        "    orig_shape = (bsz,total_hsize)\n",
        "    seq_len = 1\n",
        "    new_shape = (bsz,total_hsize,seq_len)\n",
        "    z0 = z0.reshape(*new_shape)\n",
        "    def g(_z):\n",
        "        # here it is safe to use x out of scope\n",
        "        return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(*new_shape)\n",
        "    dev = z0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    z_est = z0           # (bsz, 2d, L')\n",
        "    gz = g(z_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_zest, lowest_gz = 0, z_est, gz\n",
        "\n",
        "    while nstep < threshold:\n",
        "        z_est, gz, delta_z, delta_gz, ite = line_search_jax(update, z_est, gz, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gz)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gz + z_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_zest, lowest_gz = jnp.copy(z_est), jnp.copy(gz)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_z)\n",
        "        u = (delta_z - matvec_jax(part_Us, part_VTs, delta_gz)) / jnp.einsum('bij, bij -> b', vT, delta_gz)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    lowest_zest = lowest_zest.reshape(*orig_shape)\n",
        "    # print(\"broyden\",jnp.linalg.norm(z_est),jnp.linalg.norm(gz))\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_zest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_zest\n",
        "\n",
        "\n",
        "def newton_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    # note: f might ignore x0 (i.e. with backward pass)\n",
        "    orig_shape = z0.shape\n",
        "    def g(_z):\n",
        "      # this reshaping is to enable solving with Jacobian\n",
        "      return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(-1)\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    z = z0.reshape(-1)\n",
        "    gz = g(z)\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    nstep = 0\n",
        "\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      # solve system\n",
        "      jgz = jac_g(z)\n",
        "      # print(\"gz\",gz.shape,jnp.linalg.norm(gz))\n",
        "      # print(\"jgz\",jgz.shape,jnp.linalg.norm(jgz))\n",
        "      delta_z = jnp.linalg.solve(jgz,-gz)\n",
        "      # print(\"delta_z\",delta_z.shape,jnp.linalg.norm(delta_z))\n",
        "      z = z + delta_z\n",
        "      # need to compute gx here to decide whether to stop\n",
        "      gz = g(z)\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      nstep += 1\n",
        "\n",
        "    z = z.reshape(*orig_shape).astype(jnp.float32)\n",
        "\n",
        "    # assert False\n",
        "\n",
        "    return z\n",
        "\n",
        "def direct_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    nstep = 0\n",
        "    z_old = z0\n",
        "    z_new = f(z0,x0)\n",
        "    gz = z_new-z_old\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    min_gz_norm, min_z = gz_norm, z_new\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      z_old = z_new\n",
        "      z_new = f(z_old,x0)\n",
        "      gz = z_new-z_old\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      if gz_norm < min_gz_norm:\n",
        "        min_gz_norm, min_z = gz_norm, z_new\n",
        "      nstep += 1\n",
        "    # print(\"min_gz_norm\",min_gz_norm,\"nstep\",nstep)\n",
        "    return min_z\n"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3) \n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        # inject original input if resolution=0 \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        # residual\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EzcfTWz6agE_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        \n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        conv_down = nn.Conv(features=self.in_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        relu_down = nn.relu\n",
        "\n",
        "        for n in range(num_samples-1):\n",
        "            down_block += [conv_down, group_down, relu_down]\n",
        "        conv_down = nn.Conv(features=self.out_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        down_block += [conv_down, group_down]\n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cringy_reshape(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        raise ValueError\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps image to initial latent representation\n",
        "    AKA the grey part in the diagram\n",
        "    \"\"\"\n",
        "\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    training: bool = True\n",
        "\n",
        "    def setup(self):\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A tool for using the \n",
        "    \"\"\"\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def make_cls_block(self, in_chan, out_chan):\n",
        "          downsample = False\n",
        "          if in_chan != out_chan * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(in_chan, out_chan, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        combine_modules = []\n",
        "        for i  in range(len(self.channels)):\n",
        "            output_mod = self.make_cls_block(self.channels[i], self.output_channels[i])\n",
        "            combine_modules.append(output_mod)\n",
        "        self.combine_modules = combine_modules\n",
        "\n",
        "        self.final_layer_conv = nn.Conv(self.final_chansize, kernel_size=(1,1))\n",
        "        self.final_layer_bn = nn.BatchNorm()\n",
        "\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y):\n",
        "        y_final = self.combine_modules[0](y[0])\n",
        "        for i in range(len(self.channels)-1):\n",
        "            y_final = self.combine_modules[i+1](y[i+1]) \n",
        "        y_final = self.final_layer_bn(self.final_layer_conv(y_final), use_running_average=True)\n",
        "        y_final = nn.relu(y_final)\n",
        "        y_final = nn.avg_pool(y_final, window_shape=y_final.shape[1:3])\n",
        "        y_final = jnp.reshape(y_final, (y_final.shape[0], -1))\n",
        "        y_final = self.classifier(y_final)\n",
        "        return y_final"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 z: jnp.ndarray,\n",
        "                 x: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, z, x, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray,\n",
        "                      x: jnp.ndarray):\n",
        "    z = rootfind(solver_fn, f_fn, threshold, eps, weights, z, x)\n",
        "    # print(\"fwd residual\",jnp.linalg.norm(f_fn(weights,z,x)-z)/jnp.linalg.norm(z))\n",
        "    return z, (weights, z, x)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    weights, z, x = res\n",
        "    (_, vjp_fun) = jax.vjp(f_fn, weights, z, x)\n",
        "    def z_fn(z,x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "        return Jz_T + grad\n",
        "    z0 = jnp.zeros_like(grad)\n",
        "    x0 = None # dummy, z_fn does not use x\n",
        "    g = solver_fn(z_fn, z0, x0, threshold, eps)\n",
        "    #print(\"bwd residual\",jnp.linalg.norm(z_fn(g,x0)-g)/jnp.linalg.norm(g))\n",
        "    return (None, g, None)\n",
        "\n",
        "rootfind.defvjp(_rootfind_fwd, _rootfind_bwd)"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQFF(nn.Module):\n",
        "    \"\"\"\n",
        "    The f_{\\theta}(z,x) function that is repeatedly applied\n",
        "    AKA the yellow block in the diagram\n",
        "    \"\"\"\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "   \n",
        "    def setup(self):\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    array.append(sampled)\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  use_bias=False),\n",
        "                                           nn.relu,\n",
        "                                           nn.GroupNorm(num_groups=self.num_groups)]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, z, x, shape_tuple):\n",
        "        \n",
        "        batch_size = z.shape[0]\n",
        "        z_list = cringy_reshape(z,shape_tuple)\n",
        "        x_list = cringy_reshape(x,shape_tuple)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](z_list[i], i, x_list[i])) # z, branch, x\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "          # stick z back into into one vector\n",
        "        fuse_outputs = jnp.concatenate([fo.reshape(batch_size,-1) for fo in fuse_outputs],axis=1)\n",
        "        assert fuse_outputs.shape[1] == z.shape[1]\n",
        "        return fuse_outputs\n"
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdeq_inputs(x,num_branches):\n",
        "\n",
        "  batch_size = x.shape[0]\n",
        "  x_list = [x]\n",
        "  for i in range(1, num_branches):\n",
        "      bs, H, W, y = x_list[-1].shape\n",
        "      new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "      x_list.append(new_item)\n",
        "  z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "  shape_list = [el.shape for el in z_list]\n",
        "  # make them (batched) vectors\n",
        "  x_vec = jnp.concatenate([x.reshape(batch_size,-1) for x in x_list],axis=1)\n",
        "  z_vec = jnp.concatenate([z.reshape(batch_size,-1) for z in z_list],axis=1)\n",
        "  # i'm not sure if tuple is actually important but I like it for non-mutability\n",
        "  shape_tuple = tuple(shape_list)\n",
        "  return x_vec, z_vec, shape_tuple\n",
        "\n",
        "\n",
        "def mdeq_fn(x,encoder,decoder,deqff,all_weights,solver_fn=None,mode='broyden'):\n",
        "    \n",
        "    encoder_weights = all_weights[\"encoder\"]\n",
        "    decoder_weights = all_weights[\"decoder\"]\n",
        "    deqff_weights = all_weights[\"mdeqff\"]\n",
        "    batch_size = x.shape[0]\n",
        "    # transform the input image\n",
        "    x = encoder.apply(encoder_weights,x)\n",
        "    # construct inputs (lots of padding and concatenation)\n",
        "    x, z, shape_tuple = create_mdeq_inputs(x,deqff.num_branches)\n",
        "    \n",
        "    if mode == \"implicit\":\n",
        "        # solve a fixed point equation for fwd/bwd passes\n",
        "        threshold = 7\n",
        "        eps = 1e-3\n",
        "        # the root function can only take 3 ndarrays as input\n",
        "        def deqff_root(_weights,_z,_x):\n",
        "          # note: it's safe to pass the shape_tuple here (no tracers)\n",
        "          return deqff.apply(_weights,_z,_x,shape_tuple)\n",
        "        # apply rootfinder with custom vjp\n",
        "        z = rootfind(solver_fn,deqff_root,threshold,eps,deqff_weights,z,x)\n",
        "  \n",
        "    elif mode == \"non_implicit\":\n",
        "        # use direct solver, backpropagate through the steps\n",
        "        threshold = 10\n",
        "        eps=1e-2\n",
        "        evals = 0\n",
        "        z_old = z\n",
        "        z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "        residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "        while evals < threshold and residual > eps:\n",
        "            z_old = z_new\n",
        "            z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "            residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "            evals += 1\n",
        "        z = z_new\n",
        "      \n",
        "    elif mode == \"warmup\":\n",
        "        # fixed number of function applications\n",
        "        threshold = 5 #1\n",
        "        for i in range(threshold):\n",
        "            z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "\n",
        "    z_list = cringy_reshape(z,shape_tuple)\n",
        "    log_probs = decoder.apply(decoder_weights,z_list)\n",
        "\n",
        "    # this evaluation is just for the purposes of calculating residual\n",
        "    # we don't want any gradient stuff\n",
        "    f_z = jax.lax.stop_gradient(deqff.apply(deqff_weights,z,x,shape_tuple))\n",
        "    residuals = jax.lax.stop_gradient(jnp.linalg.norm(f_z-z) / (jnp.linalg.norm(z)+1e-9))\n",
        "    \n",
        "    return log_probs, residuals"
      ],
      "metadata": {
        "id": "5hI7s8DE1kqX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(images,labels,encoder,decoder,deqff,all_weights):\n",
        "    loss, acc = 0,0\n",
        "    batch_size = 128\n",
        "    start, end = 0, 0\n",
        "    loss_vals = []\n",
        "    acc_vals = []\n",
        "    while end < images.shape[0]:\n",
        "        end = min(start+batch_size, images.shape[0])\n",
        "        x_batch = images[start:end]\n",
        "        x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "\n",
        "        y_true = labels[start:end]\n",
        "        start = end\n",
        "\n",
        "        log_probs, residual = mdeq_fn(x_batch,encoder,decoder,deqff,all_weights,mode='predict')\n",
        "        loss, acc = cross_entropy_loss(log_probs, y_true)\n",
        "        loss_vals.append(loss * x_batch.shape[0])\n",
        "        acc_vals.append(acc * x_batch.shape[0])\n",
        "    return sum(jnp.array(loss_vals)) / images.shape[0], sum(jnp.array(acc_vals)) / images.shape[0], residual"
      ],
      "metadata": {
        "id": "1kUlVk4z0LOV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    # image = np.tile(image, (1,1,1,24))\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:5000], train_ds.targets[:5000])\n",
        "    test_images, test_labels = transform(test_ds.data[:500], test_ds.targets[:500])\n",
        "    # train_images, train_labels = transform(train_ds.data, train_ds.targets)\n",
        "    # test_images, test_labels = transform(test_ds.data, test_ds.targets)\n",
        "    print(f\"MUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"MUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(mode=\"implicit\",solver=\"broyden\"):\n",
        "\n",
        "    assert mode in [\"implicit\", \"non_implicit\", \"warmup\"], \"INCORRECT MODE\"\n",
        "    assert solver in [\"direct\",\"broyden\"]\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    image_size = train_images.shape[1]\n",
        "    batch_size = 128\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    if solver == \"broyden\":\n",
        "        solver_fn = broyden_jax\n",
        "    elif solver == \"direct\":\n",
        "        solver_fn = direct_jax\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    num_groups = 8\n",
        "    channels = [24, 24]\n",
        "    num_branches = 2\n",
        "\n",
        "    # instantiation\n",
        "    encoder = Encoder(channels=channels)\n",
        "    decoder = Classifier() # not sure about what to pass\n",
        "    mdeqff = MDEQFF(num_branches=num_branches, channels=channels, num_groups=num_groups)\n",
        "\n",
        "    # weight initialization\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng, _ = jax.random.split(prng, 2)\n",
        "    x_dummy = jnp.ones((batch_size, image_size, image_size, 24))\n",
        "    x_dummy_2, encoder_weights = encoder.init_with_output(prng,x_dummy)\n",
        "    x_dummy_3, z_dummy, shape_tuple = create_mdeq_inputs(x_dummy_2,num_branches)\n",
        "    z_dummy_2, mdeqff_weights = mdeqff.init_with_output(prng,z_dummy,x_dummy_3,shape_tuple)\n",
        "    z_dummy_3 = cringy_reshape(z_dummy_2,shape_tuple)\n",
        "    o_dummy, classifier_weights = decoder.init_with_output(prng,z_dummy_3)\n",
        "\n",
        "    # collect weights\n",
        "    weights = {'encoder': encoder_weights, 'mdeqff': mdeqff_weights ,'decoder': classifier_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    # optimizer = optax.adamw(learning_rate=0.001,weight_decay=0.0001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true, mode):\n",
        "        logits, residual = mdeq_fn(x_batch,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "        loss, acc = loss_fn(logits, y_true)\n",
        "        return loss, (acc, residual)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true, mode):\n",
        "        (loss_vals, (acc, residual)), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true, mode)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc, residual\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler(seed):\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "        return shuffled_indices\n",
        "    \n",
        "    max_epoch = 50\n",
        "    warmup_epochs = 0\n",
        "    if mode == \"warmup\":\n",
        "      warmup_epochs = 2\n",
        "      max_epoch += warmup_epochs\n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    if mode == \"warmup\":\n",
        "        print(f\"TRAINING MODE:::{mode} (then implicit)\")\n",
        "    else:\n",
        "        print(f\"TRAINING MODE:::{mode}\")\n",
        "    train_loss_vals, val_loss_vals = [], []\n",
        "    train_acc_vals, val_acc_vals = [], []\n",
        "    train_res_vals, val_res_vals = [], []\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        if mode == \"warmup\":\n",
        "          if epoch >= warmup_epochs:\n",
        "            mode = \"implicit\"\n",
        "            print(\"-------------DONE WARM UP---------------\")\n",
        "          elif epoch == 0:\n",
        "            print(\"----------STARTING WARM UP--------------\")\n",
        "            \n",
        "        idxs = list_shuffler(epoch)\n",
        "        start, end = 0, 0\n",
        "\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        res_vals = []\n",
        "\n",
        "        counter = 0\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab,...]\n",
        "            x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "  \n",
        "            weights, opt_state, batch_loss, batch_acc, batch_res = step(weights=weights,\n",
        "                                                                        opt_state=opt_state,\n",
        "                                                                        x_batch=x_batch,\n",
        "                                                                        y_true=y_true,\n",
        "                                                                        mode=mode)\n",
        "            \n",
        "            counter += 1\n",
        "\n",
        "            loss_vals.append(batch_loss * x_batch.shape[0])\n",
        "            acc_vals.append(batch_acc * x_batch.shape[0])\n",
        "            res_vals.append(batch_res * x_batch.shape[0])\n",
        "\n",
        "\n",
        "            print(f\"> epoch {epoch+1}, batch {counter} :: batch_loss = {batch_loss}, batch_acc = {batch_acc}, batch_res = {batch_res} \")\n",
        "\n",
        "        epoch_loss = sum(jnp.array(loss_vals)) / len(idxs)\n",
        "        epoch_acc = sum(jnp.array(acc_vals)) / len(idxs)\n",
        "        epoch_res = sum(jnp.array(res_vals)) / len(idxs)\n",
        "        \n",
        "        train_loss_vals.append(epoch_loss)\n",
        "        train_acc_vals.append(epoch_acc)\n",
        "        train_res_vals.append(epoch_res)\n",
        "\n",
        "        val_loss, val_acc, val_res = predict(test_images,test_labels,encoder,decoder,mdeqff,weights)\n",
        "\n",
        "        val_loss_vals.append(val_loss)\n",
        "        val_acc_vals.append(val_acc)\n",
        "        val_res_vals.append(val_res)\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(f\"\\tTRAIN epoch = {epoch+1} / loss = {epoch_loss} / acc = {epoch_acc} / res = {epoch_res}\")\n",
        "            print(f\"\\tVAL epoch = {epoch+1} / loss = {val_loss} / acc = {val_acc} / res = {val_res}\")\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss)\n",
        "    results = {'train_loss_vals': train_loss_vals,\n",
        "               'train_acc_vals': train_acc_vals,\n",
        "               'train_res_vals': train_res_vals,\n",
        "               'val_loss_vals': val_loss_vals,\n",
        "               'val_acc_vals': val_acc_vals,\n",
        "               'val_res_vals': val_res_vals}\n",
        "    return results"
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "broyden_results = train(mode=\"implicit\",solver=\"broyden\")"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ad5c659-5d7f-4cff-b9c7-21ec5dee00ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MUM TRAINING IMAGES:::5000\n",
            "MUM TEST IMAGES:::500\n",
            "TRAINING MODE:::implicit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> epoch 1, batch 1 :: batch_loss = 2.3378374576568604, batch_acc = 0.09375, batch_res = 0.7393582463264465 \n",
            "> epoch 1, batch 2 :: batch_loss = 2.299461841583252, batch_acc = 0.1171875, batch_res = 0.7329127788543701 \n",
            "> epoch 1, batch 3 :: batch_loss = 2.31638503074646, batch_acc = 0.09375, batch_res = 0.7309147119522095 \n",
            "> epoch 1, batch 4 :: batch_loss = 2.3197405338287354, batch_acc = 0.078125, batch_res = 0.7381411790847778 \n",
            "> epoch 1, batch 5 :: batch_loss = 2.317876100540161, batch_acc = 0.1171875, batch_res = 0.7402586340904236 \n",
            "> epoch 1, batch 6 :: batch_loss = 2.287112236022949, batch_acc = 0.1171875, batch_res = 0.7337208390235901 \n",
            "> epoch 1, batch 7 :: batch_loss = 2.3015542030334473, batch_acc = 0.15625, batch_res = 0.7329257726669312 \n",
            "> epoch 1, batch 8 :: batch_loss = 2.3026649951934814, batch_acc = 0.109375, batch_res = 0.737389862537384 \n",
            "> epoch 1, batch 9 :: batch_loss = 2.3343777656555176, batch_acc = 0.109375, batch_res = 0.7381626963615417 \n",
            "> epoch 1, batch 10 :: batch_loss = 2.311784029006958, batch_acc = 0.109375, batch_res = 0.7379676103591919 \n",
            "> epoch 1, batch 11 :: batch_loss = 2.3418831825256348, batch_acc = 0.1015625, batch_res = 0.7426121234893799 \n",
            "> epoch 1, batch 12 :: batch_loss = 2.342203140258789, batch_acc = 0.1015625, batch_res = 0.7421414852142334 \n",
            "> epoch 1, batch 13 :: batch_loss = 2.3232195377349854, batch_acc = 0.109375, batch_res = 0.7432236671447754 \n",
            "> epoch 1, batch 14 :: batch_loss = 2.310035467147827, batch_acc = 0.1015625, batch_res = 0.7366647124290466 \n",
            "> epoch 1, batch 15 :: batch_loss = 2.2960424423217773, batch_acc = 0.125, batch_res = 0.7317652702331543 \n",
            "> epoch 1, batch 16 :: batch_loss = 2.3115334510803223, batch_acc = 0.0703125, batch_res = 0.7371437549591064 \n",
            "> epoch 1, batch 17 :: batch_loss = 2.287083625793457, batch_acc = 0.1484375, batch_res = 0.7341077923774719 \n",
            "> epoch 1, batch 18 :: batch_loss = 2.2910842895507812, batch_acc = 0.15625, batch_res = 0.7424397468566895 \n",
            "> epoch 1, batch 19 :: batch_loss = 2.2952260971069336, batch_acc = 0.1953125, batch_res = 0.740199625492096 \n",
            "> epoch 1, batch 20 :: batch_loss = 2.2978355884552, batch_acc = 0.125, batch_res = 0.7417250871658325 \n",
            "> epoch 1, batch 21 :: batch_loss = 2.283846855163574, batch_acc = 0.1171875, batch_res = 0.7385996580123901 \n",
            "> epoch 1, batch 22 :: batch_loss = 2.3011293411254883, batch_acc = 0.09375, batch_res = 0.7394621968269348 \n",
            "> epoch 1, batch 23 :: batch_loss = 2.318173885345459, batch_acc = 0.0703125, batch_res = 0.7332516312599182 \n",
            "> epoch 1, batch 24 :: batch_loss = 2.3035778999328613, batch_acc = 0.0703125, batch_res = 0.7355089783668518 \n",
            "> epoch 1, batch 25 :: batch_loss = 2.289447069168091, batch_acc = 0.125, batch_res = 0.7385633587837219 \n",
            "> epoch 1, batch 26 :: batch_loss = 2.3167433738708496, batch_acc = 0.109375, batch_res = 0.7381483316421509 \n",
            "> epoch 1, batch 27 :: batch_loss = 2.3064069747924805, batch_acc = 0.125, batch_res = 0.7355284094810486 \n",
            "> epoch 1, batch 28 :: batch_loss = 2.3163833618164062, batch_acc = 0.09375, batch_res = 0.7379183769226074 \n",
            "> epoch 1, batch 29 :: batch_loss = 2.289519786834717, batch_acc = 0.140625, batch_res = 0.7414718270301819 \n",
            "> epoch 1, batch 30 :: batch_loss = 2.3084444999694824, batch_acc = 0.15625, batch_res = 0.7332581877708435 \n",
            "> epoch 1, batch 31 :: batch_loss = 2.2942371368408203, batch_acc = 0.15625, batch_res = 0.7451616525650024 \n",
            "> epoch 1, batch 32 :: batch_loss = 2.28556489944458, batch_acc = 0.1953125, batch_res = 0.7383474707603455 \n",
            "> epoch 1, batch 33 :: batch_loss = 2.2855770587921143, batch_acc = 0.2578125, batch_res = 0.7339800596237183 \n",
            "> epoch 1, batch 34 :: batch_loss = 2.2852118015289307, batch_acc = 0.2265625, batch_res = 0.7362414002418518 \n",
            "> epoch 1, batch 35 :: batch_loss = 2.2625322341918945, batch_acc = 0.2578125, batch_res = 0.7400206923484802 \n",
            "> epoch 1, batch 36 :: batch_loss = 2.2692389488220215, batch_acc = 0.140625, batch_res = 0.7368435263633728 \n",
            "> epoch 1, batch 37 :: batch_loss = 2.284273386001587, batch_acc = 0.1171875, batch_res = 0.7410894632339478 \n",
            "> epoch 1, batch 38 :: batch_loss = 2.278759479522705, batch_acc = 0.0859375, batch_res = 0.7338345646858215 \n",
            "> epoch 1, batch 39 :: batch_loss = 2.3054378032684326, batch_acc = 0.09375, batch_res = 0.7399564385414124 \n",
            "> epoch 1, batch 40 :: batch_loss = 2.3304929733276367, batch_acc = 0.125, batch_res = 0.7359785437583923 \n",
            "\tTRAIN epoch = 1 / loss = 2.3028507232666016 / acc = 0.1274000108242035 / res = 0.7377142906188965\n",
            "\tVAL epoch = 1 / loss = 2.3032591342926025 / acc = 0.11000000685453415 / res = 1651754532864.0\n",
            "> epoch 2, batch 1 :: batch_loss = 2.3047680854797363, batch_acc = 0.09375, batch_res = 0.7392795085906982 \n",
            "> epoch 2, batch 2 :: batch_loss = 2.280606508255005, batch_acc = 0.1328125, batch_res = 0.7414725422859192 \n",
            "> epoch 2, batch 3 :: batch_loss = 2.2681264877319336, batch_acc = 0.1484375, batch_res = 0.7340167164802551 \n",
            "> epoch 2, batch 4 :: batch_loss = 2.265988349914551, batch_acc = 0.1640625, batch_res = 0.737896203994751 \n",
            "> epoch 2, batch 5 :: batch_loss = 2.273559093475342, batch_acc = 0.1328125, batch_res = 0.7383419871330261 \n",
            "> epoch 2, batch 6 :: batch_loss = 2.2892160415649414, batch_acc = 0.0703125, batch_res = 0.7426621317863464 \n",
            "> epoch 2, batch 7 :: batch_loss = 2.2749152183532715, batch_acc = 0.125, batch_res = 0.7363957762718201 \n",
            "> epoch 2, batch 8 :: batch_loss = 2.273097038269043, batch_acc = 0.109375, batch_res = 0.7424972057342529 \n",
            "> epoch 2, batch 9 :: batch_loss = 2.2815470695495605, batch_acc = 0.1015625, batch_res = 0.7361197471618652 \n",
            "> epoch 2, batch 10 :: batch_loss = 2.27160382270813, batch_acc = 0.1875, batch_res = 0.7368106842041016 \n",
            "> epoch 2, batch 11 :: batch_loss = 2.281224250793457, batch_acc = 0.21875, batch_res = 0.7399194240570068 \n",
            "> epoch 2, batch 12 :: batch_loss = 2.2786903381347656, batch_acc = 0.265625, batch_res = 0.7336606383323669 \n",
            "> epoch 2, batch 13 :: batch_loss = 2.25535249710083, batch_acc = 0.2890625, batch_res = 0.7375785112380981 \n",
            "> epoch 2, batch 14 :: batch_loss = 2.271026134490967, batch_acc = 0.1875, batch_res = 0.7364559173583984 \n",
            "> epoch 2, batch 15 :: batch_loss = 2.264469861984253, batch_acc = 0.1875, batch_res = 0.7328975200653076 \n",
            "> epoch 2, batch 16 :: batch_loss = 2.27329158782959, batch_acc = 0.1953125, batch_res = 0.7362580299377441 \n",
            "> epoch 2, batch 17 :: batch_loss = 2.271111011505127, batch_acc = 0.1640625, batch_res = 0.7364681959152222 \n",
            "> epoch 2, batch 18 :: batch_loss = 2.2425975799560547, batch_acc = 0.21875, batch_res = 0.7386559844017029 \n",
            "> epoch 2, batch 19 :: batch_loss = 2.2337417602539062, batch_acc = 0.1953125, batch_res = 0.7359086275100708 \n",
            "> epoch 2, batch 20 :: batch_loss = 2.2814698219299316, batch_acc = 0.125, batch_res = 0.7408981919288635 \n",
            "> epoch 2, batch 21 :: batch_loss = 2.2519359588623047, batch_acc = 0.171875, batch_res = 0.7385826110839844 \n",
            "> epoch 2, batch 22 :: batch_loss = 2.265349864959717, batch_acc = 0.1015625, batch_res = 0.7341902852058411 \n",
            "> epoch 2, batch 23 :: batch_loss = 2.252713918685913, batch_acc = 0.140625, batch_res = 0.7317918539047241 \n",
            "> epoch 2, batch 24 :: batch_loss = 2.265639543533325, batch_acc = 0.1015625, batch_res = 0.7343071103096008 \n",
            "> epoch 2, batch 25 :: batch_loss = 2.24157452583313, batch_acc = 0.2109375, batch_res = 0.7369587421417236 \n",
            "> epoch 2, batch 26 :: batch_loss = 2.2466423511505127, batch_acc = 0.2421875, batch_res = 0.7373989224433899 \n",
            "> epoch 2, batch 27 :: batch_loss = 2.2459840774536133, batch_acc = 0.21875, batch_res = 0.7406800985336304 \n",
            "> epoch 2, batch 28 :: batch_loss = 2.2665979862213135, batch_acc = 0.1484375, batch_res = 0.7373884320259094 \n",
            "> epoch 2, batch 29 :: batch_loss = 2.250460624694824, batch_acc = 0.171875, batch_res = 0.7374168634414673 \n",
            "> epoch 2, batch 30 :: batch_loss = 2.2285256385803223, batch_acc = 0.203125, batch_res = 0.7397972941398621 \n",
            "> epoch 2, batch 31 :: batch_loss = 2.2481117248535156, batch_acc = 0.1953125, batch_res = 0.7315037250518799 \n",
            "> epoch 2, batch 32 :: batch_loss = 2.2490859031677246, batch_acc = 0.2265625, batch_res = 0.7365196943283081 \n",
            "> epoch 2, batch 33 :: batch_loss = 2.2138543128967285, batch_acc = 0.2421875, batch_res = 0.7367122769355774 \n",
            "> epoch 2, batch 34 :: batch_loss = 2.2531673908233643, batch_acc = 0.2421875, batch_res = 0.7404359579086304 \n",
            "> epoch 2, batch 35 :: batch_loss = 2.2208943367004395, batch_acc = 0.2421875, batch_res = 0.7395280003547668 \n",
            "> epoch 2, batch 36 :: batch_loss = 2.2514595985412598, batch_acc = 0.171875, batch_res = 0.7375299334526062 \n",
            "> epoch 2, batch 37 :: batch_loss = 2.198791027069092, batch_acc = 0.2890625, batch_res = 0.7400458455085754 \n",
            "> epoch 2, batch 38 :: batch_loss = 2.195587635040283, batch_acc = 0.2890625, batch_res = 0.7428340315818787 \n",
            "> epoch 2, batch 39 :: batch_loss = 2.2340428829193115, batch_acc = 0.1875, batch_res = 0.7422652840614319 \n",
            "> epoch 2, batch 40 :: batch_loss = 2.2340800762176514, batch_acc = 0.375, batch_res = 0.7508282661437988 \n",
            "\tTRAIN epoch = 2 / loss = 2.256804943084717 / acc = 0.1826000064611435 / res = 0.737715482711792\n",
            "\tVAL epoch = 2 / loss = 2.3208425045013428 / acc = 0.08400000631809235 / res = 1651754532864.0\n",
            "> epoch 3, batch 1 :: batch_loss = 2.210822343826294, batch_acc = 0.25, batch_res = 0.7365716695785522 \n",
            "> epoch 3, batch 2 :: batch_loss = 2.2053346633911133, batch_acc = 0.234375, batch_res = 0.7373942732810974 \n",
            "> epoch 3, batch 3 :: batch_loss = 2.1922483444213867, batch_acc = 0.3125, batch_res = 0.7355747818946838 \n",
            "> epoch 3, batch 4 :: batch_loss = 2.149860382080078, batch_acc = 0.3046875, batch_res = 0.7357430458068848 \n",
            "> epoch 3, batch 5 :: batch_loss = 2.1646273136138916, batch_acc = 0.28125, batch_res = 0.7408045530319214 \n",
            "> epoch 3, batch 6 :: batch_loss = 2.204592227935791, batch_acc = 0.2890625, batch_res = 0.739698052406311 \n",
            "> epoch 3, batch 7 :: batch_loss = 2.2069191932678223, batch_acc = 0.2109375, batch_res = 0.7340986728668213 \n",
            "> epoch 3, batch 8 :: batch_loss = 2.2318384647369385, batch_acc = 0.2421875, batch_res = 0.7369688153266907 \n",
            "> epoch 3, batch 9 :: batch_loss = 2.168752670288086, batch_acc = 0.2265625, batch_res = 0.7357695698738098 \n",
            "> epoch 3, batch 10 :: batch_loss = 2.150730609893799, batch_acc = 0.234375, batch_res = 0.7351660132408142 \n",
            "> epoch 3, batch 11 :: batch_loss = 2.137528657913208, batch_acc = 0.2890625, batch_res = 0.7434098720550537 \n",
            "> epoch 3, batch 12 :: batch_loss = 2.1593775749206543, batch_acc = 0.1796875, batch_res = 0.7451184391975403 \n",
            "> epoch 3, batch 13 :: batch_loss = 2.130953788757324, batch_acc = 0.265625, batch_res = 0.734121561050415 \n",
            "> epoch 3, batch 14 :: batch_loss = 2.124553680419922, batch_acc = 0.2421875, batch_res = 0.7351098656654358 \n",
            "> epoch 3, batch 15 :: batch_loss = 2.140763521194458, batch_acc = 0.2265625, batch_res = 0.7429141998291016 \n",
            "> epoch 3, batch 16 :: batch_loss = 2.115525245666504, batch_acc = 0.234375, batch_res = 0.7347349524497986 \n",
            "> epoch 3, batch 17 :: batch_loss = 2.1328699588775635, batch_acc = 0.2109375, batch_res = 0.7338478565216064 \n",
            "> epoch 3, batch 18 :: batch_loss = 2.1285948753356934, batch_acc = 0.2421875, batch_res = 0.7388623356819153 \n",
            "> epoch 3, batch 19 :: batch_loss = 2.0936455726623535, batch_acc = 0.2578125, batch_res = 0.7407218813896179 \n",
            "> epoch 3, batch 20 :: batch_loss = 2.131744623184204, batch_acc = 0.1875, batch_res = 0.7383214831352234 \n",
            "> epoch 3, batch 21 :: batch_loss = 2.102766513824463, batch_acc = 0.21875, batch_res = 0.7373287677764893 \n",
            "> epoch 3, batch 22 :: batch_loss = 2.0393924713134766, batch_acc = 0.3515625, batch_res = 0.7329431176185608 \n",
            "> epoch 3, batch 23 :: batch_loss = 2.0370352268218994, batch_acc = 0.2890625, batch_res = 0.7350723743438721 \n",
            "> epoch 3, batch 24 :: batch_loss = 1.9534740447998047, batch_acc = 0.3671875, batch_res = 0.7349864840507507 \n",
            "> epoch 3, batch 25 :: batch_loss = 2.080810070037842, batch_acc = 0.2265625, batch_res = 0.736905574798584 \n",
            "> epoch 3, batch 26 :: batch_loss = 2.0966382026672363, batch_acc = 0.2265625, batch_res = 0.7370452880859375 \n",
            "> epoch 3, batch 27 :: batch_loss = 2.0431408882141113, batch_acc = 0.3125, batch_res = 0.7335658669471741 \n",
            "> epoch 3, batch 28 :: batch_loss = 1.999420404434204, batch_acc = 0.328125, batch_res = 0.7374840974807739 \n",
            "> epoch 3, batch 29 :: batch_loss = 1.9749526977539062, batch_acc = 0.34375, batch_res = 0.7378399968147278 \n",
            "> epoch 3, batch 30 :: batch_loss = 1.9579708576202393, batch_acc = 0.3671875, batch_res = 0.7395051717758179 \n",
            "> epoch 3, batch 31 :: batch_loss = 2.1030969619750977, batch_acc = 0.2265625, batch_res = 0.7431526780128479 \n",
            "> epoch 3, batch 32 :: batch_loss = 2.0290231704711914, batch_acc = 0.2734375, batch_res = 0.7357969284057617 \n",
            "> epoch 3, batch 33 :: batch_loss = 1.9732868671417236, batch_acc = 0.3046875, batch_res = 0.7426143884658813 \n",
            "> epoch 3, batch 34 :: batch_loss = 1.973336100578308, batch_acc = 0.2890625, batch_res = 0.7399218678474426 \n",
            "> epoch 3, batch 35 :: batch_loss = 1.9503229856491089, batch_acc = 0.296875, batch_res = 0.7407408356666565 \n",
            "> epoch 3, batch 36 :: batch_loss = 1.938248634338379, batch_acc = 0.296875, batch_res = 0.736871063709259 \n",
            "> epoch 3, batch 37 :: batch_loss = 1.897883653640747, batch_acc = 0.375, batch_res = 0.7361592054367065 \n",
            "> epoch 3, batch 38 :: batch_loss = 1.9288644790649414, batch_acc = 0.2890625, batch_res = 0.7356112599372864 \n",
            "> epoch 3, batch 39 :: batch_loss = 1.9487754106521606, batch_acc = 0.3359375, batch_res = 0.741962194442749 \n",
            "> epoch 3, batch 40 :: batch_loss = 1.8130414485931396, batch_acc = 0.5, batch_res = 0.7439346313476562 \n",
            "\tTRAIN epoch = 3 / loss = 2.081869602203369 / acc = 0.27320000529289246 / res = 0.7377141714096069\n",
            "\tVAL epoch = 3 / loss = 2.9234609603881836 / acc = 0.08400000631809235 / res = 1651754532864.0\n",
            "> epoch 4, batch 1 :: batch_loss = 1.8998664617538452, batch_acc = 0.28125, batch_res = 0.7370054721832275 \n",
            "> epoch 4, batch 2 :: batch_loss = 1.9897942543029785, batch_acc = 0.2890625, batch_res = 0.7430018186569214 \n",
            "> epoch 4, batch 3 :: batch_loss = 2.035468339920044, batch_acc = 0.234375, batch_res = 0.7417537569999695 \n",
            "> epoch 4, batch 4 :: batch_loss = 2.0385019779205322, batch_acc = 0.234375, batch_res = 0.7399826049804688 \n",
            "> epoch 4, batch 5 :: batch_loss = 2.038498640060425, batch_acc = 0.203125, batch_res = 0.7302820086479187 \n",
            "> epoch 4, batch 6 :: batch_loss = 1.9696214199066162, batch_acc = 0.25, batch_res = 0.7375151515007019 \n",
            "> epoch 4, batch 7 :: batch_loss = 1.8955824375152588, batch_acc = 0.2734375, batch_res = 0.738776445388794 \n",
            "> epoch 4, batch 8 :: batch_loss = 1.7880380153656006, batch_acc = 0.3671875, batch_res = 0.7400956749916077 \n",
            "> epoch 4, batch 9 :: batch_loss = 1.9152781963348389, batch_acc = 0.2734375, batch_res = 0.7390428781509399 \n",
            "> epoch 4, batch 10 :: batch_loss = 1.9485538005828857, batch_acc = 0.1953125, batch_res = 0.7409157752990723 \n",
            "> epoch 4, batch 11 :: batch_loss = 1.903777837753296, batch_acc = 0.28125, batch_res = 0.7359732389450073 \n",
            "> epoch 4, batch 12 :: batch_loss = 1.8646432161331177, batch_acc = 0.328125, batch_res = 0.7383607029914856 \n",
            "> epoch 4, batch 13 :: batch_loss = 1.8659573793411255, batch_acc = 0.359375, batch_res = 0.7361564636230469 \n",
            "> epoch 4, batch 14 :: batch_loss = 1.904127836227417, batch_acc = 0.2734375, batch_res = 0.7332791090011597 \n",
            "> epoch 4, batch 15 :: batch_loss = 1.8761990070343018, batch_acc = 0.265625, batch_res = 0.7314881086349487 \n",
            "> epoch 4, batch 16 :: batch_loss = 1.7725498676300049, batch_acc = 0.328125, batch_res = 0.7331825494766235 \n",
            "> epoch 4, batch 17 :: batch_loss = 1.9073567390441895, batch_acc = 0.28125, batch_res = 0.7360221147537231 \n",
            "> epoch 4, batch 18 :: batch_loss = 1.7844040393829346, batch_acc = 0.3515625, batch_res = 0.7409085035324097 \n",
            "> epoch 4, batch 19 :: batch_loss = 1.7086195945739746, batch_acc = 0.3828125, batch_res = 0.7406742572784424 \n",
            "> epoch 4, batch 20 :: batch_loss = 1.8140863180160522, batch_acc = 0.3125, batch_res = 0.7392241954803467 \n",
            "> epoch 4, batch 21 :: batch_loss = 1.9111225605010986, batch_acc = 0.3203125, batch_res = 0.7435734272003174 \n",
            "> epoch 4, batch 22 :: batch_loss = 1.9016637802124023, batch_acc = 0.2734375, batch_res = 0.7379100918769836 \n",
            "> epoch 4, batch 23 :: batch_loss = 1.893481731414795, batch_acc = 0.25, batch_res = 0.7382609248161316 \n",
            "> epoch 4, batch 24 :: batch_loss = 1.8945000171661377, batch_acc = 0.3203125, batch_res = 0.7331425547599792 \n",
            "> epoch 4, batch 25 :: batch_loss = 1.8261499404907227, batch_acc = 0.328125, batch_res = 0.7345839142799377 \n",
            "> epoch 4, batch 26 :: batch_loss = 1.9397306442260742, batch_acc = 0.2421875, batch_res = 0.7381924986839294 \n",
            "> epoch 4, batch 27 :: batch_loss = 1.772257924079895, batch_acc = 0.3671875, batch_res = 0.7363067269325256 \n",
            "> epoch 4, batch 28 :: batch_loss = 1.8798478841781616, batch_acc = 0.2890625, batch_res = 0.7402774095535278 \n",
            "> epoch 4, batch 29 :: batch_loss = 1.811782717704773, batch_acc = 0.4140625, batch_res = 0.7376958131790161 \n",
            "> epoch 4, batch 30 :: batch_loss = 1.8179675340652466, batch_acc = 0.4296875, batch_res = 0.7300065755844116 \n",
            "> epoch 4, batch 31 :: batch_loss = 1.7521064281463623, batch_acc = 0.3125, batch_res = 0.7363170385360718 \n",
            "> epoch 4, batch 32 :: batch_loss = 1.8561818599700928, batch_acc = 0.2890625, batch_res = 0.7425931096076965 \n",
            "> epoch 4, batch 33 :: batch_loss = 1.846306562423706, batch_acc = 0.328125, batch_res = 0.7363778948783875 \n",
            "> epoch 4, batch 34 :: batch_loss = 1.727015495300293, batch_acc = 0.375, batch_res = 0.7391724586486816 \n",
            "> epoch 4, batch 35 :: batch_loss = 1.8736850023269653, batch_acc = 0.3125, batch_res = 0.7409942746162415 \n",
            "> epoch 4, batch 36 :: batch_loss = 1.8291836977005005, batch_acc = 0.34375, batch_res = 0.7343466281890869 \n",
            "> epoch 4, batch 37 :: batch_loss = 1.7775588035583496, batch_acc = 0.3515625, batch_res = 0.740138053894043 \n",
            "> epoch 4, batch 38 :: batch_loss = 1.7978296279907227, batch_acc = 0.34375, batch_res = 0.739305317401886 \n",
            "> epoch 4, batch 39 :: batch_loss = 1.811102032661438, batch_acc = 0.3515625, batch_res = 0.738118588924408 \n",
            "> epoch 4, batch 40 :: batch_loss = 1.8608551025390625, batch_acc = 0.125, batch_res = 0.7362789511680603 \n",
            "\tTRAIN epoch = 4 / loss = 1.8676916360855103 / acc = 0.3076000213623047 / res = 0.7377144694328308\n",
            "\tVAL epoch = 4 / loss = 4.1437225341796875 / acc = 0.08400000631809235 / res = 1651754532864.0\n",
            "> epoch 5, batch 1 :: batch_loss = 1.7802728414535522, batch_acc = 0.296875, batch_res = 0.7371187210083008 \n",
            "> epoch 5, batch 2 :: batch_loss = 1.823028802871704, batch_acc = 0.34375, batch_res = 0.7403053045272827 \n",
            "> epoch 5, batch 3 :: batch_loss = 1.862753987312317, batch_acc = 0.3359375, batch_res = 0.7347501516342163 \n",
            "> epoch 5, batch 4 :: batch_loss = 1.8491014242172241, batch_acc = 0.296875, batch_res = 0.7375126481056213 \n",
            "> epoch 5, batch 5 :: batch_loss = 1.789117693901062, batch_acc = 0.3515625, batch_res = 0.7320353984832764 \n",
            "> epoch 5, batch 6 :: batch_loss = 1.877526879310608, batch_acc = 0.296875, batch_res = 0.7352067828178406 \n",
            "> epoch 5, batch 7 :: batch_loss = 1.71180260181427, batch_acc = 0.3359375, batch_res = 0.7345286011695862 \n",
            "> epoch 5, batch 8 :: batch_loss = 1.8648521900177002, batch_acc = 0.2734375, batch_res = 0.7364462614059448 \n",
            "> epoch 5, batch 9 :: batch_loss = 1.692242980003357, batch_acc = 0.40625, batch_res = 0.7444710731506348 \n",
            "> epoch 5, batch 10 :: batch_loss = 1.8497990369796753, batch_acc = 0.40625, batch_res = 0.7340030074119568 \n",
            "> epoch 5, batch 11 :: batch_loss = 1.637987732887268, batch_acc = 0.421875, batch_res = 0.734083354473114 \n",
            "> epoch 5, batch 12 :: batch_loss = 1.902775764465332, batch_acc = 0.3359375, batch_res = 0.741533637046814 \n",
            "> epoch 5, batch 13 :: batch_loss = 1.7142575979232788, batch_acc = 0.3671875, batch_res = 0.7421947121620178 \n",
            "> epoch 5, batch 14 :: batch_loss = 1.7328550815582275, batch_acc = 0.359375, batch_res = 0.7388143539428711 \n",
            "> epoch 5, batch 15 :: batch_loss = 1.9359811544418335, batch_acc = 0.3125, batch_res = 0.7329581379890442 \n",
            "> epoch 5, batch 16 :: batch_loss = 1.77651047706604, batch_acc = 0.3359375, batch_res = 0.7318834066390991 \n",
            "> epoch 5, batch 17 :: batch_loss = 1.8565521240234375, batch_acc = 0.3359375, batch_res = 0.7387232184410095 \n",
            "> epoch 5, batch 18 :: batch_loss = 1.7298526763916016, batch_acc = 0.4140625, batch_res = 0.7379862070083618 \n",
            "> epoch 5, batch 19 :: batch_loss = 1.7414979934692383, batch_acc = 0.40625, batch_res = 0.7338541150093079 \n",
            "> epoch 5, batch 20 :: batch_loss = 1.734123706817627, batch_acc = 0.3515625, batch_res = 0.7369740605354309 \n",
            "> epoch 5, batch 21 :: batch_loss = 1.746652364730835, batch_acc = 0.3359375, batch_res = 0.7409241199493408 \n",
            "> epoch 5, batch 22 :: batch_loss = 1.638022780418396, batch_acc = 0.4296875, batch_res = 0.7340518832206726 \n",
            "> epoch 5, batch 23 :: batch_loss = 1.7251317501068115, batch_acc = 0.421875, batch_res = 0.7454074025154114 \n",
            "> epoch 5, batch 24 :: batch_loss = 1.6727207899093628, batch_acc = 0.375, batch_res = 0.7412945628166199 \n",
            "> epoch 5, batch 25 :: batch_loss = 1.7352683544158936, batch_acc = 0.3359375, batch_res = 0.7466042637825012 \n",
            "> epoch 5, batch 26 :: batch_loss = 1.7963762283325195, batch_acc = 0.3515625, batch_res = 0.7348114848136902 \n",
            "> epoch 5, batch 27 :: batch_loss = 1.792341947555542, batch_acc = 0.3828125, batch_res = 0.7408497333526611 \n",
            "> epoch 5, batch 28 :: batch_loss = 1.668811559677124, batch_acc = 0.453125, batch_res = 0.733552098274231 \n",
            "> epoch 5, batch 29 :: batch_loss = 1.6945829391479492, batch_acc = 0.421875, batch_res = 0.7464807033538818 \n",
            "> epoch 5, batch 30 :: batch_loss = 1.7259836196899414, batch_acc = 0.390625, batch_res = 0.7372351288795471 \n",
            "> epoch 5, batch 31 :: batch_loss = 1.788954734802246, batch_acc = 0.359375, batch_res = 0.7343985438346863 \n",
            "> epoch 5, batch 32 :: batch_loss = 1.742952823638916, batch_acc = 0.3203125, batch_res = 0.7357346415519714 \n",
            "> epoch 5, batch 33 :: batch_loss = 1.650285243988037, batch_acc = 0.40625, batch_res = 0.734075665473938 \n",
            "> epoch 5, batch 34 :: batch_loss = 1.7506401538848877, batch_acc = 0.3125, batch_res = 0.741212785243988 \n",
            "> epoch 5, batch 35 :: batch_loss = 1.660221815109253, batch_acc = 0.3359375, batch_res = 0.7354278564453125 \n",
            "> epoch 5, batch 36 :: batch_loss = 1.7857985496520996, batch_acc = 0.3515625, batch_res = 0.7379946112632751 \n",
            "> epoch 5, batch 37 :: batch_loss = 1.6385592222213745, batch_acc = 0.4296875, batch_res = 0.7347843050956726 \n",
            "> epoch 5, batch 38 :: batch_loss = 1.638970136642456, batch_acc = 0.421875, batch_res = 0.7417376041412354 \n",
            "> epoch 5, batch 39 :: batch_loss = 1.8069676160812378, batch_acc = 0.328125, batch_res = 0.7396597266197205 \n",
            "> epoch 5, batch 40 :: batch_loss = 1.4381656646728516, batch_acc = 0.25, batch_res = 0.7250210642814636 \n",
            "\tTRAIN epoch = 5 / loss = 1.7564677000045776 / acc = 0.36260002851486206 / res = 0.737713634967804\n",
            "\tVAL epoch = 5 / loss = 5.016129016876221 / acc = 0.08400000631809235 / res = 1651754532864.0\n",
            "> epoch 6, batch 1 :: batch_loss = 1.7238448858261108, batch_acc = 0.4453125, batch_res = 0.736929714679718 \n",
            "> epoch 6, batch 2 :: batch_loss = 1.604831337928772, batch_acc = 0.34375, batch_res = 0.7401124835014343 \n",
            "> epoch 6, batch 3 :: batch_loss = 1.6659293174743652, batch_acc = 0.3046875, batch_res = 0.7411319017410278 \n",
            "> epoch 6, batch 4 :: batch_loss = 1.7091789245605469, batch_acc = 0.3671875, batch_res = 0.733731746673584 \n",
            "> epoch 6, batch 5 :: batch_loss = 1.7462105751037598, batch_acc = 0.375, batch_res = 0.7408342957496643 \n",
            "> epoch 6, batch 6 :: batch_loss = 1.7293450832366943, batch_acc = 0.3515625, batch_res = 0.7360444068908691 \n",
            "> epoch 6, batch 7 :: batch_loss = 1.7102017402648926, batch_acc = 0.3515625, batch_res = 0.7374898195266724 \n",
            "> epoch 6, batch 8 :: batch_loss = 1.6824257373809814, batch_acc = 0.3515625, batch_res = 0.734737753868103 \n",
            "> epoch 6, batch 9 :: batch_loss = 1.6924396753311157, batch_acc = 0.3515625, batch_res = 0.7351019978523254 \n",
            "> epoch 6, batch 10 :: batch_loss = 1.6765811443328857, batch_acc = 0.3515625, batch_res = 0.7379721999168396 \n",
            "> epoch 6, batch 11 :: batch_loss = 1.6532020568847656, batch_acc = 0.328125, batch_res = 0.7347114086151123 \n",
            "> epoch 6, batch 12 :: batch_loss = 1.671113133430481, batch_acc = 0.34375, batch_res = 0.7377101182937622 \n",
            "> epoch 6, batch 13 :: batch_loss = 1.6115503311157227, batch_acc = 0.421875, batch_res = 0.7404255270957947 \n",
            "> epoch 6, batch 14 :: batch_loss = 1.700939655303955, batch_acc = 0.3828125, batch_res = 0.7349709868431091 \n",
            "> epoch 6, batch 15 :: batch_loss = 1.7154810428619385, batch_acc = 0.3984375, batch_res = 0.7396756410598755 \n",
            "> epoch 6, batch 16 :: batch_loss = 1.6478092670440674, batch_acc = 0.390625, batch_res = 0.7382886409759521 \n",
            "> epoch 6, batch 17 :: batch_loss = 1.7523455619812012, batch_acc = 0.390625, batch_res = 0.7408925890922546 \n",
            "> epoch 6, batch 18 :: batch_loss = 1.6091423034667969, batch_acc = 0.46875, batch_res = 0.7404192090034485 \n",
            "> epoch 6, batch 19 :: batch_loss = 1.541477084159851, batch_acc = 0.453125, batch_res = 0.7327317595481873 \n",
            "> epoch 6, batch 20 :: batch_loss = 1.7047481536865234, batch_acc = 0.3125, batch_res = 0.7354789972305298 \n",
            "> epoch 6, batch 21 :: batch_loss = 1.736443042755127, batch_acc = 0.3203125, batch_res = 0.7369968295097351 \n",
            "> epoch 6, batch 22 :: batch_loss = 1.6502656936645508, batch_acc = 0.40625, batch_res = 0.7380446195602417 \n",
            "> epoch 6, batch 23 :: batch_loss = 1.6924192905426025, batch_acc = 0.46875, batch_res = 0.7360061407089233 \n",
            "> epoch 6, batch 24 :: batch_loss = 1.678023338317871, batch_acc = 0.4921875, batch_res = 0.7420217394828796 \n",
            "> epoch 6, batch 25 :: batch_loss = 1.755755066871643, batch_acc = 0.3359375, batch_res = 0.735389769077301 \n",
            "> epoch 6, batch 26 :: batch_loss = 1.6131150722503662, batch_acc = 0.453125, batch_res = 0.7339921593666077 \n",
            "> epoch 6, batch 27 :: batch_loss = 1.7051262855529785, batch_acc = 0.375, batch_res = 0.7375081181526184 \n",
            "> epoch 6, batch 28 :: batch_loss = 1.711029291152954, batch_acc = 0.2890625, batch_res = 0.7395134568214417 \n",
            "> epoch 6, batch 29 :: batch_loss = 1.77812659740448, batch_acc = 0.3125, batch_res = 0.7360484004020691 \n",
            "> epoch 6, batch 30 :: batch_loss = 1.5493502616882324, batch_acc = 0.4375, batch_res = 0.7339240312576294 \n",
            "> epoch 6, batch 31 :: batch_loss = 1.6050655841827393, batch_acc = 0.4296875, batch_res = 0.7376709580421448 \n",
            "> epoch 6, batch 32 :: batch_loss = 1.6823934316635132, batch_acc = 0.4140625, batch_res = 0.7441385984420776 \n",
            "> epoch 6, batch 33 :: batch_loss = 1.7004467248916626, batch_acc = 0.375, batch_res = 0.7431227564811707 \n",
            "> epoch 6, batch 34 :: batch_loss = 1.581847071647644, batch_acc = 0.4453125, batch_res = 0.7371237874031067 \n",
            "> epoch 6, batch 35 :: batch_loss = 1.6506545543670654, batch_acc = 0.3828125, batch_res = 0.7360967993736267 \n",
            "> epoch 6, batch 36 :: batch_loss = 1.6194171905517578, batch_acc = 0.5, batch_res = 0.7390990257263184 \n",
            "> epoch 6, batch 37 :: batch_loss = 1.6283559799194336, batch_acc = 0.515625, batch_res = 0.7348666787147522 \n",
            "> epoch 6, batch 38 :: batch_loss = 1.7604448795318604, batch_acc = 0.34375, batch_res = 0.7389629483222961 \n",
            "> epoch 6, batch 39 :: batch_loss = 1.6589109897613525, batch_acc = 0.34375, batch_res = 0.7392694354057312 \n",
            "> epoch 6, batch 40 :: batch_loss = 1.8337392807006836, batch_acc = 0.25, batch_res = 0.7647050619125366 \n",
            "\tTRAIN epoch = 6 / loss = 1.6747674942016602 / acc = 0.38760000467300415 / res = 0.7377147078514099\n",
            "\tVAL epoch = 6 / loss = 5.367515563964844 / acc = 0.08400000631809235 / res = 1651754532864.0\n",
            "> epoch 7, batch 1 :: batch_loss = 1.597292184829712, batch_acc = 0.4296875, batch_res = 0.733018696308136 \n",
            "> epoch 7, batch 2 :: batch_loss = 1.6128828525543213, batch_acc = 0.4765625, batch_res = 0.7381977438926697 \n",
            "> epoch 7, batch 3 :: batch_loss = 1.549592137336731, batch_acc = 0.53125, batch_res = 0.7294057607650757 \n",
            "> epoch 7, batch 4 :: batch_loss = 1.7428021430969238, batch_acc = 0.2890625, batch_res = 0.7346951961517334 \n",
            "> epoch 7, batch 5 :: batch_loss = 1.609847068786621, batch_acc = 0.3671875, batch_res = 0.7356453537940979 \n",
            "> epoch 7, batch 6 :: batch_loss = 1.6205363273620605, batch_acc = 0.4453125, batch_res = 0.7447952032089233 \n",
            "> epoch 7, batch 7 :: batch_loss = 1.6027557849884033, batch_acc = 0.421875, batch_res = 0.7341888546943665 \n",
            "> epoch 7, batch 8 :: batch_loss = 1.6070442199707031, batch_acc = 0.40625, batch_res = 0.7373499870300293 \n",
            "> epoch 7, batch 9 :: batch_loss = 1.7006316184997559, batch_acc = 0.4140625, batch_res = 0.7398878931999207 \n",
            "> epoch 7, batch 10 :: batch_loss = 1.6639788150787354, batch_acc = 0.4140625, batch_res = 0.7403723001480103 \n",
            "> epoch 7, batch 11 :: batch_loss = 1.528895616531372, batch_acc = 0.3671875, batch_res = 0.7393369078636169 \n",
            "> epoch 7, batch 12 :: batch_loss = 1.5564613342285156, batch_acc = 0.546875, batch_res = 0.7370418906211853 \n",
            "> epoch 7, batch 13 :: batch_loss = 1.443854808807373, batch_acc = 0.46875, batch_res = 0.7346539497375488 \n",
            "> epoch 7, batch 14 :: batch_loss = 1.5980074405670166, batch_acc = 0.3984375, batch_res = 0.7384454607963562 \n",
            "> epoch 7, batch 15 :: batch_loss = 1.6579406261444092, batch_acc = 0.3671875, batch_res = 0.7354668378829956 \n",
            "> epoch 7, batch 16 :: batch_loss = 1.671982765197754, batch_acc = 0.328125, batch_res = 0.7418805956840515 \n",
            "> epoch 7, batch 17 :: batch_loss = 1.5612248182296753, batch_acc = 0.46875, batch_res = 0.7357749342918396 \n",
            "> epoch 7, batch 18 :: batch_loss = 1.4789726734161377, batch_acc = 0.53125, batch_res = 0.7358466982841492 \n",
            "> epoch 7, batch 19 :: batch_loss = 1.6871845722198486, batch_acc = 0.3515625, batch_res = 0.7395492196083069 \n",
            "> epoch 7, batch 20 :: batch_loss = 1.4971415996551514, batch_acc = 0.4375, batch_res = 0.7410446405410767 \n",
            "> epoch 7, batch 21 :: batch_loss = 1.6693682670593262, batch_acc = 0.3984375, batch_res = 0.7389035820960999 \n",
            "> epoch 7, batch 22 :: batch_loss = 1.575470209121704, batch_acc = 0.421875, batch_res = 0.7394734621047974 \n",
            "> epoch 7, batch 23 :: batch_loss = 1.5160748958587646, batch_acc = 0.5390625, batch_res = 0.7410194277763367 \n",
            "> epoch 7, batch 24 :: batch_loss = 1.5539929866790771, batch_acc = 0.4609375, batch_res = 0.7396157383918762 \n",
            "> epoch 7, batch 25 :: batch_loss = 1.5889861583709717, batch_acc = 0.375, batch_res = 0.7436102628707886 \n",
            "> epoch 7, batch 26 :: batch_loss = 1.5631014108657837, batch_acc = 0.4140625, batch_res = 0.7351998686790466 \n",
            "> epoch 7, batch 27 :: batch_loss = 1.53141188621521, batch_acc = 0.4609375, batch_res = 0.7376620173454285 \n",
            "> epoch 7, batch 28 :: batch_loss = 1.6538469791412354, batch_acc = 0.4140625, batch_res = 0.7364886403083801 \n",
            "> epoch 7, batch 29 :: batch_loss = 1.6836519241333008, batch_acc = 0.4140625, batch_res = 0.7430112957954407 \n",
            "> epoch 7, batch 30 :: batch_loss = 1.4985650777816772, batch_acc = 0.4609375, batch_res = 0.7361181974411011 \n",
            "> epoch 7, batch 31 :: batch_loss = 1.6087095737457275, batch_acc = 0.4296875, batch_res = 0.7377074956893921 \n",
            "> epoch 7, batch 32 :: batch_loss = 1.5308408737182617, batch_acc = 0.4453125, batch_res = 0.738537609577179 \n",
            "> epoch 7, batch 33 :: batch_loss = 1.6161173582077026, batch_acc = 0.359375, batch_res = 0.7368956804275513 \n",
            "> epoch 7, batch 34 :: batch_loss = 1.48654043674469, batch_acc = 0.4609375, batch_res = 0.7349305748939514 \n",
            "> epoch 7, batch 35 :: batch_loss = 1.6225838661193848, batch_acc = 0.46875, batch_res = 0.7364287376403809 \n",
            "> epoch 7, batch 36 :: batch_loss = 1.487501621246338, batch_acc = 0.484375, batch_res = 0.7373961806297302 \n",
            "> epoch 7, batch 37 :: batch_loss = 1.567448377609253, batch_acc = 0.46875, batch_res = 0.7360082268714905 \n",
            "> epoch 7, batch 38 :: batch_loss = 1.578038215637207, batch_acc = 0.4140625, batch_res = 0.7363316416740417 \n",
            "> epoch 7, batch 39 :: batch_loss = 1.4476948976516724, batch_acc = 0.546875, batch_res = 0.7381237745285034 \n",
            "> epoch 7, batch 40 :: batch_loss = 1.2708796262741089, batch_acc = 0.375, batch_res = 0.751387357711792 \n",
            "\tTRAIN epoch = 7 / loss = 1.5833191871643066 / acc = 0.43320003151893616 / res = 0.7377157807350159\n",
            "\tVAL epoch = 7 / loss = 5.953011989593506 / acc = 0.10000000149011612 / res = 1651754532864.0\n",
            "> epoch 8, batch 1 :: batch_loss = 1.4763038158416748, batch_acc = 0.5390625, batch_res = 0.7380009889602661 \n"
          ]
        }
      ]
    }
  ]
}