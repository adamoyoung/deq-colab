{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deq-colab.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install dependencies\n",
        "!pip install flax optax"
      ],
      "metadata": {
        "id": "si6WmYzuEMsO",
        "outputId": "83985871-4d56-4508-a4cf-8c7a7f185b3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.7/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from flax) (1.21.5)\n",
            "Requirement already satisfied: jax>=0.3 in /usr/local/lib/python3.7/dist-packages (from flax) (0.3.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from flax) (3.2.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.7/dist-packages (from flax) (1.0.3)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (3.10.0.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax>=0.3->flax) (1.0.0)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.7/dist-packages (from optax) (0.3.2+cuda11.cudnn805)\n",
            "Requirement already satisfied: chex>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from optax) (0.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax>=0.3->flax) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.11.2)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.7/dist-packages (from chex>=0.0.4->optax) (0.1.6)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from jaxlib>=0.1.37->optax) (2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (1.4.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->flax) (3.0.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "Ukan3mN8mwVX"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "import jax.lax as lax\n",
        "from jax import random, jit\n",
        "\n",
        "import optax\n",
        "\n",
        "import flax\n",
        "from flax import linen as nn\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "import torchvision\n",
        "\n",
        "import numpy as np  # TO DO remove np's -> jnp\n",
        "import contextlib\n",
        "\n",
        "from typing import Tuple, Union, List, OrderedDict, Callable, Any\n",
        "from dataclasses import field\n",
        "\n",
        "# jaxopt has already implicit differentiation!!\n",
        "import time\n",
        "\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function for local random seeding\n",
        "@contextlib.contextmanager\n",
        "def np_temp_seed(seed):\n",
        "\tstate = np.random.get_state()\n",
        "\tnp.random.seed(seed)\n",
        "\ttry:\n",
        "\t\tyield\n",
        "\tfinally:\n",
        "\t\tnp.random.set_state(state)"
      ],
      "metadata": {
        "id": "aIIhg_O_xVGC"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DEQ idea & finding stationary points with root finder, maybe root finder demo on small example (but that's close to copying from last year so maybe smth different?)"
      ],
      "metadata": {
        "id": "lEX0Pf1IGH2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _safe_norm_jax(v):\n",
        "    if not jnp.all(jnp.isfinite(v)):\n",
        "        return jnp.inf\n",
        "    return jnp.linalg.norm(v)\n",
        "\n",
        "def scalar_search_armijo_jax(phi, phi0, derphi0, c1=1e-4, alpha0=1, amin=0):\n",
        "    ite = 0\n",
        "    phi_a0 = phi(alpha0)    # First do an update with step size 1\n",
        "    if phi_a0 <= phi0 + c1*alpha0*derphi0:\n",
        "        return alpha0, phi_a0, ite\n",
        "\n",
        "    # Otherwise, compute the minimizer of a quadratic interpolant\n",
        "    alpha1 = -(derphi0) * alpha0**2 / 2.0 / (phi_a0 - phi0 - derphi0 * alpha0)\n",
        "    phi_a1 = phi(alpha1)\n",
        "\n",
        "    # Otherwise loop with cubic interpolation until we find an alpha which\n",
        "    # satisfies the first Wolfe condition (since we are backtracking, we will\n",
        "    # assume that the value of alpha is not too small and satisfies the second\n",
        "    # condition.\n",
        "    while alpha1 > amin:       # we are assuming alpha>0 is a descent direction\n",
        "        factor = alpha0**2 * alpha1**2 * (alpha1-alpha0)\n",
        "        a = alpha0**2 * (phi_a1 - phi0 - derphi0*alpha1) - \\\n",
        "            alpha1**2 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        a = a / factor\n",
        "        b = -alpha0**3 * (phi_a1 - phi0 - derphi0*alpha1) + \\\n",
        "            alpha1**3 * (phi_a0 - phi0 - derphi0*alpha0)\n",
        "        b = b / factor\n",
        "\n",
        "        alpha2 = (-b + jnp.sqrt(jnp.abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n",
        "        phi_a2 = phi(alpha2)\n",
        "        ite += 1\n",
        "\n",
        "        if (phi_a2 <= phi0 + c1*alpha2*derphi0):\n",
        "            return alpha2, phi_a2, ite\n",
        "\n",
        "        if (alpha1 - alpha2) > alpha1 / 2.0 or (1 - alpha2/alpha1) < 0.96:\n",
        "            alpha2 = alpha1 / 2.0\n",
        "\n",
        "        alpha0 = alpha1\n",
        "        alpha1 = alpha2\n",
        "        phi_a0 = phi_a1\n",
        "        phi_a1 = phi_a2\n",
        "\n",
        "    # Failed to find a suitable step length\n",
        "    return None, phi_a1, ite\n",
        "\n",
        "\n",
        "def line_search_jax(update, x0, g0, g, nstep=0, on=True):\n",
        "    \"\"\"\n",
        "    `update` is the propsoed direction of update.\n",
        "\n",
        "    Code adapted from scipy.\n",
        "    \"\"\"\n",
        "    tmp_s = [0]\n",
        "    tmp_g0 = [g0]\n",
        "    tmp_phi = [jnp.linalg.norm(g0)**2]\n",
        "    s_norm = jnp.linalg.norm(x0) / jnp.linalg.norm(update)\n",
        "\n",
        "    def phi(s, store=True):\n",
        "        if s == tmp_s[0]:\n",
        "            return tmp_phi[0]    # If the step size is so small... just return something\n",
        "        x_est = x0 + s * update\n",
        "        g0_new = g(x_est)\n",
        "        phi_new = _safe_norm_jax(g0_new)**2\n",
        "        if store:\n",
        "            tmp_s[0] = s\n",
        "            tmp_g0[0] = g0_new\n",
        "            tmp_phi[0] = phi_new\n",
        "        return phi_new\n",
        "    \n",
        "    if on:\n",
        "        s, phi1, ite = scalar_search_armijo_jax(phi, tmp_phi[0], -tmp_phi[0], amin=1e-2)\n",
        "    if (not on) or s is None:\n",
        "        s = 1.0\n",
        "        ite = 0\n",
        "\n",
        "    x_est = x0 + s * update\n",
        "    if s == tmp_s[0]:\n",
        "        g0_new = tmp_g0[0]\n",
        "    else:\n",
        "        g0_new = g(x_est)\n",
        "    return x_est, g0_new, x_est - x0, g0_new - g0, ite\n",
        "\n"
      ],
      "metadata": {
        "id": "F9GgbTf2Vbt_"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rmatvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute x^T(-I + UV^T)\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    xTU = jnp.einsum('bij, bijd -> bd', x, part_Us)   # (N, threshold)\n",
        "    return -x + jnp.einsum('bd, bdij -> bij', xTU, part_VTs)    # (N, 2d, L'), but should really be (N, 1, (2d*L'))\n",
        "\n",
        "def matvec_jax(part_Us, part_VTs, x):\n",
        "    # Compute (-I + UV^T)x\n",
        "    # x: (N, 2d, L')\n",
        "    # part_Us: (N, 2d, L', threshold)\n",
        "    # part_VTs: (N, threshold, 2d, L')\n",
        "    if jnp.size(part_Us) == 0:\n",
        "        return -x\n",
        "    VTx = jnp.einsum('bdij, bij -> bd', part_VTs, x)  # (N, threshold)\n",
        "    return -x + jnp.einsum('bijd, bd -> bij', part_Us, VTx)     # (N, 2d, L'), but should really be (N, (2d*L'), 1)\n"
      ],
      "metadata": {
        "id": "DpQtBBHPV36I"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def broyden_jax(f, z0, x0, threshold, eps=1e-3, stop_mode=\"rel\", result_dict=False, ls=False):\n",
        "    bsz, total_hsize = z0.shape\n",
        "    orig_shape = (bsz,total_hsize)\n",
        "    seq_len = 1\n",
        "    new_shape = (bsz,total_hsize,seq_len)\n",
        "    z0 = z0.reshape(*new_shape)\n",
        "    def g(_z):\n",
        "        # here it is safe to use x out of scope\n",
        "        return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(*new_shape)\n",
        "    dev = z0.device()\n",
        "    alternative_mode = 'rel' if stop_mode == 'abs' else 'abs'\n",
        "    \n",
        "    z_est = z0           # (bsz, 2d, L')\n",
        "    gz = g(z_est)        # (bsz, 2d, L')\n",
        "    nstep = 0\n",
        "    tnstep = 0\n",
        "    \n",
        "    # For fast calculation of inv_jacobian (approximately)\n",
        "    Us = jax.device_put(jnp.zeros((bsz, total_hsize, seq_len, threshold)),dev)     # One can also use an L-BFGS scheme to further reduce memory\n",
        "    VTs = jax.device_put(jnp.zeros((bsz, threshold, total_hsize, seq_len)),dev)\n",
        "    update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)      # Formally should be -torch.matmul(inv_jacobian (-I), gx)\n",
        "    prot_break = False\n",
        "    \n",
        "    # To be used in protective breaks\n",
        "    protect_thres = (1e6 if stop_mode == \"abs\" else 1e3) * seq_len\n",
        "    new_objective = 1e8\n",
        "\n",
        "    trace_dict = {'abs': [],\n",
        "                  'rel': []}\n",
        "    lowest_dict = {'abs': 1e8,\n",
        "                   'rel': 1e8}\n",
        "    lowest_step_dict = {'abs': 0,\n",
        "                        'rel': 0}\n",
        "    nstep, lowest_zest, lowest_gz = 0, z_est, gz\n",
        "\n",
        "    while nstep < threshold:\n",
        "        z_est, gz, delta_z, delta_gz, ite = line_search_jax(update, z_est, gz, g, nstep=nstep, on=ls)\n",
        "        nstep += 1\n",
        "        tnstep += (ite+1)\n",
        "        abs_diff = jnp.linalg.norm(gz)\n",
        "        rel_diff = abs_diff / (jnp.linalg.norm(gz + z_est) + 1e-9)\n",
        "        diff_dict = {'abs': abs_diff,\n",
        "                     'rel': rel_diff}\n",
        "        trace_dict['abs'].append(abs_diff)\n",
        "        trace_dict['rel'].append(rel_diff)\n",
        "        for mode in ['rel', 'abs']:\n",
        "            if diff_dict[mode] < lowest_dict[mode]:\n",
        "                if mode == stop_mode: \n",
        "                    lowest_zest, lowest_gz = jnp.copy(z_est), jnp.copy(gz)\n",
        "                lowest_dict[mode] = diff_dict[mode]\n",
        "                lowest_step_dict[mode] = nstep\n",
        "\n",
        "        new_objective = diff_dict[stop_mode]\n",
        "        if new_objective < eps: break\n",
        "        if new_objective < 3*eps and nstep > 30 and np.max(trace_dict[stop_mode][-30:]) / np.min(trace_dict[stop_mode][-30:]) < 1.3:\n",
        "            # if there's hardly been any progress in the last 30 steps\n",
        "            break\n",
        "        if new_objective > trace_dict[stop_mode][0] * protect_thres:\n",
        "            prot_break = True\n",
        "            break\n",
        "\n",
        "        part_Us, part_VTs = Us[:,:,:,:nstep-1], VTs[:,:nstep-1]\n",
        "        vT = rmatvec_jax(part_Us, part_VTs, delta_z)\n",
        "        u = (delta_z - matvec_jax(part_Us, part_VTs, delta_gz)) / jnp.einsum('bij, bij -> b', vT, delta_gz)[:,None,None]\n",
        "        vT = jnp.nan_to_num(vT,nan=0.)\n",
        "        u = jnp.nan_to_num(u,nan=0.)\n",
        "        VTs = VTs.at[:,nstep-1].set(vT)\n",
        "        Us = Us.at[:,:,:,nstep-1].set(u)\n",
        "        update = -matvec_jax(Us[:,:,:,:nstep], VTs[:,:nstep], gz)\n",
        "\n",
        "    # Fill everything up to the threshold length\n",
        "    for _ in range(threshold+1-len(trace_dict[stop_mode])):\n",
        "        trace_dict[stop_mode].append(lowest_dict[stop_mode])\n",
        "        trace_dict[alternative_mode].append(lowest_dict[alternative_mode])\n",
        "\n",
        "    lowest_zest = lowest_zest.reshape(*orig_shape)\n",
        "    # print(\"broyden\",jnp.linalg.norm(z_est),jnp.linalg.norm(gz))\n",
        "\n",
        "    if result_dict:\n",
        "        return {\"result\": lowest_zest,\n",
        "                \"lowest\": lowest_dict[stop_mode],\n",
        "                \"nstep\": lowest_step_dict[stop_mode],\n",
        "                \"prot_break\": prot_break,\n",
        "                \"abs_trace\": trace_dict['abs'],\n",
        "                \"rel_trace\": trace_dict['rel'],\n",
        "                \"eps\": eps,\n",
        "                \"threshold\": threshold}\n",
        "    else:\n",
        "        return lowest_zest\n",
        "\n",
        "\n",
        "def newton_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    # note: f might ignore x0 (i.e. with backward pass)\n",
        "    orig_shape = z0.shape\n",
        "    def g(_z):\n",
        "      # this reshaping is to enable solving with Jacobian\n",
        "      return (f(_z.reshape(*orig_shape),x0)-_z.reshape(*orig_shape)).reshape(-1)\n",
        "    jac_g = jax.jacfwd(g)\n",
        "    z = z0.reshape(-1)\n",
        "    gz = g(z)\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    nstep = 0\n",
        "\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      # solve system\n",
        "      jgz = jac_g(z)\n",
        "      # print(\"gz\",gz.shape,jnp.linalg.norm(gz))\n",
        "      # print(\"jgz\",jgz.shape,jnp.linalg.norm(jgz))\n",
        "      delta_z = jnp.linalg.solve(jgz,-gz)\n",
        "      # print(\"delta_z\",delta_z.shape,jnp.linalg.norm(delta_z))\n",
        "      z = z + delta_z\n",
        "      # need to compute gx here to decide whether to stop\n",
        "      gz = g(z)\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      nstep += 1\n",
        "\n",
        "    z = z.reshape(*orig_shape).astype(jnp.float32)\n",
        "\n",
        "    # assert False\n",
        "\n",
        "    return z\n",
        "\n",
        "def direct_jax(f, z0, x0, threshold, eps=1e-3):\n",
        "    # TODO replace with jax while\n",
        "\n",
        "    nstep = 0\n",
        "    z_old = z0\n",
        "    z_new = f(z0,x0)\n",
        "    gz = z_new-z_old\n",
        "    gz_norm = jnp.linalg.norm(gz)\n",
        "    min_gz_norm, min_z = gz_norm, z_new\n",
        "    while nstep < threshold and gz_norm > eps:\n",
        "      z_old = z_new\n",
        "      z_new = f(z_old,x0)\n",
        "      gz = z_new-z_old\n",
        "      gz_norm = jnp.linalg.norm(gz)\n",
        "      if gz_norm < min_gz_norm:\n",
        "        min_gz_norm, min_z = gz_norm, z_new\n",
        "      nstep += 1\n",
        "    # print(\"min_gz_norm\",min_gz_norm,\"nstep\",nstep)\n",
        "    return min_z\n"
      ],
      "metadata": {
        "id": "JDts6pYBWEpv"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQBlock(nn.Module):\n",
        "    curr_branch: int\n",
        "    channels: List[int]\n",
        "    kernel_size: Tuple[int] = (3, 3) \n",
        "    num_groups: int = 2\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "    bias_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):  \n",
        "        self.input_dim = self.channels[self.curr_branch]\n",
        "        self.hidden_dim = 2*self.input_dim\n",
        "\n",
        "        self.conv1 = nn.Conv(features=self.hidden_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group1 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.input_dim, kernel_size=self.kernel_size,\n",
        "                             strides=(1,1))\n",
        "        self.group2 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        self.group3 = nn.GroupNorm(num_groups=self.num_groups)\n",
        "\n",
        "\n",
        "    def __call__(self, x, branch, injection):\n",
        "        # forward pass\n",
        "        h1 = self.group1(self.conv1(x))\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.conv2(h1)\n",
        "        \n",
        "        # inject original input if resolution=0 \n",
        "        if branch == 0:\n",
        "            h2 += injection\n",
        "        \n",
        "        h2 = self.group2(h2)\n",
        "        # residual\n",
        "        h2 += x\n",
        "        h3 = self.relu(h2)\n",
        "        out = self.group3(h3)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "EzcfTWz6agE_"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DownSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan  = self.channels[self.branches[1]]\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        \n",
        "        num_samples = to_res - from_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        down_block = []\n",
        "        conv_down = nn.Conv(features=self.in_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        group_down = nn.GroupNorm(num_groups=self.num_groups)\n",
        "        relu_down = nn.relu\n",
        "\n",
        "        for n in range(num_samples-1):\n",
        "            down_block += [conv_down, group_down, relu_down]\n",
        "        conv_down = nn.Conv(features=self.out_chan, kernel_size=(3,3), strides=(2,2), padding=((1,1),(1,1)), use_bias=False)\n",
        "        down_block += [conv_down, group_down]\n",
        "        self.downsample_fn = nn.Sequential(down_block)\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        out = self.downsample_fn(z_plus)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1fO-Bsnly9Jp"
      },
      "execution_count": 260,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UpSample(nn.Module):\n",
        "    channels: List[int]\n",
        "    branches: Tuple[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "\n",
        "    \n",
        "    def setup(self):\n",
        "        self.in_chan = self.channels[self.branches[0]]\n",
        "        self.out_chan = self.channels[self.branches[1]]\n",
        "        self.upsample_fn = self._upsample()\n",
        "        \n",
        "    ''' the following is from https://github.com/google/jax/issues/862 '''\n",
        "    \n",
        "    def interpolate_bilinear(self, im, rows, cols):\n",
        "        # based on http://stackoverflow.com/a/12729229\n",
        "        col_lo = np.floor(cols).astype(int)\n",
        "        col_hi = col_lo + 1\n",
        "        row_lo = np.floor(rows).astype(int)\n",
        "        row_hi = row_lo + 1\n",
        "\n",
        "        nrows, ncols = im.shape[-3:-1]\n",
        "        def cclip(cols): return np.clip(cols, 0, ncols - 1)\n",
        "        def rclip(rows): return np.clip(rows, 0, nrows - 1)\n",
        "        Ia = im[..., rclip(row_lo), cclip(col_lo), :]\n",
        "        Ib = im[..., rclip(row_hi), cclip(col_lo), :]\n",
        "        Ic = im[..., rclip(row_lo), cclip(col_hi), :]\n",
        "        Id = im[..., rclip(row_hi), cclip(col_hi), :]\n",
        "\n",
        "        wa = np.expand_dims((col_hi - cols) * (row_hi - rows), -1)\n",
        "        wb = np.expand_dims((col_hi - cols) * (rows - row_lo), -1)\n",
        "        wc = np.expand_dims((cols - col_lo) * (row_hi - rows), -1)\n",
        "        wd = np.expand_dims((cols - col_lo) * (rows - row_lo), -1)\n",
        "\n",
        "        return wa*Ia + wb*Ib + wc*Ic + wd*Id\n",
        "\n",
        "    def upsampling_wrap(self, resize_rate):\n",
        "        def upsampling_method(img):\n",
        "            nrows, ncols = img.shape[-3:-1]\n",
        "            delta = 0.5/resize_rate\n",
        "\n",
        "            rows = np.linspace(delta,nrows-delta, np.int32(resize_rate*nrows))\n",
        "            cols = np.linspace(delta,ncols-delta, np.int32(resize_rate*ncols))\n",
        "            ROWS, COLS = np.meshgrid(rows,cols,indexing='ij')\n",
        "        \n",
        "            img_resize_vec = self.interpolate_bilinear(img, ROWS.flatten(), COLS.flatten())\n",
        "            img_resize =  img_resize_vec.reshape(img.shape[:-3] + \n",
        "                                                (len(rows),len(cols)) + \n",
        "                                                img.shape[-1:])\n",
        "        \n",
        "            return img_resize\n",
        "        return upsampling_method\n",
        "    ''' end copy '''\n",
        "\n",
        "\n",
        "    def _upsample(self):\n",
        "        to_res, from_res = self.branches  # sampling from resolution from_res to to_res\n",
        "        num_samples = from_res - to_res \n",
        "        assert num_samples > 0\n",
        "\n",
        "        return nn.Sequential([nn.Conv(features=self.out_chan, kernel_size=(1, 1), use_bias=False), #kernel_init=self.kernel_init),\n",
        "                        nn.GroupNorm(num_groups=self.num_groups),\n",
        "                        self.upsampling_wrap(resize_rate=2**num_samples)])\n",
        "\n",
        "    def __call__(self, z_plus):\n",
        "        return self.upsample_fn(z_plus)"
      ],
      "metadata": {
        "id": "pApLGNTRK8OW"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cringy_reshape(in_vec, shape_list):\n",
        "    start = 0\n",
        "    out_vec = []\n",
        "    if isinstance(in_vec, list):\n",
        "        raise ValueError\n",
        "    # in_vec = jnp.array(in_vec)\n",
        "    for size in shape_list:\n",
        "        my_elems = jnp.prod(jnp.array(size[1:]))\n",
        "        end = start+my_elems\n",
        "        my_chunk = jnp.copy(in_vec[:, start:end])\n",
        "        start += my_elems\n",
        "        my_chunk = jnp.reshape(my_chunk, size)\n",
        "        out_vec.append(my_chunk)\n",
        "\n",
        "    return out_vec\n",
        "        "
      ],
      "metadata": {
        "id": "LMLCzlwHO3A0"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Maps image to initial latent representation\n",
        "    AKA the grey part in the diagram\n",
        "    \"\"\"\n",
        "\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    training: bool = True\n",
        "\n",
        "    def setup(self):\n",
        "        self.conv1 = nn.Conv(features=self.channels[0], \n",
        "                             kernel_size=(3, 3), strides=(1, 1))\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x), use_running_average=True))\n",
        "        return x"
      ],
      "metadata": {
        "id": "ozN6cMheafUj"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLSBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A tool for using the \n",
        "    \"\"\"\n",
        "    input_dim: int\n",
        "    output_dim: int\n",
        "    downsample: bool\n",
        "    expansion: int=4\n",
        "    \n",
        "    def setup(self):  \n",
        "\n",
        "        # init-substitute for flax\n",
        "        self.conv1 = nn.Conv(features=self.output_dim, kernel_size=(1,1),\n",
        "                             strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn1 = nn.BatchNorm()\n",
        "        self.relu = nn.relu\n",
        "        self.conv2 = nn.Conv(features=self.output_dim, kernel_size=(3,3), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn2 = nn.BatchNorm()\n",
        "        self.conv3 = nn.Conv(features=self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1))#, kernel_init=self.kernel_init, bias_init=self.bias_init)\n",
        "        self.bn3 = nn.BatchNorm()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.ds_conv = nn.Conv(self.output_dim*self.expansion, kernel_size=(1,1), strides=(1,1), use_bias=False)\n",
        "            self.ds_bn = nn.BatchNorm()\n",
        "\n",
        "\n",
        "    def __call__(self, x, injection=None):\n",
        "        # forward pass\n",
        "        if injection is None:\n",
        "          injection = 0\n",
        "        h1 = self.bn1(self.conv1(x), use_running_average=True)\n",
        "        h1 = self.relu(h1)\n",
        "        h2 = self.bn2(self.conv2(h1), use_running_average=True)\n",
        "        h2 = self.relu(h2)\n",
        "        h3 = self.bn3(self.conv3(h2), use_running_average=True)\n",
        "        if self.downsample:\n",
        "          x = self.ds_bn(self.ds_conv(x), use_running_average=True)\n",
        "        h3 += x\n",
        "        return nn.relu(h3)"
      ],
      "metadata": {
        "id": "NeXb7CTi-Imp"
      },
      "execution_count": 264,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "..."
      ],
      "metadata": {
        "id": "DAYfgxMOGUNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(nn.Module):\n",
        "    channels: List[int] = field(default_factory=lambda:[24, 24])\n",
        "    output_channels: List[int] = field(default_factory=lambda:[8, 16])\n",
        "    expansion: int = 4\n",
        "    final_chansize: int = 200\n",
        "    num_classes: int = 10\n",
        "\n",
        "    def make_cls_block(self, in_chan, out_chan):\n",
        "          downsample = False\n",
        "          if in_chan != out_chan * self.expansion:\n",
        "              downsample = True\n",
        "          return CLSBlock(in_chan, out_chan, downsample)\n",
        "\n",
        "    def setup(self):\n",
        "        self.num_branches = len(self.channels)\n",
        "\n",
        "        combine_modules = []\n",
        "        for i  in range(len(self.channels)):\n",
        "            output_mod = self.make_cls_block(self.channels[i], self.output_channels[i])\n",
        "            combine_modules.append(output_mod)\n",
        "        self.combine_modules = combine_modules\n",
        "\n",
        "        self.final_layer_conv = nn.Conv(self.final_chansize, kernel_size=(1,1))\n",
        "        self.final_layer_bn = nn.BatchNorm()\n",
        "\n",
        "        self.classifier = nn.Dense(self.num_classes)\n",
        "                                         \n",
        "    def __call__(self, y):\n",
        "        y_final = self.combine_modules[0](y[0])\n",
        "        for i in range(len(self.channels)-1):\n",
        "            y_final = self.combine_modules[i+1](y[i+1]) \n",
        "        y_final = self.final_layer_bn(self.final_layer_conv(y_final), use_running_average=True)\n",
        "        y_final = nn.relu(y_final)\n",
        "        y_final = nn.avg_pool(y_final, window_shape=y_final.shape[1:3])\n",
        "        y_final = jnp.reshape(y_final, (y_final.shape[0], -1))\n",
        "        y_final = self.classifier(y_final)\n",
        "        return y_final"
      ],
      "metadata": {
        "id": "REW0m6Hf1tvv"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@partial(jax.custom_vjp, nondiff_argnums=(0, 1, 2, 3,)) # nondiff are all except for weights and z/x\n",
        "def rootfind(solver_fn: Callable,\n",
        "                 f_fn: Callable,\n",
        "                 threshold: int,\n",
        "                 eps: float,\n",
        "                 weights: dict,\n",
        "                 z: jnp.ndarray,\n",
        "                 x: jnp.ndarray):\n",
        "    f_fn = partial(f_fn, weights)\n",
        "    return jax.lax.stop_gradient(solver_fn(f_fn, z, x, threshold, eps=1e-3))\n",
        "\n",
        "# Its forward call (basically just calling it)\n",
        "def _rootfind_fwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      weights: dict,\n",
        "                      z: jnp.ndarray,\n",
        "                      x: jnp.ndarray):\n",
        "    z = rootfind(solver_fn, f_fn, threshold, eps, weights, z, x)\n",
        "    # print(\"fwd residual\",jnp.linalg.norm(f_fn(weights,z,x)-z)/jnp.linalg.norm(z))\n",
        "    return z, (weights, z, x)\n",
        "\n",
        "# Its backward call (its inputs)\n",
        "def _rootfind_bwd(solver_fn: Callable,\n",
        "                      f_fn: Callable,\n",
        "                      threshold: int,\n",
        "                      eps: float,\n",
        "                      res,  \n",
        "                      grad):\n",
        "    weights, z, x = res\n",
        "    (_, vjp_fun) = jax.vjp(f_fn, weights, z, x)\n",
        "    def z_fn(z,x): # gets transpose Jac w.r.t. weights and z using vjp_fun\n",
        "        (Jw_T, Jz_T, _) = vjp_fun(z)\n",
        "        return Jz_T + grad\n",
        "    z0 = jnp.zeros_like(grad)\n",
        "    x0 = None # dummy, z_fn does not use x\n",
        "    g = solver_fn(z_fn, z0, x0, threshold, eps)\n",
        "    #print(\"bwd residual\",jnp.linalg.norm(z_fn(g,x0)-g)/jnp.linalg.norm(g))\n",
        "    return (None, g, None)\n",
        "\n",
        "rootfind.defvjp(_rootfind_fwd, _rootfind_bwd)"
      ],
      "metadata": {
        "id": "q61ohtb5HKwj"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MDEQFF(nn.Module):\n",
        "    \"\"\"\n",
        "    The f_{\\theta}(z,x) function that is repeatedly applied\n",
        "    AKA the yellow block in the diagram\n",
        "    \"\"\"\n",
        "    num_branches: int\n",
        "    channels: List[int]\n",
        "    num_groups: int\n",
        "    kernel_init = jax.nn.initializers.glorot_normal()\n",
        "   \n",
        "    def setup(self):\n",
        "\n",
        "        self.branches = self.stack_branches()\n",
        "        self.fuse_branches = self.fuse()\n",
        "        self.transform = self.transform_output()\n",
        "\n",
        "    def stack_branches(self):\n",
        "        branches = []\n",
        "        for i in range(self.num_branches):\n",
        "          branches.append(MDEQBlock(curr_branch=i, channels=self.channels))\n",
        "        return branches\n",
        "\n",
        "    def fuse(self):#, z_plus, channel_dimensions):\n",
        "        # up- and downsampling stuff\n",
        "        # z_plus: output of residual block\n",
        "        if self.num_branches == 1:\n",
        "            return None\n",
        "        \n",
        "        fuse_layers = []\n",
        "        for i in range(self.num_branches):\n",
        "            array = []\n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                    array.append(None)\n",
        "                else:\n",
        "                    if i > j:\n",
        "                        sampled = DownSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    elif i < j:\n",
        "                        sampled = UpSample(branches=(i, j), channels=self.channels, num_groups=self.num_groups)\n",
        "                    array.append(sampled)\n",
        "            fuse_layers.append(array)\n",
        "\n",
        "        return fuse_layers\n",
        "    \n",
        "    def transform_output(self):\n",
        "        transforms = []\n",
        "        for i in range(self.num_branches):\n",
        "          transforms.append(nn.Sequential([nn.Conv(features=self.channels[i], kernel_size=(1, 1),\n",
        "                                                  use_bias=False),\n",
        "                                           nn.relu,\n",
        "                                           nn.GroupNorm(num_groups=self.num_groups)]))\n",
        "        \n",
        "        return transforms\n",
        "\n",
        "    def __call__(self, z, x, shape_tuple):\n",
        "        \n",
        "        batch_size = z.shape[0]\n",
        "        z_list = cringy_reshape(z,shape_tuple)\n",
        "        x_list = cringy_reshape(x,shape_tuple)\n",
        "        # step 1: compute residual blocks\n",
        "        branch_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            branch_outputs.append(self.branches[i](z_list[i], i, x_list[i])) # z, branch, x\n",
        "        # step 2: fuse residual blocks\n",
        "        fuse_outputs = []\n",
        "        for i in range(self.num_branches):\n",
        "            intermediate_i = jnp.zeros(branch_outputs[i].shape) \n",
        "            for j in range(self.num_branches):\n",
        "                if i == j:\n",
        "                  intermediate_i += branch_outputs[j]\n",
        "                else:\n",
        "                    if self.fuse_branches[i][j] is not None:\n",
        "                        temp = self.fuse_branches[i][j](z_plus=branch_outputs[j])#, branches=(i, j))\n",
        "                        intermediate_i += temp\n",
        "                    else:\n",
        "                        raise Exception(\"Should not happen.\")\n",
        "            fuse_outputs.append(self.transform[i](intermediate_i))\n",
        "          # stick z back into into one vector\n",
        "        fuse_outputs = jnp.concatenate([fo.reshape(batch_size,-1) for fo in fuse_outputs],axis=1)\n",
        "        assert fuse_outputs.shape[1] == z.shape[1]\n",
        "        return fuse_outputs\n"
      ],
      "metadata": {
        "id": "dWkUS52WoaiC"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mdeq_inputs(x,num_branches):\n",
        "\n",
        "  batch_size = x.shape[0]\n",
        "  x_list = [x]\n",
        "  for i in range(1, num_branches):\n",
        "      bs, H, W, y = x_list[-1].shape\n",
        "      new_item = jnp.zeros((bs, H//2, W//2, y))\n",
        "      x_list.append(new_item)\n",
        "  z_list = [jnp.zeros_like(elem) for elem in x_list]\n",
        "  shape_list = [el.shape for el in z_list]\n",
        "  # make them (batched) vectors\n",
        "  x_vec = jnp.concatenate([x.reshape(batch_size,-1) for x in x_list],axis=1)\n",
        "  z_vec = jnp.concatenate([z.reshape(batch_size,-1) for z in z_list],axis=1)\n",
        "  # i'm not sure if tuple is actually important but I like it for non-mutability\n",
        "  shape_tuple = tuple(shape_list)\n",
        "  return x_vec, z_vec, shape_tuple\n",
        "\n",
        "\n",
        "def mdeq_fn(x,encoder,decoder,deqff,all_weights,solver_fn=None,mode='broyden'):\n",
        "    \n",
        "    encoder_weights = all_weights[\"encoder\"]\n",
        "    decoder_weights = all_weights[\"decoder\"]\n",
        "    deqff_weights = all_weights[\"mdeqff\"]\n",
        "    batch_size = x.shape[0]\n",
        "    # transform the input image\n",
        "    x = encoder.apply(encoder_weights,x)\n",
        "    # construct inputs (lots of padding and concatenation)\n",
        "    x, z, shape_tuple = create_mdeq_inputs(x,deqff.num_branches)\n",
        "    \n",
        "    if mode == \"implicit\":\n",
        "        # solve a fixed point equation for fwd/bwd passes\n",
        "        threshold = 7\n",
        "        eps = 1e-3\n",
        "        # the root function can only take 3 ndarrays as input\n",
        "        def deqff_root(_weights,_z,_x):\n",
        "          # note: it's safe to pass the shape_tuple here (no tracers)\n",
        "          return deqff.apply(_weights,_z,_x,shape_tuple)\n",
        "        # apply rootfinder with custom vjp\n",
        "        z = rootfind(solver_fn,deqff_root,threshold,eps,deqff_weights,z,x)\n",
        "  \n",
        "    elif mode == \"non_implicit\":\n",
        "        # use direct solver, backpropagate through the steps\n",
        "        threshold = 5\n",
        "        eps=1e-2\n",
        "        evals = 0\n",
        "        z_old = z\n",
        "        z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "        #residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "        residual = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(z_new-z_old, axis=1) / (jnp.linalg.norm(z_old, axis=1)+1e-9),axis=0))\n",
        "\n",
        "        while evals < threshold and residual > eps:\n",
        "            z_old = z_new\n",
        "            z_new = deqff.apply(deqff_weights,z_old,x,shape_tuple)\n",
        "            #residual = jnp.linalg.norm(z_new-z_old) / (jnp.linalg.norm(z_old)+1e-9)\n",
        "            residual = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(z_new-z_old, axis=1) / (jnp.linalg.norm(z_old, axis=1)+1e-9),axis=0))\n",
        "\n",
        "            evals += 1\n",
        "        z = z_new\n",
        "      \n",
        "    elif mode == \"warmup\":\n",
        "        # fixed number of function applications\n",
        "        threshold = 5 #1\n",
        "        for i in range(threshold):\n",
        "            z = deqff.apply(deqff_weights,z,x,shape_tuple)\n",
        "\n",
        "    z_list = cringy_reshape(z,shape_tuple)\n",
        "    log_probs = decoder.apply(decoder_weights,z_list)\n",
        "\n",
        "    # this evaluation is just for the purposes of calculating residual\n",
        "    # we don't want any gradient stuff\n",
        "    f_z = jax.lax.stop_gradient(deqff.apply(deqff_weights,z,x,shape_tuple))\n",
        "    residuals = jax.lax.stop_gradient(jnp.mean(jnp.linalg.norm(f_z-z, axis=1) / (jnp.linalg.norm(z, axis=1)+1e-9),axis=0))\n",
        "    \n",
        "    return log_probs, residuals"
      ],
      "metadata": {
        "id": "5hI7s8DE1kqX"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def cross_entropy_loss(logits, labels):\n",
        "    ''' \n",
        "    should be same as  optax.softmax_cross_entropy(logits, labels); \n",
        "    if getting funny results maybe remove log of logits\n",
        "    '''\n",
        "    one_hot_labels = jax.nn.one_hot(labels, num_classes=10)\n",
        "    logits = jax.nn.log_softmax(logits)\n",
        "    acc = (jnp.argmax(logits, axis=-1) == labels).mean()\n",
        "    output = -jnp.mean(jnp.sum(one_hot_labels * logits, axis=-1))\n",
        "    return output, acc"
      ],
      "metadata": {
        "id": "r8M7Sf_gmqD7"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(images,labels,encoder,decoder,deqff,all_weights,solver,mode):\n",
        "    loss, acc = 0,0\n",
        "    batch_size = 128\n",
        "    start, end = 0, 0\n",
        "    loss_vals, acc_vals, res_vals = [], [], []\n",
        "\n",
        "    while end < images.shape[0]:\n",
        "        end = min(start+batch_size, images.shape[0])\n",
        "        x_batch = images[start:end]\n",
        "        #x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "\n",
        "        y_true = labels[start:end]\n",
        "        start = end\n",
        "\n",
        "        log_probs, residual = jax.lax.stop_gradient(mdeq_fn(x_batch,encoder,decoder,deqff,all_weights,solver_fn=solver,mode=mode))\n",
        "        loss, acc = cross_entropy_loss(log_probs, y_true)\n",
        "        loss_vals.append(loss * x_batch.shape[0])\n",
        "        acc_vals.append(acc * x_batch.shape[0])\n",
        "        res_vals.append(residual * x_batch.shape[0])\n",
        "    return sum(jnp.array(loss_vals)) / images.shape[0], sum(jnp.array(acc_vals)) / images.shape[0], sum(jnp.array(res_vals)) / images.shape[0]"
      ],
      "metadata": {
        "id": "1kUlVk4z0LOV"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform(image, label, num_classes=10):\n",
        "    image = jnp.float32(image) / 255.\n",
        "    image = np.expand_dims(image, -1)\n",
        "    # image = np.tile(image, (1,1,1,24))\n",
        "    label = jnp.array(label)\n",
        "    return image, label\n",
        "\n",
        "def load_data():\n",
        "    test_ds = torchvision.datasets.MNIST(root=\"data\", train=False,download=True)\n",
        "    train_ds = torchvision.datasets.MNIST(root=\"data\", train=True,download=True)\n",
        "\n",
        "    train_images, train_labels = transform(train_ds.data[:1000], train_ds.targets[:1000])\n",
        "    test_images, test_labels = transform(test_ds.data[:100], test_ds.targets[:100])\n",
        "\n",
        "    print(f\"NUM TRAINING IMAGES:::{train_images.shape[0]}\")\n",
        "    print(f\"NUM TEST IMAGES:::{test_images.shape[0]}\")\n",
        "\n",
        "    return train_images, train_labels, test_images, test_labels\n"
      ],
      "metadata": {
        "id": "qCtrgYH-K2b8"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(mode=\"implicit\",solver=\"broyden\"):\n",
        "\n",
        "    assert mode in [\"implicit\", \"non_implicit\", \"warmup\"], \"INCORRECT MODE\"\n",
        "    assert solver in [\"direct\",\"broyden\"]\n",
        "\n",
        "    train_images, train_labels, test_images, test_labels = load_data()\n",
        "    num_images = train_images.shape[0]\n",
        "    image_size = train_images.shape[1]\n",
        "    batch_size = 128\n",
        "    assert batch_size <= train_images.shape[0]\n",
        "\n",
        "    if solver == \"broyden\":\n",
        "        solver_fn = broyden_jax\n",
        "    elif solver == \"direct\":\n",
        "        solver_fn = direct_jax\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    num_groups = 8\n",
        "    channels = [24, 24]\n",
        "    num_branches = 2\n",
        "\n",
        "    # instantiation\n",
        "    encoder = Encoder(channels=channels)\n",
        "    decoder = Classifier() # not sure about what to pass\n",
        "    mdeqff = MDEQFF(num_branches=num_branches, channels=channels, num_groups=num_groups)\n",
        "\n",
        "    # weight initialization\n",
        "    prng = jax.random.PRNGKey(0)\n",
        "    prng, _ = jax.random.split(prng, 2)\n",
        "    x_dummy = jnp.ones((batch_size, image_size, image_size, 1))\n",
        "    x_dummy_2, encoder_weights = encoder.init_with_output(prng,x_dummy)\n",
        "    x_dummy_3, z_dummy, shape_tuple = create_mdeq_inputs(x_dummy_2,num_branches)\n",
        "    z_dummy_2, mdeqff_weights = mdeqff.init_with_output(prng,z_dummy,x_dummy_3,shape_tuple)\n",
        "    z_dummy_3 = cringy_reshape(z_dummy_2,shape_tuple)\n",
        "    o_dummy, classifier_weights = decoder.init_with_output(prng,z_dummy_3)\n",
        "\n",
        "    # collect weights\n",
        "    weights = {'encoder': encoder_weights, 'mdeqff': mdeqff_weights ,'decoder': classifier_weights}\n",
        "\n",
        "    optimizer = optax.adam(learning_rate=0.001)\n",
        "    opt_state = optimizer.init(weights)\n",
        "\n",
        "    loss_fn = cross_entropy_loss\n",
        "\n",
        "    def loss(weights, x_batch, y_true, mode):\n",
        "        logits, residual = mdeq_fn(x_batch,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "        loss, acc = loss_fn(logits, y_true)\n",
        "        return loss, (acc, residual)\n",
        "\n",
        "    def step(weights, opt_state, x_batch, y_true, mode):\n",
        "        (loss_vals, (acc, residual)), grad = jax.value_and_grad(loss, has_aux=True)(weights, x_batch, y_true, mode)\n",
        "        updates, opt_state = optimizer.update(grad, opt_state, weights)\n",
        "        weights = optax.apply_updates(weights, updates)\n",
        "\n",
        "        return weights, opt_state, loss_vals, acc, residual\n",
        "\n",
        "    def generator(batch_size: int=10):\n",
        "        ''' https://optax.readthedocs.io/en/latest/meta_learning.html?highlight=generator#meta-learning '''\n",
        "        rng = jax.random.PRNGKey(0)\n",
        "\n",
        "        while True:\n",
        "            rng, k1 = jax.random.split(rng, num=2)\n",
        "            idxs = jax.random.randint(k1, shape=(batch_size,), minval=0, maxval=num_images, dtype=jnp.int32)\n",
        "            yield idxs\n",
        "\n",
        "    def list_shuffler(seed):\n",
        "        rng = jax.random.PRNGKey(seed)\n",
        "        rng, k1 = jax.random.split(rng, num=2)\n",
        "        indices = jnp.arange(0, train_images.shape[0])\n",
        "        shuffled_indices = jax.random.shuffle(k1, indices)\n",
        "        return shuffled_indices\n",
        "    \n",
        "    max_epoch = 50\n",
        "    warmup_epochs = 0\n",
        "    if mode == \"warmup\":\n",
        "      warmup_epochs = 2\n",
        "      max_epoch += warmup_epochs\n",
        "    print_interval = 1\n",
        "\n",
        "    train_log, val_log = [],[]\n",
        "    if mode == \"warmup\":\n",
        "        print(f\"TRAINING MODE:::{mode} (then implicit)\")\n",
        "    else:\n",
        "        print(f\"TRAINING MODE:::{mode}\")\n",
        "    train_loss_vals, val_loss_vals = [], []\n",
        "    train_acc_vals, val_acc_vals = [], []\n",
        "    train_res_vals, val_res_vals = [], []\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "        if mode == \"warmup\":\n",
        "          if epoch >= warmup_epochs:\n",
        "            mode = \"implicit\"\n",
        "            print(\"-------------DONE WARM UP---------------\")\n",
        "          elif epoch == 0:\n",
        "            print(\"----------STARTING WARM UP--------------\")\n",
        "            \n",
        "        idxs = list_shuffler(epoch)\n",
        "        start, end = 0, 0\n",
        "\n",
        "        loss_vals = []\n",
        "        acc_vals = []\n",
        "        res_vals = []\n",
        "\n",
        "        counter = 0\n",
        "        while end < len(idxs):\n",
        "            end = min(start+batch_size, len(idxs))\n",
        "            idxs_to_grab = idxs[start:end]\n",
        "            x_batch = train_images[idxs_to_grab,...]\n",
        "            #x_batch = jnp.tile(x_batch, (1,1,1,24))\n",
        "            y_true = train_labels[idxs_to_grab]\n",
        "            start = end\n",
        "            weights, opt_state, batch_loss, batch_acc, batch_res = step(weights=weights,\n",
        "                                                                        opt_state=opt_state,\n",
        "                                                                        x_batch=x_batch,\n",
        "                                                                        y_true=y_true,\n",
        "                                                                        mode=mode)\n",
        "            \n",
        "            counter += 1\n",
        "            loss_vals.append(batch_loss * x_batch.shape[0])\n",
        "            acc_vals.append(batch_acc * x_batch.shape[0])\n",
        "            res_vals.append(batch_res * x_batch.shape[0])\n",
        "\n",
        "\n",
        "            print(f\"> epoch {epoch+1}, batch {counter} :: batch_loss = {batch_loss}, batch_acc = {batch_acc}, batch_res = {batch_res} \")\n",
        "\n",
        "        epoch_loss = sum(jnp.array(loss_vals)) / len(idxs)\n",
        "        epoch_acc = sum(jnp.array(acc_vals)) / len(idxs)\n",
        "        epoch_res = sum(jnp.array(res_vals)) / len(idxs)\n",
        "        \n",
        "        train_loss_vals.append(epoch_loss)\n",
        "        train_acc_vals.append(epoch_acc)\n",
        "        train_res_vals.append(epoch_res)\n",
        "\n",
        "        val_loss, val_acc, val_res = predict(test_images,test_labels,encoder,decoder,mdeqff,weights,solver_fn,mode)\n",
        "\n",
        "        val_loss_vals.append(val_loss)\n",
        "        val_acc_vals.append(val_acc)\n",
        "        val_res_vals.append(val_res)\n",
        "\n",
        "        if epoch % print_interval == 0:\n",
        "            print(f\"\\tTRAIN epoch = {epoch+1} / loss = {epoch_loss} / acc = {epoch_acc} / res = {epoch_res}\")\n",
        "            print(f\"\\tVAL epoch = {epoch+1} / loss = {val_loss} / acc = {val_acc} / res = {val_res}\")\n",
        "\n",
        "        if epoch_loss < 1e-5:\n",
        "            break\n",
        "\n",
        "            print('finally', batch_loss)\n",
        "    results = {'train_loss_vals': train_loss_vals,\n",
        "               'train_acc_vals': train_acc_vals,\n",
        "               'train_res_vals': train_res_vals,\n",
        "               'val_loss_vals': val_loss_vals,\n",
        "               'val_acc_vals': val_acc_vals,\n",
        "               'val_res_vals': val_res_vals}\n",
        "    return results"
      ],
      "metadata": {
        "id": "S_3FNBfqojTY"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#broyden_results = train(mode=\"implicit\",solver=\"broyden\")\n",
        "results = train(mode=\"implicit\", solver='broyden')"
      ],
      "metadata": {
        "id": "wNA8y6PdRdZr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa6f0af3-b51a-4cab-c1ac-214a2dae3cb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM TRAINING IMAGES:::1000\n",
            "NUM TEST IMAGES:::100\n",
            "TRAINING MODE:::implicit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/jax/_src/random.py:371: FutureWarning: jax.random.shuffle is deprecated and will be removed in a future release. Use jax.random.permutation with independent=True.\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> epoch 1, batch 1 :: batch_loss = 2.31075382232666, batch_acc = 0.1328125, batch_res = 0.5749743580818176 \n",
            "> epoch 1, batch 2 :: batch_loss = 2.30481219291687, batch_acc = 0.1015625, batch_res = 0.5804055333137512 \n",
            "> epoch 1, batch 3 :: batch_loss = 2.327786684036255, batch_acc = 0.09375, batch_res = 0.5749181509017944 \n",
            "> epoch 1, batch 4 :: batch_loss = 2.2836661338806152, batch_acc = 0.1640625, batch_res = 0.5762720108032227 \n",
            "> epoch 1, batch 5 :: batch_loss = 2.300065040588379, batch_acc = 0.09375, batch_res = 0.5708030462265015 \n",
            "> epoch 1, batch 6 :: batch_loss = 2.3182971477508545, batch_acc = 0.1015625, batch_res = 0.575866162776947 \n",
            "> epoch 1, batch 7 :: batch_loss = 2.30239200592041, batch_acc = 0.15625, batch_res = 0.5762382745742798 \n",
            "> epoch 1, batch 8 :: batch_loss = 2.3025920391082764, batch_acc = 0.10576923191547394, batch_res = 0.5763995051383972 \n",
            "\tTRAIN epoch = 1 / loss = 2.306384563446045 / acc = 0.11900000274181366 / res = 0.5757187008857727\n",
            "\tVAL epoch = 1 / loss = 2.2918519973754883 / acc = 0.14000000059604645 / res = 0.580933690071106\n",
            "> epoch 2, batch 1 :: batch_loss = 2.293337345123291, batch_acc = 0.125, batch_res = 0.5760354995727539 \n",
            "> epoch 2, batch 2 :: batch_loss = 2.2950522899627686, batch_acc = 0.109375, batch_res = 0.5731780529022217 \n",
            "> epoch 2, batch 3 :: batch_loss = 2.2865724563598633, batch_acc = 0.21875, batch_res = 0.5755276083946228 \n",
            "> epoch 2, batch 4 :: batch_loss = 2.3097076416015625, batch_acc = 0.0859375, batch_res = 0.577655017375946 \n",
            "> epoch 2, batch 5 :: batch_loss = 2.3019964694976807, batch_acc = 0.125, batch_res = 0.5773431658744812 \n",
            "> epoch 2, batch 6 :: batch_loss = 2.283212900161743, batch_acc = 0.171875, batch_res = 0.5712376832962036 \n"
          ]
        }
      ]
    }
  ]
}